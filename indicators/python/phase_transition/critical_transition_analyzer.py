"""
ä¸´ç•Œç›¸å˜å¸‚åœºé¢„è­¦ç³»ç»Ÿ
Critical Phase Transition Market Warning System

åŸºäºç»Ÿè®¡ç‰©ç†å’Œä¸´ç•Œç†è®ºçš„é‡‘èå¸‚åœºç›¸å˜æ£€æµ‹ä¸é¢„è­¦ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.distributions import MultivariateNormal
    TORCH_AVAILABLE = True
    print("ğŸ§  PyTorch å·²å¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ä¸å¯ç”¨")

try:
    from scipy import stats
    from scipy.optimize import minimize_scalar
    from scipy.signal import find_peaks
    from scipy.fft import fft, fftfreq
    SCIPY_AVAILABLE = True
    print("ğŸ”¬ SciPy å·²å¯ç”¨")
except ImportError:
    SCIPY_AVAILABLE = False
    print("âš ï¸ SciPy ä¸å¯ç”¨")

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
    print("ğŸ•¸ï¸ NetworkX å·²å¯ç”¨")
except ImportError:
    NETWORKX_AVAILABLE = False
    print("âš ï¸ NetworkX ä¸å¯ç”¨")

class CriticalTransitionAnalyzer:
    """
    ä¸´ç•Œç›¸å˜åˆ†æå™¨

    æ£€æµ‹é‡‘èå¸‚åœºçš„ä¸´ç•Œç›¸å˜ç°è±¡ï¼Œæä¾›æ—©æœŸé¢„è­¦ä¿¡å·
    """

    def __init__(self,
                 window_size: int = 100,
                 detection_threshold: float = 2.0,
                 early_warning_threshold: float = 1.5,
                 sensitivity: float = 0.8):
        """
        åˆå§‹åŒ–ä¸´ç•Œç›¸å˜åˆ†æå™¨

        Args:
            window_size: åˆ†æçª—å£å¤§å°
            detection_threshold: æ£€æµ‹é˜ˆå€¼
            early_warning_threshold: æ—©æœŸé¢„è­¦é˜ˆå€¼
            sensitivity: ç³»ç»Ÿçµæ•åº¦
        """
        self.window_size = window_size
        self.detection_threshold = detection_threshold
        self.early_warning_threshold = early_warning_threshold
        self.sensitivity = sensitivity

        # ä¸´ç•ŒæŒ‡æ ‡
        self.critical_indicators = {}
        self.early_warning_signals = []
        self.phase_transition_history = []

        # ä¸´ç•Œå‚æ•°
        self.critical_exponents = {}
        self.correlation_length = {}
        self.order_parameters = {}

        # é¢„è­¦çŠ¶æ€
        self.warning_level = 0  # 0: æ­£å¸¸, 1: è­¦å‘Š, 2: å±é™©, 3: ä¸´ç•Œ
        self.warning_history = []

        # ç³»ç»ŸçŠ¶æ€
        self.system_state = 'normal'
        self.stability_metrics = {}
        self.resilience_indicators = {}

        # ç›¸å˜æ£€æµ‹æ¨¡å‹
        self.phase_detection_model = None
        self.early_warning_model = None

        if TORCH_AVAILABLE:
            self._build_detection_models()

    def _build_detection_models(self):
        """æ„å»ºæ£€æµ‹æ¨¡å‹"""
        # ä¸´ç•Œç›¸å˜æ£€æµ‹æ¨¡å‹
        self.phase_detection_model = CriticalPhaseDetector(
            input_dim=10,  # å¤šç§ä¸´ç•ŒæŒ‡æ ‡
            hidden_dims=[64, 32, 16],
            sensitivity=self.sensitivity
        )

        # æ—©æœŸé¢„è­¦æ¨¡å‹
        self.early_warning_model = EarlyWarningPredictor(
            window_size=self.window_size,
            prediction_horizon=20
        )

    def analyze_market_phase_transition(self, market_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºç›¸å˜"""
        print("ğŸŒ€ åˆ†æå¸‚åœºä¸´ç•Œç›¸å˜...")

        # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—ä¸´ç•ŒæŒ‡æ ‡
        critical_metrics = self._calculate_critical_metrics(market_data)

        # ç¬¬äºŒæ­¥ï¼šæ£€æµ‹æ—©æœŸé¢„è­¦ä¿¡å·
        early_warnings = self._detect_early_warning_signals(critical_metrics)

        # ç¬¬ä¸‰æ­¥ï¼šè¯†åˆ«ä¸´ç•Œç›¸å˜
        phase_transitions = self._identify_phase_transitions(critical_metrics)

        # ç¬¬å››æ­¥ï¼šè®¡ç®—ä¸´ç•ŒæŒ‡æ•°
        critical_exponents = self._calculate_critical_exponents(market_data, critical_metrics)

        # ç¬¬äº”æ­¥ï¼šè¯„ä¼°ç³»ç»Ÿç¨³å®šæ€§
        stability_assessment = self._assess_system_stability(critical_metrics)

        # ç¬¬å…­æ­¥ï¼šé¢„æµ‹ç›¸å˜é£é™©
        risk_prediction = self._predict_transition_risk(critical_metrics, early_warnings)

        # ç¬¬ä¸ƒæ­¥ï¼šç”Ÿæˆé¢„è­¦æŠ¥å‘Š
        warning_report = self._generate_warning_report(
            critical_metrics, early_warnings, phase_transitions, risk_prediction
        )

        results = {
            'critical_metrics': critical_metrics,
            'early_warnings': early_warnings,
            'phase_transitions': phase_transitions,
            'critical_exponents': critical_exponents,
            'stability_assessment': stability_assessment,
            'risk_prediction': risk_prediction,
            'warning_report': warning_report,
            'system_state': self.system_state,
            'warning_level': self.warning_level,
            'analysis_timestamp': pd.Timestamp.now()
        }

        # æ›´æ–°å†å²è®°å½•
        self.phase_transition_history.append(results)
        self.warning_history.append(self.warning_level)

        return results

    def _calculate_critical_metrics(self, market_data: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—ä¸´ç•ŒæŒ‡æ ‡"""
        print("ğŸ“Š è®¡ç®—ä¸´ç•ŒæŒ‡æ ‡...")

        metrics = {}

        # ä»·æ ¼æ•°æ®æå–
        if 'close' in market_data.columns:
            prices = market_data['close'].dropna()
            if len(prices) > self.window_size:
                # æ³¢åŠ¨ç‡æŒ‡æ ‡
                metrics['volatility_metrics'] = self._calculate_volatility_metrics(prices)

                # ç›¸å…³æ€§æŒ‡æ ‡
                metrics['correlation_metrics'] = self._calculate_correlation_metrics(market_data)

                # åˆ†å½¢æŒ‡æ ‡
                metrics['fractal_metrics'] = self._calculate_fractal_metrics(prices)

                # ä¿¡æ¯è®ºæŒ‡æ ‡
                metrics['information_metrics'] = self._calculate_information_metrics(prices)

                # ä¸´ç•Œæ¥è¿‘åº¦æŒ‡æ ‡
                metrics['critical_proximity'] = self._calculate_critical_proximity(prices)

                # åºç»Ÿè®¡é‡
                metrics['order_statistics'] = self._calculate_order_statistics(prices)

                # åŠŸç‡è°±æŒ‡æ ‡
                metrics['power_spectrum_metrics'] = self._calculate_power_spectrum_metrics(prices)

                # ç½‘ç»œæŒ‡æ ‡
                metrics['network_metrics'] = self._calculate_network_metrics(market_data)

                # æ—¶åºæŒ‡æ ‡
                metrics['temporal_metrics'] = self._calculate_temporal_metrics(prices)

        return metrics

    def _calculate_volatility_metrics(self, prices: pd.Series) -> Dict[str, float]:
        """è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡"""
        metrics = {}

        returns = prices.pct_change().dropna()
        if len(returns) < 10:
            return metrics

        # åŸºç¡€æ³¢åŠ¨ç‡
        metrics['volatility'] = returns.std()

        # æ¡ä»¶æ³¢åŠ¨ç‡
        metrics['conditional_volatility'] = self._calculate_conditional_volatility(returns)

        # æ³¢åŠ¨ç‡èšé›†æ€§
        metrics['volatility_clustering'] = self._calculate_volatility_clustering(returns)

        # æ³¢åŠ¨ç‡ååº¦
        metrics['volatility_skewness'] = returns.skew()

        # æ³¢åŠ¨ç‡å³°åº¦
        metrics['volatility_kurtosis'] = returns.kurtosis()

        # æ³¢åŠ¨ç‡é•¿è®°å¿†æ€§
        metrics['volatility_long_memory'] = self._calculate_long_memory(returns)

        return metrics

    def _calculate_conditional_volatility(self, returns: pd.Series) -> float:
        """è®¡ç®—æ¡ä»¶æ³¢åŠ¨ç‡"""
        if len(returns) < 20:
            return 0.0

        # GARCH(1,1) ç®€åŒ–ç‰ˆæœ¬
        omega = 0.1 * returns.var()
        alpha = 0.1
        beta = 0.85

        conditional_var = [omega / (1 - alpha - beta)]  # é•¿æœŸæ–¹å·®

        for i in range(1, min(len(returns), 50)):
            new_var = omega + alpha * returns.iloc[-i]**2 + beta * conditional_var[-1]
            conditional_var.append(new_var)

        return np.sqrt(conditional_var[-1])

    def _calculate_volatility_clustering(self, returns: pd.Series) -> float:
        """è®¡ç®—æ³¢åŠ¨ç‡èšé›†æ€§"""
        if len(returns) < 20:
            return 0.0

        # è®¡ç®—ç»å¯¹æ”¶ç›Šçš„è‡ªç›¸å…³
        abs_returns = np.abs(returns)
        autocorr = [abs_returns.autocorr(lag=i) for i in range(1, min(11, len(returns)//2))]

        return np.mean(np.abs(autocorr))

    def _calculate_long_memory(self, returns: pd.Series) -> float:
        """è®¡ç®—é•¿è®°å¿†æ€§"""
        if len(returns) < 50:
            return 0.0

        # ç®€åŒ–çš„é•¿è®°å¿†æ€§æ£€æµ‹
        abs_returns = np.abs(returns)

        # è®¡ç®—ä¸åŒæ»åæœŸçš„è‡ªç›¸å…³
        lags = range(1, min(21, len(returns)//4))
        autocorrs = [abs_returns.autocorr(lag=lag) for lag in lags]

        # æ‹Ÿåˆå¹‚å¾‹è¡°å‡
        if len(autocorrs) > 5:
            log_lags = np.log(lags)
            log_autocorrs = np.log(np.abs(autocorrs))

            # çº¿æ€§å›å½’æ‹Ÿåˆ
            slope, _ = np.polyfit(log_lags, log_autocorrs, 1)

            return -slope  # èµ«æ–¯ç‰¹æŒ‡æ•°çš„è¿‘ä¼¼

        return 0.5

    def _calculate_correlation_metrics(self, market_data: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡"""
        metrics = {}

        numeric_columns = market_data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) < 2:
            return metrics

        # ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = market_data[numeric_columns].corr()

        # å¹³å‡ç›¸å…³æ€§
        avg_correlation = correlation_matrix.abs().mean()
        metrics['average_correlation'] = avg_correlation

        # ç›¸å…³æ€§æ³¢åŠ¨ç‡
        metrics['correlation_volatility'] = correlation_matrix.values.std()

        # æœ€å¤§ç›¸å…³æ€§
        metrics['maximum_correlation'] = correlation_matrix.abs().max().max()

        # ç›¸å…³æ€§ååº¦
        correlations_flat = correlation_matrix.values.flatten()
        correlations_flat = correlations_flat[~np.isnan(correlations_flat)]
        if len(correlations_flat) > 0:
            metrics['correlation_skewness'] = stats.skew(correlations_flat)

        # ç›¸å…³æ€§ç³»ç»Ÿé£é™©
        metrics['systemic_risk_measure'] = self._calculate_systemic_risk(correlation_matrix)

        return metrics

    def _calculate_systemic_risk(self, correlation_matrix: pd.DataFrame) -> float:
        """è®¡ç®—ç³»ç»Ÿæ€§é£é™©"""
        try:
            # æœ€å¤§ç‰¹å¾å€¼ï¼ˆç³»ç»Ÿæ€§é£é™©æŒ‡æ ‡ï¼‰
            eigenvalues = np.linalg.eigvals(correlation_matrix.fillna(0))
            max_eigenvalue = np.max(eigenvalues.real)

            # å½’ä¸€åŒ–
            n = len(correlation_matrix)
            normalized_max_eigenvalue = max_eigenvalue / n

            return normalized_max_eigenvalue
        except:
            return 0.0

    def _calculate_fractal_metrics(self, prices: pd.Series) -> Dict[str, float]:
        """è®¡ç®—åˆ†å½¢æŒ‡æ ‡"""
        metrics = {}

        if len(prices) < 50:
            return metrics

        # èµ«æ–¯ç‰¹æŒ‡æ•°
        metrics['hurst_exponent'] = self._calculate_hurst_exponent(prices)

        # åˆ†å½¢ç»´æ•°
        metrics['fractal_dimension'] = self._calculate_fractal_dimension(prices)

        # é‡æ ‡æå·®åˆ†æ
        metrics['rescaled_range'] = self._calculate_rescaled_range(prices)

        return metrics

    def _calculate_hurst_exponent(self, prices: pd.Series) -> float:
        """è®¡ç®—èµ«æ–¯ç‰¹æŒ‡æ•°"""
        if len(prices) < 100:
            return 0.5

        try:
            # R/Såˆ†æ
            N = len(prices)
            max_k = min(N // 4, 50)

            R_S_values = []
            k_values = []

            for k in range(10, max_k + 1):
                # åˆ†å‰²æ—¶é—´åºåˆ—
                segments = N // k
                if segments < 2:
                    continue

                R_S_ratios = []
                for i in range(segments):
                    segment = prices.iloc[i*k:(i+1)*k]
                    if len(segment) < 2:
                        continue

                    # è®¡ç®—ç´¯ç§¯åå·®
                    mean = segment.mean()
                    cumulative_deviation = (segment - mean).cumsum()
                    R = np.max(cumulative_deviation) - np.min(cumulative_deviation)
                    S = segment.std()

                    if S > 0:
                        R_S_ratios.append(R / S)

                if R_S_ratios:
                    R_S_values.append(np.mean(R_S_ratios))
                    k_values.append(k)

            if len(R_S_values) > 5:
                # å¯¹æ•°çº¿æ€§å›å½’
                log_k = np.log(k_values)
                log_R_S = np.log(R_S_values)

                slope, _ = np.polyfit(log_k, log_R_S, 1)
                return slope

        except Exception as e:
            print(f"âš ï¸ èµ«æ–¯ç‰¹æŒ‡æ•°è®¡ç®—å¤±è´¥: {e}")

        return 0.5

    def _calculate_fractal_dimension(self, prices: pd.Series) -> float:
        """è®¡ç®—åˆ†å½¢ç»´æ•°"""
        if len(prices) < 20:
            return 1.0

        try:
            # ç®€åŒ–çš„ç›’ç»´æ•°è®¡ç®—
            N = len(prices)
            price_range = prices.max() - prices.min()

            if price_range == 0:
                return 1.0

            # ä¸åŒå°ºåº¦çš„ç›’å­
            scales = np.logspace(0, np.log10(N//10), 10)
            counts = []

            for scale in scales:
                box_size = price_range / scale
                if box_size <= 0:
                    continue

                # è®¡ç®—éœ€è¦çš„ç›’å­æ•°
                normalized_prices = (prices - prices.min()) / price_range
                boxes = set()
                for i, price in enumerate(normalized_prices):
                    box_index = int(price * scale)
                    boxes.add(box_index)

                counts.append(len(boxes))

            if len(counts) > 5:
                # å¯¹æ•°çº¿æ€§å›å½’
                log_scales = np.log(scales[:len(counts)])
                log_counts = np.log(counts)

                slope, _ = np.polyfit(log_scales, log_counts, 1)
                return -slope

        except Exception as e:
            print(f"âš ï¸ åˆ†å½¢ç»´æ•°è®¡ç®—å¤±è´¥: {e}")

        return 1.0

    def _calculate_rescaled_range(self, prices: pd.Series) -> float:
        """è®¡ç®—é‡æ ‡æå·®"""
        if len(prices) < 20:
            return 0.0

        try:
            # ç®€åŒ–çš„R/Sè®¡ç®—
            N = len(prices)
            mean_price = prices.mean()
            cumulative_deviation = (prices - mean_price).cumsum()

            R = np.max(cumulative_deviation) - np.min(cumulative_deviation)
            S = prices.std()

            return R / S if S > 0 else 0.0

        except:
            return 0.0

    def _calculate_information_metrics(self, prices: pd.Series) -> Dict[str, float]:
        """è®¡ç®—ä¿¡æ¯è®ºæŒ‡æ ‡"""
        metrics = {}

        if len(prices) < 20:
            return metrics

        returns = prices.pct_change().dropna()
        if len(returns) < 10:
            return metrics

        # è¿‘ä¼¼ç†µ
        metrics['approximate_entropy'] = self._calculate_approximate_entropy(returns)

        # æ ·æœ¬ç†µ
        metrics['sample_entropy'] = self._calculate_sample_entropy(returns)

        # æ’åˆ—ç†µ
        metrics['permutation_entropy'] = self._calculate_permutation_entropy(returns)

        # æäºšæ™®è¯ºå¤«æŒ‡æ•°
        metrics['lyapunov_exponent'] = self._calculate_lyapunov_exponent(returns)

        return metrics

    def _calculate_approximate_entropy(self, data: pd.Series, m: int = 2, r: float = 0.2) -> float:
        """è®¡ç®—è¿‘ä¼¼ç†µ"""
        if len(data) < m + 1:
            return 0.0

        try:
            N = len(data)
            data_std = data.std()

            if data_std == 0:
                return 0.0

            r_value = r * data_std

            # è®¡ç®—æ¨¡æ¿åŒ¹é…
            def _count_matches(data, m, r):
                matches = 0
                total = 0

                for i in range(N - m + 1):
                    template = data.iloc[i:i+m]
                    for j in range(N - m + 1):
                        if i != j:
                            comparison = data.iloc[j:j+m]
                            if np.max(np.abs(template - comparison)) <= r_value:
                                matches += 1
                    total += 1

                return matches / total if total > 0 else 0

            # è®¡ç®—må’Œm+1ç»´çš„åŒ¹é…
            phi_m = np.log(_count_matches(data, m, r) + 1e-10)
            phi_m1 = np.log(_count_matches(data, m+1, r) + 1e-10)

            return phi_m - phi_m1

        except:
            return 0.0

    def _calculate_sample_entropy(self, data: pd.Series, m: int = 2, r: float = 0.2) -> float:
        """è®¡ç®—æ ·æœ¬ç†µ"""
        if len(data) < m + 1:
            return 0.0

        try:
            N = len(data)
            data_std = data.std()

            if data_std == 0:
                return 0.0

            r_value = r * data_std

            # è®¡ç®—æ¨¡æ¿åŒ¹é…ï¼ˆä¸åŒ…æ‹¬è‡ªåŒ¹é…ï¼‰
            def _count_matches_sample(data, m, r):
                matches_A = 0
                matches_B = 0

                for i in range(N - m):
                    template_m = data.iloc[i:i+m]
                    template_m1 = data.iloc[i:i+m+1]

                    for j in range(N - m):
                        if i != j:
                            comparison_m = data.iloc[j:j+m]
                            comparison_m1 = data.iloc[j:j+m+1]

                            if np.max(np.abs(template_m - comparison_m)) <= r_value:
                                matches_A += 1
                                if np.max(np.abs(template_m1 - comparison_m1)) <= r_value:
                                    matches_B += 1

                return matches_A, matches_B

            matches_A, matches_B = _count_matches_sample(data, m, r)

            if matches_A > 0:
                return -np.log(matches_B / matches_A)
            else:
                return 0.0

        except:
            return 0.0

    def _calculate_permutation_entropy(self, data: pd.Series, m: int = 3) -> float:
        """è®¡ç®—æ’åˆ—ç†µ"""
        if len(data) < m + 1:
            return 0.0

        try:
            N = len(data)
            patterns = []

            for i in range(N - m + 1):
                # æå–æ¨¡å¼
                pattern = data.iloc[i:i+m]
                # æ’åºè·å–æ¨¡å¼ç±»å‹
                sorted_indices = np.argsort(pattern.values)
                pattern_type = tuple(sorted_indices)

                patterns.append(pattern_type)

            # è®¡ç®—æ¨¡å¼é¢‘ç‡
            unique_patterns, counts = np.unique(patterns, return_counts=True)
            probabilities = counts / len(patterns)

            # è®¡ç®—ç†µ
            entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))

            # å½’ä¸€åŒ–
            max_entropy = np.log(np.math.factorial(m))

            return entropy / max_entropy if max_entropy > 0 else 0.0

        except:
            return 0.0

    def _calculate_lyapunov_exponent(self, data: pd.Series) -> float:
        """è®¡ç®—æäºšæ™®è¯ºå¤«æŒ‡æ•°"""
        if len(data) < 50:
            return 0.0

        try:
            # ç®€åŒ–çš„æäºšæ™®è¯ºå¤«æŒ‡æ•°è®¡ç®—
            N = len(data)
            m = min(10, N // 5)  # åµŒå…¥ç»´åº¦
            tau = 1  # æ—¶é—´å»¶è¿Ÿ

            # ç›¸ç©ºé—´é‡æ„
            trajectories = []
            for i in range(N - (m-1)*tau):
                trajectory = data.iloc[i:i+(m-1)*tau+1:tau].values
                trajectories.append(trajectory)

            if len(trajectories) < 10:
                return 0.0

            # è®¡ç®—æœ€è¿‘é‚»å‘æ•£
            divergences = []
            for i, traj in enumerate(trajectories[:-1]):
                # æ‰¾åˆ°æœ€è¿‘é‚»
                distances = [np.linalg.norm(traj - other) for j, other in enumerate(trajectories) if i != j]
                if distances:
                    min_distance = min(distances)
                    # è®¡ç®—ä¸€æ­¥åçš„å‘æ•£
                    if i+1 < len(trajectories):
                        next_distance = np.linalg.norm(trajectories[i+1] - trajectories[distances.index(min_distance)+1])
                        if min_distance > 0:
                            divergence = np.log(next_distance / min_distance)
                            divergences.append(divergence)

            if divergences:
                return np.mean(divergences)

        except:
            return 0.0

        return 0.0

    def _calculate_critical_proximity(self, prices: pd.Series) -> Dict[str, float]:
        """è®¡ç®—ä¸´ç•Œæ¥è¿‘åº¦"""
        proximity = {}

        if len(prices) < 50:
            return proximity

        returns = prices.pct_change().dropna()
        if len(returns) < 20:
            return proximity

        # ä»·æ ¼åºåˆ—ä¸´ç•Œæ¥è¿‘åº¦
        proximity['price_criticality'] = self._assess_price_criticality(prices)

        # æ³¢åŠ¨ç‡ä¸´ç•Œæ¥è¿‘åº¦
        proximity['volatility_criticality'] = self._assess_volatility_criticality(returns)

        # ç›¸å…³æ€§ä¸´ç•Œæ¥è¿‘åº¦
        proximity['correlation_criticality'] = self._assess_correlation_criticality(returns)

        # ç»¼åˆä¸´ç•Œæ¥è¿‘åº¦
        criticality_scores = [
            proximity.get('price_criticality', 0),
            proximity.get('volatility_criticality', 0),
            proximity.get('correlation_criticality', 0)
        ]
        proximity['overall_criticality'] = np.mean([score for score in criticality_scores if score > 0])

        return proximity

    def _assess_price_criticality(self, prices: pd.Series) -> float:
        """è¯„ä¼°ä»·æ ¼ä¸´ç•Œæ€§"""
        try:
            # ä»·æ ¼åºåˆ—çš„ç»Ÿè®¡ç‰¹æ€§
            returns = prices.pct_change().dropna()

            # ä»·æ ¼å˜åŠ¨çš„æ–¹å·®
            variance = returns.var()

            # ä»·æ ¼å˜åŠ¨çš„ååº¦
            skewness = returns.skew()

            # ä»·æ ¼å˜åŠ¨çš„å³°åº¦
            kurtosis = returns.kurtosis()

            # ä¸´ç•Œæ€§è¯„åˆ†ï¼ˆç»¼åˆæŒ‡æ ‡ï¼‰
            criticality_score = 0

            # é«˜æ³¢åŠ¨ç‡è´¡çŒ®
            if variance > 0.01:  # æ—¥æ³¢åŠ¨ç‡è¶…è¿‡1%
                criticality_score += 0.3

            # æç«¯ååº¦è´¡çŒ®
            if abs(skewness) > 1:
                criticality_score += 0.3

            # é«˜å³°åº¦è´¡çŒ®
            if kurtosis > 3:
                criticality_score += 0.4

            return min(criticality_score, 1.0)

        except:
            return 0.0

    def _assess_volatility_criticality(self, returns: pd.Series) -> float:
        """è¯„ä¼°æ³¢åŠ¨ç‡ä¸´ç•Œæ€§"""
        try:
            # æ³¢åŠ¨ç‡èšé›†æ€§
            abs_returns = np.abs(returns)
            clustering = abs_returns.autocorr()

            # æ³¢åŠ¨ç‡é•¿è®°å¿†æ€§
            long_memory = self._calculate_long_memory(returns)

            criticality_score = 0

            # å¼ºèšé›†æ€§
            if clustering > 0.2:
                criticality_score += 0.5

            # é•¿è®°å¿†æ€§
            if long_memory > 0.6:
                criticality_score += 0.5

            return min(criticality_score, 1.0)

        except:
            return 0.0

    def _assess_correlation_criticality(self, returns: pd.Series) -> float:
        """è¯„ä¼°ç›¸å…³æ€§ä¸´ç•Œæ€§"""
        try:
            # è‡ªç›¸å…³åˆ†æ
            autocorrs = [returns.autocorr(lag=i) for i in range(1, min(11, len(returns)//2))]

            if not autocorrs:
                return 0.0

            # è‡ªç›¸å…³çš„æŒç»­æ€§
            persistence = np.mean(np.abs(autocorrs))

            criticality_score = 0

            # å¼ºè‡ªç›¸å…³
            if persistence > 0.3:
                criticality_score += 1.0

            return min(criticality_score, 1.0)

        except:
            return 0.0

    def _calculate_order_statistics(self, prices: pd.Series) -> Dict[str, float]:
        """è®¡ç®—åºç»Ÿè®¡é‡"""
        order_stats = {}

        if len(prices) < 20:
            return order_stats

        returns = prices.pct_change().dropna()
        if len(returns) < 10:
            return order_stats

        # åˆ†ä½æ•°åˆ†æ
        quantiles = [0.05, 0.25, 0.5, 0.75, 0.95]
        for q in quantiles:
            order_stats[f'quantile_{int(q*100)}'] = returns.quantile(q)

        # æå€¼ç»Ÿè®¡
        order_stats['max_return'] = returns.max()
        order_stats['min_return'] = returns.min()
        order_stats['range'] = returns.max() - returns.min()

        # æå€¼é—´éš”
        order_stats['extreme_interval'] = self._calculate_extreme_interval(returns)

        return order_stats

    def _calculate_extreme_interval(self, returns: pd.Series, threshold: float = 0.95) -> float:
        """è®¡ç®—æå€¼é—´éš”"""
        try:
            # ç¡®å®šæå€¼é˜ˆå€¼
            high_threshold = returns.quantile(threshold)
            low_threshold = returns.quantile(1 - threshold)

            # æ‰¾åˆ°æå€¼ç‚¹
            extreme_points = returns[(returns > high_threshold) | (returns < low_threshold)]

            if len(extreme_points) < 2:
                return 0.0

            # è®¡ç®—å¹³å‡é—´éš”
            intervals = []
            for i in range(1, len(extreme_points)):
                interval = extreme_points.index[i] - extreme_points.index[i-1]
                intervals.append(interval)

            return np.mean(intervals) if intervals else 0.0

        except:
            return 0.0

    def _calculate_power_spectrum_metrics(self, prices: pd.Series) -> Dict[str, float]:
        """è®¡ç®—åŠŸç‡è°±æŒ‡æ ‡"""
        spectrum_metrics = {}

        if len(prices) < 50:
            return spectrum_metrics

        try:
            # FFTå˜æ¢
            returns = prices.pct_change().dropna()
            if len(returns) < 20:
                return spectrum_metrics

            # å»é™¤è¶‹åŠ¿
            detrended = returns - returns.mean()

            # FFT
            fft_values = fft(detrended.values)
            power_spectrum = np.abs(fft_values) ** 2
            frequencies = fftfreq(len(detrended))

            # åªè€ƒè™‘æ­£é¢‘ç‡
            positive_freq_idx = frequencies > 0
            positive_frequencies = frequencies[positive_freq_idx]
            positive_power = power_spectrum[positive_freq_idx]

            if len(positive_power) > 0:
                # åŠŸç‡è°±æ–œç‡
                log_freq = np.log(positive_frequencies[1:])  # å»é™¤ç›´æµåˆ†é‡
                log_power = np.log(positive_power[1:])

                if len(log_freq) > 5:
                    slope, _ = np.polyfit(log_freq, log_power, 1)
                    spectrum_metrics['power_slope'] = slope

                # å…‰è°±ç†µ
                power_prob = positive_power / np.sum(positive_power)
                spectral_entropy = -np.sum(power_prob * np.log(power_prob + 1e-10))
                spectrum_metrics['spectral_entropy'] = spectral_entropy

                # ä¸»é¢‘ç‡
                dominant_freq_idx = np.argmax(positive_power)
                spectrum_metrics['dominant_frequency'] = positive_frequencies[dominant_freq_idx]

        except:
            pass

        return spectrum_metrics

    def _calculate_network_metrics(self, market_data: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—ç½‘ç»œæŒ‡æ ‡"""
        network_metrics = {}

        if not NETWORKX_AVAILABLE:
            return network_metrics

        numeric_columns = market_data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) < 3:
            return network_metrics

        try:
            # æ„å»ºç›¸å…³æ€§ç½‘ç»œ
            correlation_matrix = market_data[numeric_columns].corr()
            adjacency_matrix = correlation_matrix.abs()

            # åˆ›å»ºç½‘ç»œ
            G = nx.from_numpy_array(adjacency_matrix.values)

            # ç½‘ç»œå¯†åº¦
            network_metrics['network_density'] = nx.density(G)

            # èšç±»ç³»æ•°
            network_metrics['clustering_coefficient'] = nx.average_clustering(G)

            # å¹³å‡è·¯å¾„é•¿åº¦
            if nx.is_connected(G):
                network_metrics['average_path_length'] = nx.average_shortest_path_length(G)
            else:
                network_metrics['average_path_length'] = 0.0

            # ç½‘ç»œä¸­å¿ƒæ€§
            centrality = nx.degree_centrality(G)
            network_metrics['centrality_entropy'] = self._calculate_entropy(list(centrality.values()))

        except:
            pass

        return network_metrics

    def _calculate_entropy(self, values: List[float]) -> float:
        """è®¡ç®—ç†µ"""
        if not values:
            return 0.0

        # å½’ä¸€åŒ–
        values = np.array(values)
        values = values / np.sum(values)

        # è®¡ç®—ç†µ
        entropy = -np.sum(values * np.log(values + 1e-10))

        return entropy

    def _calculate_temporal_metrics(self, prices: pd.Series) -> Dict[str, float]:
        """è®¡ç®—æ—¶åºæŒ‡æ ‡"""
        temporal_metrics = {}

        if len(prices) < 30:
            return temporal_metrics

        returns = prices.pct_change().dropna()
        if len(returns) < 20:
            return temporal_metrics

        # æ—¶åºå¤æ‚åº¦
        temporal_metrics['temporal_complexity'] = self._calculate_temporal_complexity(returns)

        # ä¸å¯é¢„æµ‹æ€§
        temporal_metrics['unpredictability'] = self._calculate_unpredictability(returns)

        # è®°å¿†æ€§
        temporal_metrics['memory_strength'] = self._calculate_memory_strength(returns)

        return temporal_metrics

    def _calculate_temporal_complexity(self, returns: pd.Series) -> float:
        """è®¡ç®—æ—¶åºå¤æ‚åº¦"""
        try:
            # åŸºäºæ’åˆ—ç†µçš„å¤æ‚åº¦
            perm_entropy = self._calculate_permutation_entropy(returns, m=4)

            # åŸºäºæ ·æœ¬ç†µçš„å¤æ‚åº¦
            sample_entropy = self._calculate_sample_entropy(returns, m=2, r=0.2)

            # ç»¼åˆå¤æ‚åº¦
            complexity = (perm_entropy + sample_entropy) / 2

            return min(complexity, 1.0)

        except:
            return 0.0

    def _calculate_unpredictability(self, returns: pd.Series) -> float:
        """è®¡ç®—ä¸å¯é¢„æµ‹æ€§"""
        try:
            # åŸºäºè‡ªç›¸å…³çš„ä¸å¯é¢„æµ‹æ€§
            autocorrs = [returns.autocorr(lag=i) for i in range(1, min(6, len(returns)//3))]

            if autocorrs:
                # è‡ªç›¸å…³çš„å¹³å‡å¼ºåº¦
                autocorr_strength = np.mean(np.abs(autocorrs))
                unpredictability = 1.0 - autocorr_strength
                return max(0.0, unpredictability)

        except:
            pass

        return 0.5

    def _calculate_memory_strength(self, returns: pd.Series) -> float:
        """è®¡ç®—è®°å¿†å¼ºåº¦"""
        try:
            # é•¿è®°å¿†æ€§
            hurst = self._calculate_hurst_exponent(returns)

            # è®°å¿†å¼ºåº¦
            if hurst > 0.5:
                memory_strength = (hurst - 0.5) * 2  # 0åˆ°1ä¹‹é—´
            elif hurst < 0.5:
                memory_strength = (0.5 - hurst) * 2  # åè®°å¿†
            else:
                memory_strength = 0.0

            return min(memory_strength, 1.0)

        except:
            return 0.0

    def _detect_early_warning_signals(self, critical_metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ—©æœŸé¢„è­¦ä¿¡å·"""
        print("âš ï¸ æ£€æµ‹æ—©æœŸé¢„è­¦ä¿¡å·...")

        warnings = []

        # ä¸´ç•Œæ¥è¿‘åº¦é¢„è­¦
        if 'critical_proximity' in critical_metrics:
            proximity = critical_metrics['critical_proximity']
            overall_criticality = proximity.get('overall_criticality', 0)

            if overall_criticality > self.early_warning_threshold:
                warnings.append({
                    'type': 'critical_proximity',
                    'severity': 'high' if overall_criticality > self.detection_threshold else 'medium',
                    'value': overall_criticality,
                    'threshold': self.early_warning_threshold,
                    'description': 'å¸‚åœºæ¥è¿‘ä¸´ç•ŒçŠ¶æ€'
                })

        # æ³¢åŠ¨ç‡é¢„è­¦
        if 'volatility_metrics' in critical_metrics:
            vol_metrics = critical_metrics['volatility_metrics']
            volatility = vol_metrics.get('volatility', 0)
            clustering = vol_metrics.get('volatility_clustering', 0)

            if volatility > 0.05:  # æ—¥æ³¢åŠ¨ç‡è¶…è¿‡5%
                warnings.append({
                    'type': 'high_volatility',
                    'severity': 'high',
                    'value': volatility,
                    'threshold': 0.05,
                    'description': 'å¸‚åœºæ³¢åŠ¨ç‡å¼‚å¸¸å‡é«˜'
                })

            if clustering > 0.3:
                warnings.append({
                    'type': 'volatility_clustering',
                    'severity': 'medium',
                    'value': clustering,
                    'threshold': 0.3,
                    'description': 'æ£€æµ‹åˆ°æ³¢åŠ¨ç‡èšé›†ç°è±¡'
                })

        # ç›¸å…³æ€§é¢„è­¦
        if 'correlation_metrics' in critical_metrics:
            corr_metrics = critical_metrics['correlation_metrics']
            systemic_risk = corr_metrics.get('systemic_risk_measure', 0)

            if systemic_risk > 0.8:
                warnings.append({
                    'type': 'systemic_risk',
                    'severity': 'high',
                    'value': systemic_risk,
                    'threshold': 0.8,
                    'description': 'ç³»ç»Ÿæ€§é£é™©æ˜¾è‘—å¢åŠ '
                })

        # åˆ†å½¢é¢„è­¦
        if 'fractal_metrics' in critical_metrics:
            fractal_metrics = critical_metrics['fractal_metrics']
            hurst = fractal_metrics.get('hurst_exponent', 0.5)

            if hurst > 0.8:
                warnings.append({
                    'type': 'persistent_trend',
                    'severity': 'medium',
                    'value': hurst,
                    'threshold': 0.8,
                    'description': 'æ£€æµ‹åˆ°å¼ºæŒç»­æ€§è¶‹åŠ¿'
                })

        # ä¿¡æ¯è®ºé¢„è­¦
        if 'information_metrics' in critical_metrics:
            info_metrics = critical_metrics['information_metrics']
            entropy = info_metrics.get('approximate_entropy', 0)

            if entropy < 0.5:
                warnings.append({
                    'type': 'low_complexity',
                    'severity': 'medium',
                    'value': entropy,
                    'threshold': 0.5,
                    'description': 'å¸‚åœºå¤æ‚åº¦é™ä½ï¼Œå¯èƒ½é¢„ç¤ºç›¸å˜'
                })

        return warnings

    def _identify_phase_transitions(self, critical_metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«ä¸´ç•Œç›¸å˜"""
        print("ğŸ”„ è¯†åˆ«ä¸´ç•Œç›¸å˜...")

        transitions = []

        # ç»¼åˆä¸´ç•Œæ€§è¯„åˆ†
        criticality_score = self._calculate_composite_criticality(critical_metrics)

        # åˆ¤æ–­ç›¸å˜çŠ¶æ€
        if criticality_score > self.detection_threshold:
            transitions.append({
                'type': 'critical_transition',
                'severity': 'critical',
                'criticality_score': criticality_score,
                'threshold': self.detection_threshold,
                'description': 'æ£€æµ‹åˆ°ä¸´ç•Œç›¸å˜ï¼Œç³»ç»Ÿå¯èƒ½å‘ç”Ÿé‡å¤§è½¬å˜',
                'timestamp': pd.Timestamp.now()
            })

            # æ›´æ–°ç³»ç»ŸçŠ¶æ€
            self.system_state = 'critical'
            self.warning_level = 3
        elif criticality_score > self.early_warning_threshold:
            transitions.append({
                'type': 'approaching_transition',
                'severity': 'warning',
                'criticality_score': criticality_score,
                'threshold': self.early_warning_threshold,
                'description': 'ç³»ç»Ÿæ¥è¿‘ä¸´ç•ŒçŠ¶æ€ï¼Œéœ€è¦å¯†åˆ‡å…³æ³¨',
                'timestamp': pd.Timestamp.now()
            })

            # æ›´æ–°ç³»ç»ŸçŠ¶æ€
            self.system_state = 'warning'
            self.warning_level = 2
        else:
            self.system_state = 'normal'
            self.warning_level = 1

        return transitions

    def _calculate_composite_criticality(self, critical_metrics: Dict[str, Any]) -> float:
        """è®¡ç®—ç»¼åˆä¸´ç•Œæ€§è¯„åˆ†"""
        criticality_components = []

        # ä»å„ä¸ªæŒ‡æ ‡ç»„æå–ä¸´ç•Œæ€§
        if 'critical_proximity' in critical_metrics:
            proximity = critical_metrics['critical_proximity']
            overall_criticality = proximity.get('overall_criticality', 0)
            criticality_components.append(overall_criticality)

        if 'volatility_metrics' in critical_metrics:
            vol_metrics = critical_metrics['volatility_metrics']
            volatility = vol_metrics.get('volatility', 0)
            clustering = vol_metrics.get('volatility_clustering', 0)
            criticality_components.extend([min(volatility * 10, 1.0), clustering])

        if 'correlation_metrics' in critical_metrics:
            corr_metrics = critical_metrics['correlation_metrics']
            systemic_risk = corr_metrics.get('systemic_risk_measure', 0)
            criticality_components.append(systemic_risk)

        if 'information_metrics' in critical_metrics:
            info_metrics = critical_metrics['information_metrics']
            entropy = info_metrics.get('approximate_entropy', 1.0)
            # ä½ç†µé¢„ç¤ºä¸´ç•Œæ€§
            criticality_components.append(1.0 - entropy)

        # åŠ æƒç»¼åˆ
        if criticality_components:
            weights = [0.3, 0.2, 0.2, 0.3]  # å¯ä»¥è°ƒæ•´æƒé‡
            weighted_criticality = np.average(
                criticality_components[:len(weights)],
                weights=weights[:len(criticality_components)]
            )
            return weighted_criticality

        return 0.0

    def _calculate_critical_exponents(self, market_data: pd.DataFrame, critical_metrics: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—ä¸´ç•ŒæŒ‡æ•°"""
        exponents = {}

        if 'close' in market_data.columns:
            prices = market_data['close'].dropna()
            if len(prices) > self.window_size:
                # ç›¸å…³é•¿åº¦æŒ‡æ•°
                exponents['correlation_length'] = self._estimate_correlation_length_exponent(prices)

                # åºå‚æ•°æŒ‡æ•°
                exponents['order_parameter'] = self._estimate_order_parameter_exponent(prices)

                # åŠ¨åŠ›å­¦æŒ‡æ•°
                exponents['dynamical'] = self._estimate_dynamical_exponent(prices)

        return exponents

    def _estimate_correlation_length_exponent(self, prices: pd.Series) -> float:
        """ä¼°è®¡ç›¸å…³é•¿åº¦æŒ‡æ•°"""
        try:
            returns = prices.pct_change().dropna()
            if len(returns) < 50:
                return 0.0

            # è®¡ç®—ä¸åŒæ—¶é—´å°ºåº¦çš„è‡ªç›¸å…³
            lags = range(1, min(31, len(returns)//3))
            autocorrs = [returns.autocorr(lag=lag) for lag in lags]

            # æ‹Ÿåˆå¹‚å¾‹è¡°å‡
            if len(autocorrs) > 5:
                log_lags = np.log(lags)
                log_autocorrs = np.log(np.abs(autocorrs))

                # çº¿æ€§å›å½’
                slope, _ = np.polyfit(log_lags, log_autocorrs, 1)

                # ç›¸å…³é•¿åº¦æŒ‡æ•°
                nu = -1.0 / slope if slope != 0 else 0.0

                return max(0.0, min(nu, 5.0))  # åˆç†èŒƒå›´é™åˆ¶

        except:
            pass

        return 0.0

    def _estimate_order_parameter_exponent(self, prices: pd.Series) -> float:
        """ä¼°è®¡åºå‚æ•°æŒ‡æ•°"""
        try:
            # ä½¿ç”¨ä»·æ ¼å˜åŠ¨çš„å¹…åº¦ä½œä¸ºåºå‚æ•°
            returns = prices.pct_change().dropna()
            if len(returns) < 50:
                return 0.0

            # è®¡ç®—ä¸åŒçª—å£å¤§å°çš„åºå‚æ•°
            window_sizes = [10, 20, 30, 40, 50]
            order_parameters = []

            for window_size in window_sizes:
                if len(returns) >= window_size:
                    # çª—å£å†…çš„å¹³å‡ç»å¯¹æ”¶ç›Š
                    order_param = np.abs(returns.iloc[-window_size:]).mean()
                    order_parameters.append(order_param)

            if len(order_parameters) > 3:
                # æ‹Ÿåˆå¹‚å¾‹å…³ç³»
                log_windows = np.log(window_sizes[:len(order_parameters)])
                log_order_params = np.log(order_parameters)

                slope, _ = np.polyfit(log_windows, log_order_params, 1)

                # åºå‚æ•°æŒ‡æ•°
                beta = slope

                return max(-2.0, min(beta, 2.0))  # åˆç†èŒƒå›´é™åˆ¶

        except:
            pass

        return 0.0

    def _estimate_dynamical_exponent(self, prices: pd.Series) -> float:
        """ä¼°è®¡åŠ¨åŠ›å­¦æŒ‡æ•°"""
        try:
            returns = prices.pct_change().dropna()
            if len(returns) < 100:
                return 0.0

            # ä½¿ç”¨å¤šé‡åˆ†å½¢åˆ†æä¼°è®¡åŠ¨åŠ›å­¦æŒ‡æ•°
            q_values = [-2, -1, 0, 1, 2]
            tau_q = []

            for q in q_values:
                # ç®€åŒ–çš„å¤šé‡åˆ†å½¢è°±è®¡ç®—
                if q == 0:
                    continue

                # è®¡ç®—qé˜¶çŸ©
                absolute_returns = np.abs(returns)
                if len(absolute_returns) < 10:
                    continue

                # ä¸åŒæ—¶é—´å°ºåº¦çš„qé˜¶çŸ©
                scales = [5, 10, 15, 20, 25]
                q_moments = []

                for scale in scales:
                    if len(absolute_returns) >= scale:
                        # åˆ†å‰²åºåˆ—å¹¶è®¡ç®—qé˜¶çŸ©
                        segments = len(absolute_returns) // scale
                        if segments > 0:
                            moment_segments = []
                            for i in range(segments):
                                segment = absolute_returns.iloc[i*scale:(i+1)*scale]
                                if len(segment) > 0:
                                    segment_sum = np.sum(segment ** q)
                                    moment_segments.append(segment_sum)

                            if moment_segments:
                                q_moment = np.mean(moment_segments) ** (1.0/q)
                                q_moments.append(q_moment)

                if len(q_moments) > 3:
                    # æ‹Ÿåˆæ ‡åº¦å…³ç³»
                    log_scales = np.log(scales[:len(q_moments)])
                    log_q_moments = np.log(q_moments)

                    slope, _ = np.polyfit(log_scales, log_q_moments, 1)
                    tau_q.append(slope)

            if len(tau_q) > 2:
                # è®¡ç®—å¤šé‡åˆ†å½¢è°±
                q_values_used = [q for q in q_values if q != 0][:len(tau_q)]
                if len(q_values_used) > 2:
                    # ä¼°è®¡åŠ¨åŠ›å­¦æŒ‡æ•°
                    tau_q = np.array(tau_q)
                    q_array = np.array(q_values_used)

                    # çº¿æ€§å›å½’ä¼°è®¡
                    slope, _ = np.polyfit(q_array, tau_q, 1)
                    z = slope

                    # åŠ¨åŠ›å­¦æŒ‡æ•°
                    if z != 0:
                        dynamical_exponent = 1.0 / z
                        return max(0.1, min(dynamical_exponent, 10.0))

        except:
            pass

        return 1.0

    def _assess_system_stability(self, critical_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°ç³»ç»Ÿç¨³å®šæ€§"""
        stability = {}

        # ç»¼åˆç¨³å®šæ€§è¯„åˆ†
        stability_score = self._calculate_stability_score(critical_metrics)
        stability['overall_stability'] = stability_score

        # ç¨³å®šæ€§åˆ†ç±»
        if stability_score > 0.8:
            stability['stability_level'] = 'high_stability'
            stability['risk_level'] = 'low'
        elif stability_score > 0.6:
            stability['stability_level'] = 'moderate_stability'
            stability['risk_level'] = 'medium'
        elif stability_score > 0.4:
            stability['stability_level'] = 'low_stability'
            stability['risk_level'] = 'high'
        else:
            stability['stability_level'] = 'critical_instability'
            stability['risk_level'] = 'critical'

        # ç¨³å®šæ€§å› ç´ åˆ†æ
        stability['stability_factors'] = self._analyze_stability_factors(critical_metrics)

        # æ¢å¤åŠ›è¯„ä¼°
        stability['resilience_metrics'] = self._assess_resilience(critical_metrics)

        return stability

    def _calculate_stability_score(self, critical_metrics: Dict[str, Any]) -> float:
        """è®¡ç®—ç¨³å®šæ€§è¯„åˆ†"""
        stability_components = []

        # æ³¢åŠ¨ç‡ç¨³å®šæ€§
        if 'volatility_metrics' in critical_metrics:
            vol_metrics = critical_metrics['volatility_metrics']
            volatility = vol_metrics.get('volatility', 0)
            # ä½æ³¢åŠ¨ç‡è´¡çŒ®ç¨³å®šæ€§
            stability_components.append(max(0, 1.0 - volatility * 10))

        # ç›¸å…³æ€§ç¨³å®šæ€§
        if 'correlation_metrics' in critical_metrics:
            corr_metrics = critical_metrics['correlation_metrics']
            systemic_risk = corr_metrics.get('systemic_risk_measure', 0)
            # ä½ç³»ç»Ÿæ€§é£é™©è´¡çŒ®ç¨³å®šæ€§
            stability_components.append(max(0, 1.0 - systemic_risk))

        # å¤æ‚åº¦ç¨³å®šæ€§
        if 'information_metrics' in critical_metrics:
            info_metrics = critical_metrics['information_metrics']
            entropy = info_metrics.get('approximate_entropy', 0.5)
            # é€‚åº¦å¤æ‚åº¦è´¡çŒ®ç¨³å®šæ€§
            stability_components.append(1.0 - abs(entropy - 0.7))

        # ä¸´ç•Œæ¥è¿‘åº¦ç¨³å®šæ€§
        if 'critical_proximity' in critical_metrics:
            proximity = critical_metrics['critical_proximity']
            overall_criticality = proximity.get('overall_criticality', 0)
            # ä½ä¸´ç•Œæ€§è´¡çŒ®ç¨³å®šæ€§
            stability_components.append(max(0, 1.0 - overall_criticality))

        if stability_components:
            return np.mean(stability_components)

        return 0.5

    def _analyze_stability_factors(self, critical_metrics: Dict[str, Any]) -> Dict[str, float]:
        """åˆ†æç¨³å®šæ€§å› ç´ """
        factors = {}

        # æ³¢åŠ¨ç‡è´¡çŒ®
        if 'volatility_metrics' in critical_metrics:
            vol_metrics = critical_metrics['volatility_metrics']
            factors['volatility_contribution'] = min(vol_metrics.get('volatility', 0) * 10, 1.0)

        # ç›¸å…³æ€§è´¡çŒ®
        if 'correlation_metrics' in critical_metrics:
            corr_metrics = critical_metrics['correlation_metrics']
            factors['correlation_contribution'] = min(corr_metrics.get('systemic_risk_measure', 0), 1.0)

        # å¤æ‚åº¦è´¡çŒ®
        if 'information_metrics' in critical_metrics:
            info_metrics = critical_metrics['information_metrics']
            entropy = info_metrics.get('approximate_entropy', 0.5)
            factors['complexity_contribution'] = abs(entropy - 0.7)

        # ä¸´ç•Œæ€§è´¡çŒ®
        if 'critical_proximity' in critical_metrics:
            proximity = critical_metrics['critical_proximity']
            factors['criticality_contribution'] = proximity.get('overall_criticality', 0)

        return factors

    def _assess_resilience(self, critical_metrics: Dict[str, Any]) -> Dict[str, float]:
        """è¯„ä¼°æ¢å¤åŠ›"""
        resilience = {}

        # åŸºäºåˆ†å½¢ç»´æ•°çš„æ¢å¤åŠ›
        if 'fractal_metrics' in critical_metrics:
            fractal_metrics = critical_metrics['fractal_metrics']
            fractal_dim = fractal_metrics.get('fractal_dimension', 1.0)
            # é€‚ä¸­çš„åˆ†å½¢ç»´æ•°è¡¨ç¤ºè¾ƒå¥½çš„æ¢å¤åŠ›
            resilience['fractal_resilience'] = 1.0 - abs(fractal_dim - 1.5) / 1.5

        # åŸºäºèµ«æ–¯ç‰¹æŒ‡æ•°çš„æ¢å¤åŠ›
        if 'fractal_metrics' in critical_metrics:
            fractal_metrics = critical_metrics['fractal_metrics']
            hurst = fractal_metrics.get('hurst_exponent', 0.5)
            # æ¥è¿‘0.5çš„èµ«æ–¯ç‰¹æŒ‡æ•°è¡¨ç¤ºè¾ƒå¥½çš„æ¢å¤åŠ›
            resilience['hurst_resilience'] = 1.0 - abs(hurst - 0.5) * 2

        # åŸºäºç½‘ç»œæŒ‡æ ‡çš„æ¢å¤åŠ›
        if 'network_metrics' in critical_metrics:
            network_metrics = critical_metrics['network_metrics']
            clustering = network_metrics.get('clustering_coefficient', 0)
            # é«˜èšç±»ç³»æ•°è¡¨ç¤ºè¾ƒå¥½çš„æ¢å¤åŠ›
            resilience['network_resilience'] = clustering

        # ç»¼åˆæ¢å¤åŠ›
        resilience_values = [v for v in resilience.values() if isinstance(v, (int, float))]
        resilience['overall_resilience'] = np.mean(resilience_values) if resilience_values else 0.5

        return resilience

    def _predict_transition_risk(self, critical_metrics: Dict[str, Any], early_warnings: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é¢„æµ‹ç›¸å˜é£é™©"""
        risk_prediction = {}

        # åŸºäºä¸´ç•ŒæŒ‡æ ‡çš„é£é™©è¯„åˆ†
        criticality_score = self._calculate_composite_criticality(critical_metrics)

        # åŸºäºé¢„è­¦ä¿¡å·çš„é£é™©è¯„åˆ†
        warning_score = 0
        for warning in early_warnings:
            if warning['severity'] == 'high':
                warning_score += 0.4
            elif warning['severity'] == 'medium':
                warning_score += 0.2

        warning_score = min(warning_score, 1.0)

        # ç»¼åˆé£é™©è¯„åˆ†
        risk_prediction['overall_risk_score'] = (criticality_score + warning_score) / 2

        # é£é™©ç­‰çº§
        if risk_prediction['overall_risk_score'] > 0.8:
            risk_prediction['risk_level'] = 'critical'
            risk_prediction['probability'] = 0.9
        elif risk_prediction['overall_risk_score'] > 0.6:
            risk_prediction['risk_level'] = 'high'
            risk_prediction['probability'] = 0.7
        elif risk_prediction['overall_risk_score'] > 0.4:
            risk_prediction['risk_level'] = 'medium'
            risk_prediction['probability'] = 0.4
        else:
            risk_prediction['risk_level'] = 'low'
            risk_prediction['probability'] = 0.1

        # æ—¶é—´çª—å£é¢„æµ‹
        risk_prediction['expected_timeframe'] = self._predict_transition_timeframe(criticality_score, warning_score)

        # é£é™©å› ç´ åˆ†æ
        risk_prediction['risk_factors'] = self._analyze_risk_factors(critical_metrics, early_warnings)

        return risk_prediction

    def _predict_transition_timeframe(self, criticality_score: float, warning_score: float) -> str:
        """é¢„æµ‹ç›¸å˜æ—¶é—´æ¡†æ¶"""
        combined_score = (criticality_score + warning_score) / 2

        if combined_score > 0.8:
            return "immediate (0-5 days)"
        elif combined_score > 0.6:
            return "short_term (5-15 days)"
        elif combined_score > 0.4:
            return "medium_term (15-30 days)"
        else:
            return "long_term (30+ days)"

    def _analyze_risk_factors(self, critical_metrics: Dict[str, Any], early_warnings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆ†æé£é™©å› ç´ """
        risk_factors = []

        # ä¸´ç•Œæ¥è¿‘åº¦é£é™©
        if 'critical_proximity' in critical_metrics:
            proximity = critical_metrics['critical_proximity']
            for factor, value in proximity.items():
                if isinstance(value, (int, float)) and value > 0.5:
                    risk_factors.append({
                        'factor': f'critical_proximity_{factor}',
                        'contribution': value,
                        'type': 'criticality'
                    })

        # é¢„è­¦ä¿¡å·é£é™©
        for warning in early_warnings:
            risk_factors.append({
                'factor': warning['type'],
                'contribution': warning['value'] / warning['threshold'],
                'type': 'early_warning'
            })

        # æŒ‰è´¡çŒ®åº¦æ’åº
        risk_factors.sort(key=lambda x: x['contribution'], reverse=True)

        return risk_factors[:5]  # è¿”å›å‰5ä¸ªä¸»è¦é£é™©å› ç´ 

    def _generate_warning_report(self,
                              critical_metrics: Dict[str, Any],
                              early_warnings: List[Dict[str, Any]],
                              phase_transitions: List[Dict[str, Any]],
                              risk_prediction: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé¢„è­¦æŠ¥å‘Š"""
        report = {
            'warning_level': self.warning_level,
            'system_state': self.system_state,
            'critical_summary': self._generate_critical_summary(critical_metrics),
            'warning_summary': self._generate_warning_summary(early_warnings),
            'transition_summary': self._generate_transition_summary(phase_transitions),
            'risk_summary': self._generate_risk_summary(risk_prediction),
            'recommendations': self._generate_recommendations(),
            'confidence_assessment': self._assess_prediction_confidence(critical_metrics, early_warnings)
        }

        return report

    def _generate_critical_summary(self, critical_metrics: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¸´ç•ŒçŠ¶æ€æ‘˜è¦"""
        summary_parts = []

        # ä¸´ç•Œæ¥è¿‘åº¦
        if 'critical_proximity' in critical_metrics:
            proximity = critical_metrics['critical_proximity']
            overall_criticality = proximity.get('overall_criticality', 0)

            if overall_criticality > 0.8:
                summary_parts.append("ç³»ç»Ÿå¤„äºé«˜åº¦ä¸´ç•ŒçŠ¶æ€")
            elif overall_criticality > 0.6:
                summary_parts.append("ç³»ç»Ÿä¸´ç•Œæ€§æ˜¾è‘—å¢åŠ ")
            elif overall_criticality > 0.4:
                summary_parts.append("ç³»ç»Ÿä¸´ç•Œæ€§ä¸­ç­‰")
            else:
                summary_parts.append("ç³»ç»Ÿä¸´ç•Œæ€§è¾ƒä½")

        # æ³¢åŠ¨ç‡çŠ¶æ€
        if 'volatility_metrics' in critical_metrics:
            vol_metrics = critical_metrics['volatility_metrics']
            volatility = vol_metrics.get('volatility', 0)

            if volatility > 0.05:
                summary_parts.append("å¸‚åœºæ³¢åŠ¨ç‡å¼‚å¸¸é«˜")
            elif volatility > 0.02:
                summary_parts.append("å¸‚åœºæ³¢åŠ¨ç‡è¾ƒé«˜")
            else:
                summary_parts.append("å¸‚åœºæ³¢åŠ¨ç‡æ­£å¸¸")

        return "ï¼›".join(summary_parts)

    def _generate_warning_summary(self, early_warnings: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆé¢„è­¦æ‘˜è¦"""
        if not early_warnings:
            return "æœªæ£€æµ‹åˆ°æ˜æ˜¾é¢„è­¦ä¿¡å·"

        warning_types = [w['type'] for w in early_warnings]
        severe_warnings = [w for w in early_warnings if w['severity'] == 'high']

        if severe_warnings:
            return f"æ£€æµ‹åˆ°{len(severe_warnings)}ä¸ªé«˜çº§åˆ«é¢„è­¦ä¿¡å·ï¼š{', '.join([w['type'] for w in severe_warnings])}"
        else:
            return f"æ£€æµ‹åˆ°{len(early_warnings)}ä¸ªé¢„è­¦ä¿¡å·ï¼š{', '.join(warning_types)}"

    def _generate_transition_summary(self, phase_transitions: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆç›¸å˜æ‘˜è¦"""
        if not phase_transitions:
            return "å½“å‰æœªæ£€æµ‹åˆ°æ˜æ˜¾ç›¸å˜"

        transition = phase_transitions[0]  # å–æœ€ä¸»è¦çš„ç›¸å˜
        return f"æ£€æµ‹åˆ°{transition['type']}ï¼Œä¸¥é‡ç¨‹åº¦ï¼š{transition['severity']}"

    def _generate_risk_summary(self, risk_prediction: Dict[str, Any]) -> str:
        """ç”Ÿæˆé£é™©æ‘˜è¦"""
        risk_level = risk_prediction.get('risk_level', 'unknown')
        probability = risk_prediction.get('probability', 0)
        timeframe = risk_prediction.get('expected_timeframe', 'unknown')

        return f"ç›¸å˜é£é™©ç­‰çº§ï¼š{risk_level}ï¼Œå‘ç”Ÿæ¦‚ç‡ï¼š{probability:.1%}ï¼Œé¢„æœŸæ—¶é—´æ¡†æ¶ï¼š{timeframe}"

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []

        if self.warning_level >= 3:  # ä¸´ç•ŒçŠ¶æ€
            recommendations.extend([
                "ç«‹å³é‡‡å–ä¿æŠ¤æ€§æªæ–½",
                "è€ƒè™‘å‡å°‘é«˜é£é™©èµ„äº§æ•å£",
                "å¢åŠ ç°é‡‘å’Œé˜²å¾¡æ€§èµ„äº§é…ç½®",
                "è®¾ç½®ä¸¥æ ¼çš„æ­¢æŸç‚¹",
                "å¯†åˆ‡ç›‘æ§å¸‚åœºåŠ¨æ€"
            ])
        elif self.warning_level >= 2:  # è­¦å‘ŠçŠ¶æ€
            recommendations.extend([
                "é€‚åº¦é™ä½é£é™©æ•å£",
                "å¢åŠ æŠ•èµ„ç»„åˆå¤šæ ·æ€§",
                "è®¾ç½®é¢„è­¦å’Œæ­¢æŸæœºåˆ¶",
                "å…³æ³¨å¸‚åœºæƒ…ç»ªå˜åŒ–"
            ])
        elif self.warning_level >= 1:  # æ­£å¸¸çŠ¶æ€
            recommendations.extend([
                "ç»´æŒæ­£å¸¸æŠ•èµ„ç­–ç•¥",
                "ä¿æŒé€‚åº¦é£é™©æ•å£",
                "ç»§ç»­å¸¸è§„ç›‘æ§"
            ])

        return recommendations

    def _assess_prediction_confidence(self, critical_metrics: Dict[str, Any], early_warnings: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯„ä¼°é¢„æµ‹ç½®ä¿¡åº¦"""
        confidence_assessment = {}

        # æ•°æ®è´¨é‡è¯„ä¼°
        data_quality = self._assess_data_quality(critical_metrics)
        confidence_assessment['data_quality'] = data_quality

        # ä¿¡å·å¼ºåº¦è¯„ä¼°
        signal_strength = self._assess_signal_strength(critical_metrics, early_warnings)
        confidence_assessment['signal_strength'] = signal_strength

        # æ¨¡å‹ä¸€è‡´æ€§è¯„ä¼°
        model_consistency = self._assess_model_consistency(critical_metrics)
        confidence_assessment['model_consistency'] = model_consistency

        # ç»¼åˆç½®ä¿¡åº¦
        confidence_factors = [data_quality, signal_strength, model_consistency]
        confidence_assessment['overall_confidence'] = np.mean(confidence_factors)

        # ç½®ä¿¡åº¦ç­‰çº§
        if confidence_assessment['overall_confidence'] > 0.8:
            confidence_assessment['confidence_level'] = 'high'
        elif confidence_assessment['overall_confidence'] > 0.6:
            confidence_assessment['confidence_level'] = 'medium'
        else:
            confidence_assessment['confidence_level'] = 'low'

        return confidence_assessment

    def _assess_data_quality(self, critical_metrics: Dict[str, Any]) -> float:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        # åŸºäºæŒ‡æ ‡å®Œæ•´æ€§è¯„ä¼°æ•°æ®è´¨é‡
        total_metrics = 0
        available_metrics = 0

        metric_categories = [
            'volatility_metrics', 'correlation_metrics', 'fractal_metrics',
            'information_metrics', 'critical_proximity', 'order_statistics'
        ]

        for category in metric_categories:
            total_metrics += 1
            if category in critical_metrics and critical_metrics[category]:
                available_metrics += 1

        return available_metrics / total_metrics if total_metrics > 0 else 0.0

    def _assess_signal_strength(self, critical_metrics: Dict[str, Any], early_warnings: List[Dict[str, Any]]) -> float:
        """è¯„ä¼°ä¿¡å·å¼ºåº¦"""
        # åŸºäºé¢„è­¦ä¿¡å·å¼ºåº¦è¯„ä¼°
        if not early_warnings:
            return 0.0

        signal_strength = 0
        for warning in early_warnings:
            if warning['severity'] == 'high':
                signal_strength += 0.5
            elif warning['severity'] == 'medium':
                signal_strength += 0.3

        return min(signal_strength, 1.0)

    def _assess_model_consistency(self, critical_metrics: Dict[str, Any]) -> float:
        """è¯„ä¼°æ¨¡å‹ä¸€è‡´æ€§"""
        # åŸºäºä¸åŒæŒ‡æ ‡ä¹‹é—´çš„ä¸€è‡´æ€§è¯„ä¼°
        consistency_score = 0.0

        # æ£€æŸ¥ä¸åŒæŒ‡æ ‡ç»„ä¹‹é—´çš„é€»è¾‘ä¸€è‡´æ€§
        volatility_high = False
        correlation_high = False
        criticality_high = False

        if 'volatility_metrics' in critical_metrics:
            vol_metrics = critical_metrics['volatility_metrics']
            if vol_metrics.get('volatility', 0) > 0.03:
                volatility_high = True

        if 'correlation_metrics' in critical_metrics:
            corr_metrics = critical_metrics['correlation_metrics']
            if corr_metrics.get('systemic_risk_measure', 0) > 0.7:
                correlation_high = True

        if 'critical_proximity' in critical_metrics:
            proximity = critical_metrics['critical_proximity']
            if proximity.get('overall_criticality', 0) > 0.6:
                criticality_high = True

        # è¯„ä¼°ä¸€è‡´æ€§
        aligned_signals = sum([volatility_high, correlation_high, criticality_high])
        total_signals = 3

        if aligned_signals == total_signals or aligned_signals == 0:
            consistency_score = 1.0  # å®Œå…¨ä¸€è‡´
        elif aligned_signals == 2:
            consistency_score = 0.7  # å¤§éƒ¨åˆ†ä¸€è‡´
        elif aligned_signals == 1:
            consistency_score = 0.3  # éƒ¨åˆ†ä¸€è‡´
        else:
            consistency_score = 0.0  # ä¸ä¸€è‡´

        return consistency_score

    def get_analysis_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ€»ç»“"""
        return {
            'window_size': self.window_size,
            'detection_threshold': self.detection_threshold,
            'early_warning_threshold': self.early_warning_threshold,
            'sensitivity': self.sensitivity,
            'current_warning_level': self.warning_level,
            'current_system_state': self.system_state,
            'total_analysis_sessions': len(self.phase_transition_history),
            'phase_transition_detected': any(pt['type'] == 'critical_transition' for pt in self.phase_transition_history),
            'model_info': self.get_model_info()
        }

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_type': 'Critical Phase Transition Analyzer',
            'torch_available': TORCH_AVAILABLE,
            'scipy_available': SCIPY_AVAILABLE,
            'networkx_available': NETWORKX_AVAILABLE,
            'window_size': self.window_size,
            'detection_threshold': self.detection_threshold,
            'early_warning_threshold': self.early_warning_threshold,
            'sensitivity': self.sensitivity,
            'phase_detection_model_available': self.phase_detection_model is not None,
            'early_warning_model_available': self.early_warning_model is not None
        }


class CriticalPhaseDetector(nn.Module):
    """ä¸´ç•Œç›¸å˜æ£€æµ‹æ¨¡å‹"""
    def __init__(self, input_dim: int, hidden_dims: List[int], sensitivity: float = 0.8):
        super().__init__()
        self.input_dim = input_dim
        self.sensitivity = sensitivity

        # ç¥ç»ç½‘ç»œå±‚
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim

        # è¾“å‡ºå±‚ï¼šä¸´ç•Œæ¦‚ç‡
        layers.append(nn.Linear(prev_dim, 1))
        layers.append(nn.Sigmoid())

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


class EarlyWarningPredictor(nn.Module):
    """æ—©æœŸé¢„è­¦é¢„æµ‹æ¨¡å‹"""
    def __init__(self, window_size: int, prediction_horizon: int):
        super().__init__()
        self.window_size = window_size
        self.prediction_horizon = prediction_horizon

        # LSTMå±‚ç”¨äºæ—¶åºé¢„æµ‹
        self.lstm = nn.LSTM(input_size=10, hidden_size=32, num_layers=2, batch_first=True)
        self.fc = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x shape: (batch_size, window_size, features)
        lstm_out, _ = self.lstm(x)
        prediction = self.sigmoid(self.fc(lstm_out[:, -1, :]))
        return prediction


# ä¾¿æ·å‡½æ•°
def create_critical_transition_analyzer(window_size: int = 100) -> CriticalTransitionAnalyzer:
    """åˆ›å»ºä¸´ç•Œç›¸å˜åˆ†æå™¨å®ä¾‹"""
    return CriticalTransitionAnalyzer(window_size=window_size)

def quick_phase_analysis(market_data: pd.DataFrame) -> Dict[str, Any]:
    """å¿«é€Ÿç›¸å˜åˆ†æ"""
    analyzer = create_critical_transition_analyzer()
    return analyzer.analyze_market_phase_transition(market_data)

def get_critical_warning_level(market_data: pd.DataFrame) -> str:
    """è·å–ä¸´ç•Œé¢„è­¦ç­‰çº§"""
    results = quick_phase_analysis(market_data)
    warning_level = results.get('warning_level', 0)

    level_descriptions = {
        0: 'normal',
        1: 'normal',
        2: 'warning',
        3: 'critical'
    }

    return level_descriptions.get(warning_level, 'unknown')