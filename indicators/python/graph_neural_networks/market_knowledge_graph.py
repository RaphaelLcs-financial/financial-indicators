"""
å¸‚åœºçŸ¥è¯†å›¾è°±å›¾ç¥ç»ç½‘ç»œæŒ‡æ ‡
Market Knowledge Graph with Graph Neural Networks

åŸºäºå›¾ç¥ç»ç½‘ç»œå’ŒçŸ¥è¯†å›¾è°±çš„å¸‚åœºå…³ç³»åˆ†æå’Œé¢„æµ‹
"""

import numpy as np
import pandas as pd
import networkx as nx
from typing import Dict, Any, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch_geometric.data import Data, DataLoader
    from torch_geometric.nn import GCNConv, GATConv, GraphSAGE
    TORCH_GEOMETRIC_AVAILABLE = True
    print("ğŸ”— PyTorch Geometric å·²å¯ç”¨")
except ImportError:
    TORCH_GEOMETRIC_AVAILABLE = False
    print("âš ï¸ PyTorch Geometric ä¸å¯ç”¨")

class MarketKnowledgeGraph:
    """
    å¸‚åœºçŸ¥è¯†å›¾è°±åˆ†æå™¨

    åŸºäºå›¾ç¥ç»ç½‘ç»œåˆ†æèµ„äº§é—´çš„å¤æ‚å…³ç³»å’Œä¾èµ–æ€§
    æ„å»ºé‡‘èçŸ¥è¯†å›¾è°±è¿›è¡Œå¸‚åœºé¢„æµ‹å’Œé£é™©è¯„ä¼°
    """

    def __init__(self, graph_type: str = 'correlation', gnn_type: str = 'gcn',
                 hidden_dim: int = 64, num_layers: int = 3):
        """
        åˆå§‹åŒ–å¸‚åœºçŸ¥è¯†å›¾è°±

        Args:
            graph_type: å›¾ç±»å‹ ('correlation', 'causal', 'semantic')
            gnn_type: GNNç±»å‹ ('gcn', 'gat', 'graphsage')
            hidden_dim: éšè—å±‚ç»´åº¦
            num_layers: ç½‘ç»œå±‚æ•°
        """
        self.graph_type = graph_type
        self.gnn_type = gnn_type
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # å›¾æ•°æ®
        self.graph = nx.Graph()
        self.node_features = {}
        self.edge_features = {}

        # GNNæ¨¡å‹
        self.gnn_model = None
        self.is_trained = False

        # çŸ¥è¯†å›¾è°±
        self.knowledge_graph = {}
        self.entity_types = {}
        self.relation_types = {}

        # åˆ†æç»“æœ
        self.analysis_history = []

    def build_correlation_graph(self, data: pd.DataFrame, window: int = 20) -> nx.Graph:
        """æ„å»ºç›¸å…³æ€§å›¾"""
        print("ğŸ“Š æ„å»ºå¸‚åœºç›¸å…³æ€§å›¾...")

        # è®¡ç®—æ»šåŠ¨ç›¸å…³æ€§
        returns = data.pct_change().fillna(0)
        correlation_matrix = returns.rolling(window=window).corr().dropna()

        # åˆ›å»ºå›¾
        G = nx.Graph()

        # æ·»åŠ èŠ‚ç‚¹ï¼ˆèµ„äº§ï¼‰
        assets = correlation_matrix.columns
        for asset in assets:
            G.add_node(asset)

        # æ·»åŠ è¾¹ï¼ˆç›¸å…³æ€§ï¼‰
        threshold = 0.5  # ç›¸å…³æ€§é˜ˆå€¼
        for i, asset1 in enumerate(assets):
            for j, asset2 in enumerate(assets[i+1:], i+1):
                # ä½¿ç”¨æœ€æ–°ç›¸å…³æ€§
                latest_corr = correlation_matrix.iloc[-1, correlation_matrix.columns.get_loc(asset2)]
                if abs(latest_corr) > threshold:
                    G.add_edge(asset1, asset2, weight=latest_corr, correlation_type='positive' if latest_corr > 0 else 'negative')

        return G

    def build_semantic_graph(self, data: pd.DataFrame, entity_info: Dict[str, str]) -> nx.Graph:
        """æ„å»ºè¯­ä¹‰å›¾"""
        print("ğŸ§  æ„å»ºå¸‚åœºè¯­ä¹‰å›¾...")

        G = nx.Graph()

        # æ·»åŠ èŠ‚ç‚¹
        for entity, entity_type in entity_info.items():
            G.add_node(entity, type=entity_type)
            self.entity_types[entity] = entity_type

        # åŸºäºè¯­ä¹‰å…³ç³»æ·»åŠ è¾¹
        # è¿™é‡Œå¯ä»¥åŸºäºè¡Œä¸šã€æ¿å—ã€åœ°åŸŸç­‰è¯­ä¹‰å…³ç³»
        for entity1, type1 in entity_info.items():
            for entity2, type2 in entity_info.items():
                if entity1 != entity2:
                    # ç®€å•çš„è¯­ä¹‰å…³ç³»ï¼šåŒç±»å‹å®ä½“æœ‰æ›´å¼ºè¿æ¥
                    if type1 == type2:
                        G.add_edge(entity1, entity2, weight=0.8, relation_type='same_type')
                    else:
                        G.add_edge(entity1, entity2, weight=0.3, relation_type='different_type')

        return G

    def create_gnn_model(self, num_node_features: int, num_classes: int = 3):
        """åˆ›å»ºGNNæ¨¡å‹"""
        if not TORCH_GEOMETRIC_AVAILABLE:
            raise ImportError("PyTorch Geometric is required for GNN models")

        class GNNModel(nn.Module):
            def __init__(self, num_features, hidden_dim, num_classes, gnn_type, num_layers):
                super(GNNModel, self).__init__()

                self.gnn_type = gnn_type
                self.num_layers = num_layers

                # è¾“å…¥å±‚
                if gnn_type == 'gcn':
                    self.conv1 = GCNConv(num_features, hidden_dim)
                elif gnn_type == 'gat':
                    self.conv1 = GATConv(num_features, hidden_dim, heads=4, concat=False)
                elif gnn_type == 'graphsage':
                    self.conv1 = GraphSAGE(num_features, hidden_dim)

                # éšè—å±‚
                self.convs = nn.ModuleList()
                for _ in range(num_layers - 2):
                    if gnn_type == 'gcn':
                        self.convs.append(GCNConv(hidden_dim, hidden_dim))
                    elif gnn_type == 'gat':
                        self.convs.append(GATConv(hidden_dim, hidden_dim, heads=4, concat=False))
                    elif gnn_type == 'graphsage':
                        self.convs.append(GraphSAGE(hidden_dim, hidden_dim))

                # è¾“å‡ºå±‚
                if gnn_type == 'gcn':
                    self.conv_out = GCNConv(hidden_dim, num_classes)
                elif gnn_type == 'gat':
                    self.conv_out = GATConv(hidden_dim, num_classes, heads=1, concat=False)
                elif gnn_type == 'graphsage':
                    self.conv_out = GraphSAGE(hidden_dim, num_classes)

            def forward(self, x, edge_index):
                # è¾“å…¥å±‚
                x = F.relu(self.conv1(x, edge_index))
                x = F.dropout(x, p=0.2, training=self.training)

                # éšè—å±‚
                for conv in self.convs:
                    x = F.relu(conv(x, edge_index))
                    x = F.dropout(x, p=0.2, training=self.training)

                # è¾“å‡ºå±‚
                x = self.conv_out(x, edge_index)
                return F.log_softmax(x, dim=1)

        self.gnn_model = GNNModel(
            num_node_features=num_node_features,
            hidden_dim=hidden_dim,
            num_classes=num_classes,
            gnn_type=self.gnn_type,
            num_layers=self.num_layers
        )

    def extract_node_features(self, data: pd.DataFrame) -> Dict[str, np.ndarray]:
        """æå–èŠ‚ç‚¹ç‰¹å¾"""
        features = {}

        for asset in data.columns:
            asset_data = data[asset]

            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            returns = asset_data.pct_change().fillna(0)
            features[asset] = np.array([
                np.mean(returns),           # å¹³å‡æ”¶ç›Š
                np.std(returns),            # æ³¢åŠ¨ç‡
                np.min(returns),            # æœ€å°æ”¶ç›Š
                np.max(returns),            # æœ€å¤§æ”¶ç›Š
                np.median(returns),         # ä¸­ä½æ•°æ”¶ç›Š
                np.skew(returns),           # ååº¦
                np.kurtosis(returns),       # å³°åº¦
                np.percentile(returns, 5),  # 5%åˆ†ä½æ•°
                np.percentile(returns, 95), # 95%åˆ†ä½æ•°
                len(returns[returns > 0]) / len(returns),  # æ­£æ”¶ç›Šæ¯”ä¾‹
            ])

        return features

    def prepare_graph_data(self, graph: nx.Graph, node_features: Dict[str, np.ndarray]) -> Data:
        """å‡†å¤‡å›¾æ•°æ®"""
        # èŠ‚ç‚¹æ˜ å°„
        node_mapping = {node: i for i, node in enumerate(graph.nodes())}

        # èŠ‚ç‚¹ç‰¹å¾
        feature_matrix = np.array([node_features[node] for node in graph.nodes()])

        # è¾¹ç´¢å¼•
        edge_index = []
        for edge in graph.edges():
            edge_index.append([node_mapping[edge[0]], node_mapping[edge[1]]])
            edge_index.append([node_mapping[edge[1]], node_mapping[edge[0]]])  # æ— å‘å›¾

        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()

        # è¾¹æƒé‡
        edge_weights = []
        for edge in graph.edges():
            weight = graph.edges[edge].get('weight', 1.0)
            edge_weights.extend([weight, weight])

        edge_weights = torch.tensor(edge_weights, dtype=torch.float)

        return Data(
            x=torch.tensor(feature_matrix, dtype=torch.float),
            edge_index=edge_index,
            edge_attr=edge_weights
        )

    def train_gnn_model(self, data: Data, labels: np.ndarray, epochs: int = 100) -> Dict[str, Any]:
        """è®­ç»ƒGNNæ¨¡å‹"""
        if not TORCH_GEOMETRIC_AVAILABLE:
            raise ImportError("PyTorch Geometric is required for GNN training")

        print("ğŸ§  å¼€å§‹è®­ç»ƒGNNæ¨¡å‹...")

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        data = data.to(device)

        # åˆ›å»ºæ¨¡å‹
        self.create_gnn_model(
            num_node_features=data.x.shape[1],
            num_classes=len(np.unique(labels))
        )

        model = self.gnn_model.to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.NLLLoss()

        # è®­ç»ƒå¾ªç¯
        model.train()
        training_loss = []

        for epoch in range(epochs):
            optimizer.zero_grad()
            out = model(data.x, data.edge_index)

            # åªä½¿ç”¨æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹è¿›è¡Œè®­ç»ƒ
            labeled_mask = torch.tensor(labels != -1, dtype=torch.bool)
            if labeled_mask.sum() > 0:
                loss = criterion(out[labeled_mask], torch.tensor(labels[labeled_mask], dtype=torch.long).to(device))
                loss.backward()
                optimizer.step()
                training_loss.append(loss.item())

            if epoch % 20 == 0:
                print(f"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}")

        self.is_trained = True

        return {
            'final_loss': training_loss[-1] if training_loss else 0,
            'training_loss': training_loss,
            'model_parameters': sum(p.numel() for p in model.parameters())
        }

    def analyze_market_structure(self, graph: nx.Graph) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºç»“æ„"""
        print("ğŸ—ï¸ åˆ†æå¸‚åœºæ‹“æ‰‘ç»“æ„...")

        # åŸºç¡€å›¾æŒ‡æ ‡
        analysis = {
            'num_nodes': graph.number_of_nodes(),
            'num_edges': graph.number_of_edges(),
            'density': nx.density(graph),
            'average_degree': np.mean([d for n, d in graph.degree()]),
            'clustering_coefficient': nx.average_clustering(graph),
            'diameter': nx.diameter(graph) if nx.is_connected(graph) else float('inf'),
            'average_path_length': nx.average_shortest_path_length(graph) if nx.is_connected(graph) else 0
        }

        # ä¸­å¿ƒæ€§åˆ†æ
        centrality_measures = {
            'degree_centrality': nx.degree_centrality(graph),
            'betweenness_centrality': nx.betweenness_centrality(graph),
            'closeness_centrality': nx.closeness_centrality(graph),
            'eigenvector_centrality': nx.eigenvector_centrality_numpy(graph)
        }

        analysis['centrality'] = centrality_measures

        # ç¤¾åŒºæ£€æµ‹
        try:
            communities = nx.community.greedy_modularity_communities(graph)
            analysis['communities'] = {
                'num_communities': len(communities),
                'modularity': nx.community.modularity(graph, communities),
                'community_sizes': [len(c) for c in communities]
            }
        except:
            analysis['communities'] = {'error': 'Community detection failed'}

        # ç³»ç»Ÿæ€§é£é™©åˆ†æ
        analysis['systemic_risk'] = self._calculate_systemic_risk(graph)

        return analysis

    def _calculate_systemic_risk(self, graph: nx.Graph) -> Dict[str, float]:
        """è®¡ç®—ç³»ç»Ÿæ€§é£é™©"""
        risk_metrics = {}

        # è¿é€šæ€§é£é™©
        if nx.is_connected(graph):
            # ç§»é™¤å…³é”®èŠ‚ç‚¹çš„å½±å“
            centrality = nx.degree_centrality(graph)
            critical_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]

            fragmentation_scores = []
            for node, _ in critical_nodes:
                temp_graph = graph.copy()
                temp_graph.remove_node(node)
                if nx.is_connected(temp_graph):
                    fragmentation_scores.append(1.0)
                else:
                    # è®¡ç®—è¿é€šåˆ†é‡å¤§å°
                    components = list(nx.connected_components(temp_graph))
                    largest_component_size = len(max(components, key=len))
                    fragmentation = largest_component_size / len(temp_graph.nodes())
                    fragmentation_scores.append(fragmentation)

            risk_metrics['node_removal_impact'] = np.mean(fragmentation_scores)
        else:
            risk_metrics['node_removal_impact'] = 0

        # è¾¹å¯†åº¦é£é™©
        risk_metrics['edge_density_risk'] = 1 - nx.density(graph)

        # é›†ä¸­åº¦é£é™©
        degrees = [d for n, d in graph.degree()]
        risk_metrics['concentration_risk'] = np.std(degrees) / np.mean(degrees) if np.mean(degrees) > 0 else 0

        return risk_metrics

    def predict_market_movements(self, data: pd.DataFrame, horizon: int = 5) -> Dict[str, Any]:
        """é¢„æµ‹å¸‚åœºèµ°åŠ¿"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train_gnn_modelæ–¹æ³•")

        print("ğŸ”® é¢„æµ‹å¸‚åœºèµ°åŠ¿...")

        # æ„å»ºå›¾
        if self.graph_type == 'correlation':
            graph = self.build_correlation_graph(data)
        else:
            graph = self.build_semantic_graph(data, {})

        # æå–ç‰¹å¾
        node_features = self.extract_node_features(data)
        graph_data = self.prepare_graph_data(graph, node_features)

        # ä½¿ç”¨GNNé¢„æµ‹
        if TORCH_GEOMETRIC_AVAILABLE and self.gnn_model is not None:
            self.gnn_model.eval()
            with torch.no_grad():
                predictions = self.gnn_model(graph_data.x, graph_data.edge_index)
                predicted_labels = torch.argmax(predictions, dim=1).cpu().numpy()
                probabilities = torch.exp(predictions).cpu().numpy()
        else:
            # ç®€åŒ–çš„é¢„æµ‹æ–¹æ³•
            predicted_labels = np.zeros(len(graph.nodes()))
            probabilities = np.ones((len(graph.nodes()), 3)) / 3

        # åˆ†æé¢„æµ‹ç»“æœ
        prediction_analysis = {
            'predicted_labels': dict(zip(graph.nodes(), predicted_labels)),
            'probabilities': dict(zip(graph.nodes(), probabilities)),
            'market_sentiment': self._analyze_market_sentiment(predicted_labels),
            'risk_assessment': self._assess_prediction_risk(probabilities)
        }

        return prediction_analysis

    def _analyze_market_sentiment(self, labels: np.ndarray) -> Dict[str, float]:
        """åˆ†æå¸‚åœºæƒ…ç»ª"""
        unique_labels, counts = np.unique(labels, return_counts=True)
        total = len(labels)

        sentiment = {}
        for label, count in zip(unique_labels, counts):
            if label == 0:
                sentiment['neutral'] = count / total
            elif label == 1:
                sentiment['bullish'] = count / total
            elif label == 2:
                sentiment['bearish'] = count / total

        return sentiment

    def _assess_prediction_risk(self, probabilities: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°é¢„æµ‹é£é™©"""
        # è®¡ç®—é¢„æµ‹ä¸ç¡®å®šæ€§
        uncertainty = 1 - np.max(probabilities, axis=1)
        avg_uncertainty = np.mean(uncertainty)

        # è®¡ç®—ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_scores = np.max(probabilities, axis=1)
        low_confidence_ratio = (confidence_scores < 0.6).mean()

        return {
            'average_uncertainty': avg_uncertainty,
            'low_confidence_ratio': low_confidence_ratio,
            'prediction_reliability': 1 - avg_uncertainty
        }

    def detect_anomalies(self, data: pd.DataFrame, threshold: float = 2.0) -> Dict[str, Any]:
        """æ£€æµ‹å¸‚åœºå¼‚å¸¸"""
        print("ğŸš¨ æ£€æµ‹å¸‚åœºå¼‚å¸¸...")

        # æ„å»ºå›¾
        graph = self.build_correlation_graph(data)

        # è®¡ç®—èŠ‚ç‚¹å¼‚å¸¸åˆ†æ•°
        anomaly_scores = {}

        for node in graph.nodes():
            # åŸºäºä¸­å¿ƒæ€§çš„å¼‚å¸¸æ£€æµ‹
            degree_centrality = nx.degree_centrality(graph)[node]
            betweenness_centrality = nx.betweenness_centrality(graph)[node]

            # åŸºäºå±€éƒ¨ç»“æ„çš„å¼‚å¸¸æ£€æµ‹
            neighbors = list(graph.neighbors(node))
            if neighbors:
                neighbor_degrees = [graph.degree(n) for n in neighbors]
                degree_anomaly = abs(graph.degree(node) - np.mean(neighbor_degrees)) / (np.std(neighbor_degrees) + 1e-6)
            else:
                degree_anomaly = 0

            # ç»¼åˆå¼‚å¸¸åˆ†æ•°
            anomaly_score = (degree_anomaly + betweenness_centrality * 2) / 3
            anomaly_scores[node] = anomaly_score

        # è¯†åˆ«å¼‚å¸¸èŠ‚ç‚¹
        threshold_score = np.mean(list(anomaly_scores.values())) + threshold * np.std(list(anomaly_scores.values()))
        anomalies = {node: score for node, score in anomaly_scores.items() if score > threshold_score}

        return {
            'anomaly_scores': anomaly_scores,
            'anomalies': anomalies,
            'num_anomalies': len(anomalies),
            'anomaly_threshold': threshold_score,
            'anomaly_severity': self._classify_anomaly_severity(anomalies)
        }

    def _classify_anomaly_severity(self, anomalies: Dict[str, float]) -> Dict[str, int]:
        """åˆ†ç±»å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        if not anomalies:
            return {'low': 0, 'medium': 0, 'high': 0}

        scores = list(anomalies.values())
        mean_score = np.mean(scores)

        severity = {
            'low': len([s for s in scores if s < mean_score]),
            'medium': len([s for s in scores if mean_score <= s < mean_score * 1.5]),
            'high': len([s for s in scores if s >= mean_score * 1.5])
        }

        return severity

    def analyze_market_regimes(self, data: pd.DataFrame, window: int = 20) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºæœºåˆ¶"""
        print("ğŸ“ˆ åˆ†æå¸‚åœºæœºåˆ¶...")

        # æ»šåŠ¨çª—å£åˆ†æ
        regime_history = []

        for i in range(window, len(data)):
            window_data = data.iloc[i-window:i]

            # æ„å»ºçª—å£å›¾
            graph = self.build_correlation_graph(window_data)

            # è®¡ç®—å›¾ç‰¹å¾
            graph_features = {
                'density': nx.density(graph),
                'clustering': nx.average_clustering(graph),
                'average_path_length': nx.average_shortest_path_length(graph) if nx.is_connected(graph) else 0,
                'assortativity': nx.degree_assortativity_coefficient(graph) if len(graph.edges()) > 0 else 0
            }

            # è¯†åˆ«æœºåˆ¶
            regime = self._identify_regime(graph_features)

            regime_history.append({
                'date': data.index[i],
                'regime': regime,
                'features': graph_features
            })

        # åˆ†ææœºåˆ¶è½¬æ¢
        regime_transitions = self._analyze_regime_transitions(regime_history)

        return {
            'regime_history': regime_history,
            'current_regime': regime_history[-1]['regime'] if regime_history else 'unknown',
            'regime_transitions': regime_transitions,
            'regime_stability': self._calculate_regime_stability(regime_history)
        }

    def _identify_regime(self, features: Dict[str, float]) -> str:
        """è¯†åˆ«å¸‚åœºæœºåˆ¶"""
        density = features.get('density', 0)
        clustering = features.get('clustering', 0)
        path_length = features.get('average_path_length', 0)

        if density > 0.7 and clustering > 0.6:
            return 'high_correlation'
        elif density < 0.3 and path_length > 2.0:
            return 'low_correlation'
        elif clustering > 0.5:
            return 'clustered'
        else:
            return 'normal'

    def _analyze_regime_transitions(self, history: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææœºåˆ¶è½¬æ¢"""
        if len(history) < 2:
            return {'transitions': [], 'stability_score': 0}

        transitions = []
        for i in range(1, len(history)):
            if history[i]['regime'] != history[i-1]['regime']:
                transitions.append({
                    'from_regime': history[i-1]['regime'],
                    'to_regime': history[i]['regime'],
                    'date': history[i]['date']
                })

        # è®¡ç®—ç¨³å®šæ€§
        regime_changes = len(transitions)
        total_periods = len(history)
        stability_score = 1 - (regime_changes / total_periods)

        return {
            'transitions': transitions,
            'num_transitions': len(transitions),
            'stability_score': stability_score
        }

    def _calculate_regime_stability(self, history: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—æœºåˆ¶ç¨³å®šæ€§"""
        if not history:
            return {'overall_stability': 0, 'regime_durations': {}}

        # è®¡ç®—å„æœºåˆ¶æŒç»­æ—¶é—´
        regime_durations = {}
        current_regime = None
        current_duration = 0

        for period in history:
            regime = period['regime']
            if regime == current_regime:
                current_duration += 1
            else:
                if current_regime is not None:
                    if current_regime not in regime_durations:
                        regime_durations[current_regime] = []
                    regime_durations[current_regime].append(current_duration)
                current_regime = regime
                current_duration = 1

        # æ·»åŠ æœ€åä¸€ä¸ªæœºåˆ¶
        if current_regime is not None:
            if current_regime not in regime_durations:
                regime_durations[current_regime] = []
            regime_durations[current_regime].append(current_duration)

        # è®¡ç®—å¹³å‡æŒç»­æ—¶é—´
        avg_durations = {}
        for regime, durations in regime_durations.items():
            avg_durations[regime] = np.mean(durations)

        overall_stability = np.mean(list(avg_durations.values())) if avg_durations else 0

        return {
            'overall_stability': overall_stability,
            'average_regime_durations': avg_durations,
            'regime_distributions': {k: len(v) for k, v in regime_durations.items()}
        }

    def get_trading_signals(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ç›¸å…³è®­ç»ƒæ–¹æ³•")

        # å¸‚åœºç»“æ„åˆ†æ
        graph = self.build_correlation_graph(data)
        structure_analysis = self.analyze_market_structure(graph)

        # å¼‚å¸¸æ£€æµ‹
        anomaly_analysis = self.detect_anomalies(data)

        # æœºåˆ¶åˆ†æ
        regime_analysis = self.analyze_market_regimes(data)

        # ç”Ÿæˆç»¼åˆä¿¡å·
        signals = []

        for i, date in enumerate(data.index):
            signal = self._generate_composite_signal(
                structure_analysis,
                anomaly_analysis,
                regime_analysis,
                i,
                len(data)
            )
            signals.append(signal)

        signals_df = pd.DataFrame({
            'signal': signals,
            'market_regime': regime_analysis['current_regime'],
            'anomaly_count': anomaly_analysis['num_anomalies'],
            'systemic_risk': structure_analysis['systemic_risk'].get('concentration_risk', 0)
        }, index=data.index)

        return signals_df

    def _generate_composite_signal(self, structure_analysis: Dict, anomaly_analysis: Dict,
                                 regime_analysis: Dict, current_idx: int, total_length: int) -> int:
        """ç”Ÿæˆç»¼åˆäº¤æ˜“ä¿¡å·"""
        signal = 0

        # åŸºäºå¸‚åœºæœºåˆ¶çš„ä¿¡å·
        current_regime = regime_analysis['current_regime']
        regime_signals = {
            'high_correlation': -1,  # é«˜ç›¸å…³æ€§å‡ä»“
            'low_correlation': 0,    # ä½ç›¸å…³æ€§è§‚æœ›
            'clustered': 0,         # é›†ç¾¤çŠ¶æ€è§‚æœ›
            'normal': 1             # æ­£å¸¸çŠ¶æ€é€‚é‡å‚ä¸
        }
        signal += regime_signals.get(current_regime, 0)

        # åŸºäºå¼‚å¸¸çš„ä¿¡å·
        anomaly_count = anomaly_analysis['num_anomalies']
        if anomaly_count > len(structure_analysis.get('centrality', {}).get('degree_centrality', {})) * 0.2:
            signal -= 1  # å¼‚å¸¸è¾ƒå¤šæ—¶å‡ä»“

        # åŸºäºç³»ç»Ÿæ€§é£é™©çš„ä¿¡å·
        systemic_risk = structure_analysis['systemic_risk'].get('concentration_risk', 0)
        if systemic_risk > 1.0:
            signal -= 1
        elif systemic_risk < 0.5:
            signal += 1

        # é™åˆ¶ä¿¡å·èŒƒå›´
        return max(-1, min(1, signal))

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'graph_type': self.graph_type,
            'gnn_type': self.gnn_type,
            'hidden_dim': self.hidden_dim,
            'num_layers': self.num_layers,
            'is_trained': self.is_trained,
            'torch_geometric_available': TORCH_GEOMETRIC_AVAILABLE,
            'graph_stats': {
                'num_nodes': self.graph.number_of_nodes(),
                'num_edges': self.graph.number_of_edges(),
                'density': nx.density(self.graph) if len(self.graph.nodes()) > 0 else 0
            },
            'model_type': 'Market Knowledge Graph with GNN'
        }

# ä¾¿æ·å‡½æ•°
def create_market_knowledge_graph(graph_type: str = 'correlation', gnn_type: str = 'gcn') -> MarketKnowledgeGraph:
    """åˆ›å»ºå¸‚åœºçŸ¥è¯†å›¾è°±å®ä¾‹"""
    return MarketKnowledgeGraph(graph_type, gnn_type)

def quick_graph_analysis(data: pd.DataFrame) -> Dict[str, Any]:
    """å¿«é€Ÿå›¾åˆ†æ"""
    graph_analyzer = MarketKnowledgeGraph()

    # æ„å»ºå›¾
    graph = graph_analyzer.build_correlation_graph(data)

    # åˆ†æç»“æ„
    structure_analysis = graph_analyzer.analyze_market_structure(graph)

    # æ£€æµ‹å¼‚å¸¸
    anomaly_analysis = graph_analyzer.detect_anomalies(data)

    # åˆ†ææœºåˆ¶
    regime_analysis = graph_analyzer.analyze_market_regimes(data)

    return {
        'structure_analysis': structure_analysis,
        'anomaly_analysis': anomaly_analysis,
        'regime_analysis': regime_analysis,
        'model_info': graph_analyzer.get_model_info()
    }