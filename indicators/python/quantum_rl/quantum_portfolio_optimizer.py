"""
é‡å­å¼ºåŒ–å­¦ä¹ æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨
Quantum Reinforcement Learning Portfolio Optimizer

ç»“åˆé‡å­è®¡ç®—ä¸å¼ºåŒ–å­¦ä¹ çš„æŠ•èµ„ç»„åˆä¼˜åŒ–ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.distributions import Normal, Categorical
    TORCH_AVAILABLE = True
    print("ğŸ§  PyTorch å·²å¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ä¸å¯ç”¨")

try:
    from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister
    from qiskit.circuit import Parameter
    from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes
    from qiskit.algorithms import VQE
    from qiskit.algorithms.optimizers import COBYLA
    from qiskit.primitives import Sampler
    QISKIT_AVAILABLE = True
    print("âš›ï¸ Qiskit å·²å¯ç”¨")
except ImportError:
    QISKIT_AVAILABLE = False
    print("âš ï¸ Qiskit ä¸å¯ç”¨")

try:
    import gym
    from gym import spaces
    GYM_AVAILABLE = True
    print("ğŸ® Gym å·²å¯ç”¨")
except ImportError:
    GYM_AVAILABLE = False
    print("âš ï¸ Gym ä¸å¯ç”¨")

class QuantumPortfolioOptimizer:
    """
    é‡å­å¼ºåŒ–å­¦ä¹ æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨

    ä½¿ç”¨é‡å­è®¡ç®—å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–æŠ•èµ„ç»„åˆé…ç½®
    """

    def __init__(self,
                 n_assets: int = 5,
                 n_qubits: int = 4,
                 quantum_depth: int = 3,
                 learning_rate: float = 0.001,
                 risk_aversion: float = 0.5,
                 transaction_cost: float = 0.001):
        """
        åˆå§‹åŒ–é‡å­æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨

        Args:
            n_assets: èµ„äº§æ•°é‡
            n_qubits: é‡å­æ¯”ç‰¹æ•°
            quantum_depth: é‡å­ç”µè·¯æ·±åº¦
            learning_rate: å­¦ä¹ ç‡
            risk_aversion: é£é™©åŒæ¶ç³»æ•°
            transaction_cost: äº¤æ˜“æˆæœ¬
        """
        self.n_assets = n_assets
        self.n_qubits = n_qubits
        self.quantum_depth = quantum_depth
        self.learning_rate = learning_rate
        self.risk_aversion = risk_aversion
        self.transaction_cost = transaction_cost

        # é‡å­ç»„ä»¶
        self.quantum_circuit = None
        self.quantum_params = None
        self.quantum_optimizer = None

        # å¼ºåŒ–å­¦ä¹ ç»„ä»¶
        self.q_network = None
        self.target_network = None
        self.optimizer = None
        self.memory = ReplayBuffer(capacity=10000)

        # æŠ•èµ„ç»„åˆçŠ¶æ€
        self.current_portfolio = np.ones(n_assets) / n_assets
        self.portfolio_history = []
        self.performance_metrics = {}

        # è®­ç»ƒçŠ¶æ€
        self.is_trained = False
        self.training_episodes = 0
        self.training_rewards = []

        if TORCH_AVAILABLE:
            self._build_quantum_circuit()
            self._build_neural_networks()

    def _build_quantum_circuit(self):
        """æ„å»ºé‡å­ç”µè·¯"""
        if not QISKIT_AVAILABLE:
            return

        # é‡å­å¯„å­˜å™¨
        qr = QuantumRegister(self.n_qubits, 'q')
        cr = ClassicalRegister(self.n_qubits, 'c')
        self.quantum_circuit = QuantumCircuit(qr, cr)

        # å‚æ•°åŒ–é‡å­ç”µè·¯
        self.quantum_params = []
        for depth in range(self.quantum_depth):
            for qubit in range(self.n_qubits):
                # æ—‹è½¬é—¨å‚æ•°
                theta_param = Parameter(f'Î¸_{depth}_{qubit}')
                phi_param = Parameter(f'Ï†_{depth}_{qubit}')
                self.quantum_params.extend([theta_param, phi_param])

                # æ·»åŠ æ—‹è½¬é—¨
                self.quantum_circuit.ry(theta_param, qr[qubit])
                self.quantum_circuit.rz(phi_param, qr[qubit])

            # çº ç¼ å±‚
            for qubit in range(self.n_qubits - 1):
                self.quantum_circuit.cz(qr[qubit], qr[qubit + 1])

        # æµ‹é‡
        self.quantum_circuit.measure(qr, cr)

    def _build_neural_networks(self):
        """æ„å»ºç¥ç»ç½‘ç»œ"""
        if not TORCH_AVAILABLE:
            return

        # é‡å­å¢å¼ºQç½‘ç»œ
        self.q_network = QuantumQNetwork(
            input_dim=self.n_assets * 3,  # ä»·æ ¼ã€æ³¢åŠ¨ç‡ã€ç›¸å…³æ€§
            hidden_dims=[128, 64, 32],
            output_dim=self.n_assets,
            n_qubits=self.n_qubits,
            quantum_depth=self.quantum_depth
        )

        # ç›®æ ‡ç½‘ç»œ
        self.target_network = QuantumQNetwork(
            input_dim=self.n_assets * 3,
            hidden_dims=[128, 64, 32],
            output_dim=self.n_assets,
            n_qubits=self.n_qubits,
            quantum_depth=self.quantum_depth
        )

        # å¤åˆ¶å‚æ•°
        self.target_network.load_state_dict(self.q_network.state_dict())

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=self.learning_rate)

    def train_portfolio_optimization(self,
                                   market_data: pd.DataFrame,
                                   n_episodes: int = 100,
                                   max_steps: int = 252) -> Dict[str, Any]:
        """è®­ç»ƒæŠ•èµ„ç»„åˆä¼˜åŒ–"""
        print("ğŸš€ å¼€å§‹é‡å­å¼ºåŒ–å­¦ä¹ æŠ•èµ„ç»„åˆä¼˜åŒ–è®­ç»ƒ...")

        # åˆ›å»ºç¯å¢ƒ
        env = PortfolioEnvironment(market_data, self.n_assets, self.transaction_cost)

        training_stats = {
            'episode_rewards': [],
            'portfolio_values': [],
            'sharpe_ratios': [],
            'max_drawdowns': [],
            'quantum_advantages': []
        }

        for episode in range(n_episodes):
            print(f"ğŸ“Š è®­ç»ƒå›åˆ {episode + 1}/{n_episodes}")

            # é‡ç½®ç¯å¢ƒ
            state = env.reset()
            episode_reward = 0
            portfolio_values = []

            for step in range(max_steps):
                # é‡å­å¢å¼ºçš„åŠ¨ä½œé€‰æ‹©
                action, quantum_info = self._quantum_action_selection(state)

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(action)

                # å­˜å‚¨ç»éªŒ
                self.memory.push(state, action, reward, next_state, done)

                # å­¦ä¹ 
                self._learn_step()

                state = next_state
                episode_reward += reward
                portfolio_values.append(info['portfolio_value'])

                if done:
                    break

            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            if episode % 10 == 0:
                self.target_network.load_state_dict(self.q_network.state_dict())

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            training_stats['episode_rewards'].append(episode_reward)
            training_stats['portfolio_values'].append(portfolio_values)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            if len(portfolio_values) > 1:
                returns = pd.Series(portfolio_values).pct_change().dropna()
                sharpe_ratio = self._calculate_sharpe_ratio(returns)
                max_drawdown = self._calculate_max_drawdown(portfolio_values)

                training_stats['sharpe_ratios'].append(sharpe_ratio)
                training_stats['max_drawdowns'].append(max_drawdown)

                # é‡å­ä¼˜åŠ¿è¯„ä¼°
                quantum_advantage = self._assess_quantum_advantage(quantum_info)
                training_stats['quantum_advantages'].append(quantum_advantage)

            self.training_episodes += 1
            self.training_rewards.append(episode_reward)

        self.is_trained = True

        return {
            'training_stats': training_stats,
            'final_portfolio': self.current_portfolio,
            'total_episodes': n_episodes,
            'convergence_info': self._analyze_convergence(training_stats),
            'quantum_performance': self._analyze_quantum_performance(training_stats)
        }

    def _quantum_action_selection(self, state: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """é‡å­å¢å¼ºåŠ¨ä½œé€‰æ‹©"""
        quantum_info = {}

        if not TORCH_AVAILABLE:
            # éšæœºåŠ¨ä½œ
            action = np.random.dirichlet(np.ones(self.n_assets))
            return action, quantum_info

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)

            # Qç½‘ç»œè¾“å‡º
            q_values = self.q_network(state_tensor)

            # é‡å­å¢å¼ºæ¢ç´¢
            if QISKIT_AVAILABLE and np.random.random() < 0.1:  # 10%æ¦‚ç‡ä½¿ç”¨é‡å­æ¢ç´¢
                quantum_action = self._generate_quantum_action()
                quantum_weights = self._quantum_to_portfolio_weights(quantum_action)
                quantum_info['quantum_used'] = True
                quantum_info['quantum_state'] = quantum_action
                return quantum_weights, quantum_info

            # Îµ-è´ªå©ªç­–ç•¥
            epsilon = max(0.01, 0.5 - self.training_episodes * 0.001)
            if np.random.random() < epsilon:
                action = np.random.dirichlet(np.ones(self.n_assets))
                quantum_info['exploration'] = True
            else:
                # è´ªå©ªåŠ¨ä½œé€‰æ‹©
                action_weights = F.softmax(q_values, dim=-1).squeeze().numpy()
                action = self._normalize_portfolio_weights(action_weights)
                quantum_info['exploration'] = False

        return action, quantum_info

    def _generate_quantum_action(self) -> np.ndarray:
        """ç”Ÿæˆé‡å­åŠ¨ä½œ"""
        if not QISKIT_AVAILABLE:
            return np.random.random(self.n_qubits)

        try:
            # ç»‘å®šå‚æ•°
            bound_circuit = self.quantum_circuit.bind_parameters(
                {param: np.random.uniform(0, 2*np.pi) for param in self.quantum_params}
            )

            # æ‰§è¡Œé‡å­ç”µè·¯
            sampler = Sampler()
            job = sampler.run(bound_circuit, shots=1000)
            result = job.result()
            counts = result.quasi_dists[0]

            # è½¬æ¢ä¸ºåŠ¨ä½œæ¦‚ç‡
            action_probs = np.zeros(self.n_qubits)
            for bitstring, prob in counts.items():
                for i, bit in enumerate(bitstring):
                    if bit == '1':
                        action_probs[i] += prob

            return action_probs / np.sum(action_probs) if np.sum(action_probs) > 0 else np.random.random(self.n_qubits)

        except Exception as e:
            print(f"âš ï¸ é‡å­ç”µè·¯æ‰§è¡Œå¤±è´¥: {e}")
            return np.random.random(self.n_qubits)

    def _quantum_to_portfolio_weights(self, quantum_state: np.ndarray) -> np.ndarray:
        """é‡å­æ€è½¬æŠ•èµ„ç»„åˆæƒé‡"""
        # å°†é‡å­æ€æ˜ å°„åˆ°æŠ•èµ„ç»„åˆæƒé‡
        if len(quantum_state) != self.n_assets:
            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œæ’å€¼æˆ–æˆªæ–­
            if len(quantum_state) > self.n_assets:
                quantum_state = quantum_state[:self.n_assets]
            else:
                # å¡«å……åˆ°æ­£ç¡®ç»´åº¦
                padded_state = np.zeros(self.n_assets)
                padded_state[:len(quantum_state)] = quantum_state
                quantum_state = padded_state

        # å½’ä¸€åŒ–ä¸ºæŠ•èµ„ç»„åˆæƒé‡
        weights = quantum_state / np.sum(quantum_state) if np.sum(quantum_state) > 0 else np.ones(self.n_assets) / self.n_assets

        return weights

    def _normalize_portfolio_weights(self, weights: np.ndarray) -> np.ndarray:
        """å½’ä¸€åŒ–æŠ•èµ„ç»„åˆæƒé‡"""
        weights = np.abs(weights)  # ç¡®ä¿éè´Ÿ
        return weights / np.sum(weights) if np.sum(weights) > 0 else np.ones(self.n_assets) / self.n_assets

    def _learn_step(self):
        """å­¦ä¹ æ­¥éª¤"""
        if len(self.memory) < 32:
            return

        # é‡‡æ ·æ‰¹æ¬¡
        batch = self.memory.sample(32)
        if not batch:
            return

        states, actions, rewards, next_states, dones = zip(*batch)

        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(np.array(states))
        actions = torch.FloatTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.BoolTensor(dones)

        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, torch.argmax(actions, dim=1).unsqueeze(1))

        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (1 - dones.float()) * 0.99 * next_q_values

        # è®¡ç®—æŸå¤±
        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def optimize_portfolio(self,
                          market_data: pd.DataFrame,
                          current_portfolio: np.ndarray = None,
                          optimization_horizon: int = 252) -> Dict[str, Any]:
        """ä¼˜åŒ–æŠ•èµ„ç»„åˆ"""
        print("ğŸ¯ æ‰§è¡Œé‡å­æŠ•èµ„ç»„åˆä¼˜åŒ–...")

        if current_portfolio is None:
            current_portfolio = np.ones(self.n_assets) / self.n_assets

        self.current_portfolio = current_portfolio

        # é¢„å¤„ç†å¸‚åœºæ•°æ®
        processed_data = self._preprocess_market_data(market_data)

        # é‡å­è®¡ç®—å¢å¼ºçš„ä¼˜åŒ–
        if QISKIT_AVAILABLE:
            quantum_optimization = self._quantum_portfolio_optimization(processed_data)
        else:
            quantum_optimization = self._classical_fallback_optimization(processed_data)

        # å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–
        rl_optimization = self._rl_portfolio_optimization(processed_data)

        # èåˆä¼˜åŒ–ç»“æœ
        final_weights = self._fuse_optimization_results(quantum_optimization, rl_optimization)

        # é£é™©è°ƒæ•´
        risk_adjusted_weights = self._apply_risk_adjustment(final_weights, processed_data)

        # äº¤æ˜“æˆæœ¬è°ƒæ•´
        final_portfolio = self._apply_transaction_costs(risk_adjusted_weights, current_portfolio)

        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        optimization_report = {
            'optimal_weights': final_portfolio,
            'previous_weights': current_portfolio,
            'quantum_contribution': quantum_optimization,
            'rl_contribution': rl_optimization,
            'expected_return': self._calculate_expected_return(final_portfolio, processed_data),
            'expected_risk': self._calculate_portfolio_risk(final_portfolio, processed_data),
            'sharpe_ratio': self._calculate_portfolio_sharpe_ratio(final_portfolio, processed_data),
            'optimization_metrics': self._calculate_optimization_metrics(final_portfolio, current_portfolio),
            'quantum_advantage_score': self._calculate_quantum_advantage_score(quantum_optimization)
        }

        return optimization_report

    def _preprocess_market_data(self, market_data: pd.DataFrame) -> Dict[str, Any]:
        """é¢„å¤„ç†å¸‚åœºæ•°æ®"""
        processed = {}

        # ä»·æ ¼æ•°æ®
        numeric_columns = market_data.select_dtypes(include=[np.number]).columns
        price_data = market_data[numeric_columns].fillna(method='ffill')

        # æ”¶ç›Šç‡
        returns = price_data.pct_change().dropna()
        processed['returns'] = returns

        # æœŸæœ›æ”¶ç›Šç‡
        processed['expected_returns'] = returns.mean()

        # åæ–¹å·®çŸ©é˜µ
        processed['covariance_matrix'] = returns.cov()

        # æ³¢åŠ¨ç‡
        processed['volatilities'] = returns.std()

        # ç›¸å…³æ€§
        processed['correlations'] = returns.corr()

        # å¸‚åœºçŠ¶æ€
        processed['market_state'] = self._analyze_market_state(price_data)

        return processed

    def _analyze_market_state(self, price_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºçŠ¶æ€"""
        state = {}

        if 'close' in price_data.columns:
            prices = price_data['close'].dropna()
            if len(prices) > 20:
                # è¶‹åŠ¿åˆ†æ
                returns = prices.pct_change().dropna()
                trend = np.polyfit(range(len(returns)), returns.values, 1)[0]
                state['trend'] = 'bullish' if trend > 0 else 'bearish' if trend < 0 else 'neutral'

                # æ³¢åŠ¨ç‡çŠ¶æ€
                volatility = returns.std()
                state['volatility_state'] = 'high' if volatility > 0.2 else 'low' if volatility < 0.1 else 'medium'

                # å¸‚åœºæ•ˆç‡
                autocorr = returns.autocorr(lag=1)
                state['market_efficiency'] = 'efficient' if abs(autocorr) < 0.1 else 'inefficient'

        return state

    def _quantum_portfolio_optimization(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """é‡å­æŠ•èµ„ç»„åˆä¼˜åŒ–"""
        quantum_result = {}

        if not QISKIT_AVAILABLE:
            return quantum_result

        try:
            # æ„å»ºé‡å­ä¼˜åŒ–é—®é¢˜
            expected_returns = processed_data['expected_returns'].values
            covariance_matrix = processed_data['covariance_matrix'].values

            # é‡å­å˜åˆ†ä¼˜åŒ–å™¨
            quantum_optimizer = self._create_quantum_optimizer(expected_returns, covariance_matrix)

            # è¿è¡Œä¼˜åŒ–
            optimal_params = self._run_quantum_optimization(quantum_optimizer)

            # è½¬æ¢ä¸ºæŠ•èµ„ç»„åˆæƒé‡
            optimal_weights = self._quantum_params_to_weights(optimal_params)

            quantum_result = {
                'weights': optimal_weights,
                'optimization_params': optimal_params,
                'convergence_history': [],
                'quantum_circuits_used': self.quantum_depth,
                'optimization_time': 0.0  # å®é™…åº”ç”¨ä¸­åº”è¯¥è®¡æ—¶
            }

        except Exception as e:
            print(f"âš ï¸ é‡å­ä¼˜åŒ–å¤±è´¥: {e}")
            quantum_result['error'] = str(e)

        return quantum_result

    def _create_quantum_optimizer(self, expected_returns: np.ndarray, covariance_matrix: np.ndarray):
        """åˆ›å»ºé‡å­ä¼˜åŒ–å™¨"""
        # è¿™é‡Œåº”è¯¥æ„å»ºé€‚å½“çš„é‡å­ä¼˜åŒ–ç”µè·¯
        # ç®€åŒ–çš„å®ç°
        return {
            'expected_returns': expected_returns,
            'covariance_matrix': covariance_matrix,
            'n_assets': len(expected_returns)
        }

    def _run_quantum_optimization(self, optimizer: Dict[str, Any]) -> np.ndarray:
        """è¿è¡Œé‡å­ä¼˜åŒ–"""
        # ç®€åŒ–çš„é‡å­ä¼˜åŒ–å®ç°
        n_assets = optimizer['n_assets']
        return np.random.random(n_assets)  # å®é™…åº”è¯¥æ˜¯é‡å­ä¼˜åŒ–ç»“æœ

    def _quantum_params_to_weights(self, quantum_params: np.ndarray) -> np.ndarray:
        """é‡å­å‚æ•°è½¬æƒé‡"""
        weights = np.abs(quantum_params)
        return weights / np.sum(weights) if np.sum(weights) > 0 else np.ones(len(quantum_params)) / len(quantum_params)

    def _classical_fallback_optimization(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç»å…¸ä¼˜åŒ–å›é€€"""
        # ç®€å•çš„å‡å€¼æ–¹å·®ä¼˜åŒ–
        expected_returns = processed_data['expected_returns'].values
        covariance_matrix = processed_data['covariance_matrix'].values

        # æœ€å°æ–¹å·®ç»„åˆ
        try:
            inv_cov = np.linalg.inv(covariance_matrix)
            ones = np.ones(len(expected_returns))
            min_var_weights = inv_cov @ ones / (ones @ inv_cov @ ones)

            return {
                'weights': min_var_weights,
                'method': 'minimum_variance',
                'fallback_reason': 'quantum_not_available'
            }
        except:
            # ç­‰æƒé‡ç»„åˆ
            equal_weights = np.ones(len(expected_returns)) / len(expected_returns)
            return {
                'weights': equal_weights,
                'method': 'equal_weight',
                'fallback_reason': 'matrix_inversion_failed'
            }

    def _rl_portfolio_optimization(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–"""
        rl_result = {}

        if not self.is_trained or not TORCH_AVAILABLE:
            # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥æˆ–éšæœºç­–ç•¥
            rl_result['weights'] = np.ones(self.n_assets) / self.n_assets
            rl_result['method'] = 'untrained_or_no_torch'
            return rl_result

        # ä½¿ç”¨è®­ç»ƒå¥½çš„Qç½‘ç»œè¿›è¡Œä¼˜åŒ–
        try:
            # æ„å»ºçŠ¶æ€è¡¨ç¤º
            state_features = self._build_rl_state_features(processed_data)

            with torch.no_grad():
                state_tensor = torch.FloatTensor(state_features).unsqueeze(0)
                q_values = self.q_network(state_tensor)
                weights = F.softmax(q_values, dim=-1).squeeze().numpy()

            rl_result = {
                'weights': weights,
                'method': 'trained_q_network',
                'confidence': float(torch.max(q_values))
            }

        except Exception as e:
            print(f"âš ï¸ å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¤±è´¥: {e}")
            rl_result['weights'] = np.ones(self.n_assets) / self.n_assets
            rl_result['error'] = str(e)

        return rl_result

    def _build_rl_state_features(self, processed_data: Dict[str, Any]) -> np.ndarray:
        """æ„å»ºå¼ºåŒ–å­¦ä¹ çŠ¶æ€ç‰¹å¾"""
        features = []

        # æœŸæœ›æ”¶ç›Šç‡
        features.extend(processed_data['expected_returns'].values)

        # æ³¢åŠ¨ç‡
        features.extend(processed_data['volatilities'].values)

        # å¸‚åœºçŠ¶æ€ç¼–ç 
        market_state = processed_data['market_state']
        state_encoding = [
            1.0 if market_state.get('trend') == 'bullish' else 0.0,
            1.0 if market_state.get('trend') == 'bearish' else 0.0,
            1.0 if market_state.get('volatility_state') == 'high' else 0.0,
            1.0 if market_state.get('market_efficiency') == 'efficient' else 0.0
        ]
        features.extend(state_encoding)

        return np.array(features)

    def _fuse_optimization_results(self, quantum_result: Dict[str, Any], rl_result: Dict[str, Any]) -> np.ndarray:
        """èåˆä¼˜åŒ–ç»“æœ"""
        # æƒé‡èåˆç­–ç•¥
        quantum_weights = quantum_result.get('weights', np.ones(self.n_assets) / self.n_assets)
        rl_weights = rl_result.get('weights', np.ones(self.n_assets) / self.n_assets)

        # åŠ¨æ€æƒé‡åˆ†é…
        quantum_confidence = self._assess_quantum_confidence(quantum_result)
        rl_confidence = rl_result.get('confidence', 0.5)

        total_confidence = quantum_confidence + rl_confidence
        if total_confidence > 0:
            quantum_weight = quantum_confidence / total_confidence
            rl_weight = rl_confidence / total_confidence
        else:
            quantum_weight = 0.5
            rl_weight = 0.5

        # åŠ æƒèåˆ
        fused_weights = quantum_weight * quantum_weights + rl_weight * rl_weights

        return self._normalize_portfolio_weights(fused_weights)

    def _assess_quantum_confidence(self, quantum_result: Dict[str, Any]) -> float:
        """è¯„ä¼°é‡å­ä¼˜åŒ–ç½®ä¿¡åº¦"""
        if 'error' in quantum_result:
            return 0.1  # ä½ç½®ä¿¡åº¦

        if 'fallback_reason' in quantum_result:
            return 0.3  # ä¸­ç­‰ç½®ä¿¡åº¦

        return 0.8  # é«˜ç½®ä¿¡åº¦

    def _apply_risk_adjustment(self, weights: np.ndarray, processed_data: Dict[str, Any]) -> np.ndarray:
        """åº”ç”¨é£é™©è°ƒæ•´"""
        # åŸºäºé£é™©åŒæ¶ç³»æ•°è°ƒæ•´æƒé‡
        volatilities = processed_data['volatilities'].values

        # é£é™©è°ƒæ•´å› å­
        risk_factors = 1.0 / (1.0 + self.risk_aversion * volatilities)

        # è°ƒæ•´æƒé‡
        adjusted_weights = weights * risk_factors

        return self._normalize_portfolio_weights(adjusted_weights)

    def _apply_transaction_costs(self, optimal_weights: np.ndarray, current_weights: np.ndarray) -> np.ndarray:
        """åº”ç”¨äº¤æ˜“æˆæœ¬è°ƒæ•´"""
        # è®¡ç®—äº¤æ˜“é‡
        trading_volume = np.abs(optimal_weights - current_weights)

        # äº¤æ˜“æˆæœ¬æƒ©ç½š
        cost_penalty = self.transaction_cost * trading_volume

        # è°ƒæ•´æƒé‡ä»¥å‡å°‘äº¤æ˜“
        adjusted_weights = current_weights + 0.8 * (optimal_weights - current_weights)

        return self._normalize_portfolio_weights(adjusted_weights)

    def _calculate_expected_return(self, weights: np.ndarray, processed_data: Dict[str, Any]) -> float:
        """è®¡ç®—æœŸæœ›æ”¶ç›Šç‡"""
        expected_returns = processed_data['expected_returns'].values
        return np.dot(weights, expected_returns)

    def _calculate_portfolio_risk(self, weights: np.ndarray, processed_data: Dict[str, Any]) -> float:
        """è®¡ç®—æŠ•èµ„ç»„åˆé£é™©"""
        covariance_matrix = processed_data['covariance_matrix'].values
        return np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))

    def _calculate_portfolio_sharpe_ratio(self, weights: np.ndarray, processed_data: Dict[str, Any]) -> float:
        """è®¡ç®—å¤æ™®æ¯”ç‡"""
        expected_return = self._calculate_expected_return(weights, processed_data)
        risk = self._calculate_portfolio_risk(weights, processed_data)

        if risk > 0:
            return expected_return / risk
        else:
            return 0.0

    def _calculate_optimization_metrics(self, new_weights: np.ndarray, old_weights: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—ä¼˜åŒ–æŒ‡æ ‡"""
        metrics = {}

        # æƒé‡å˜åŒ–
        weight_change = np.mean(np.abs(new_weights - old_weights))
        metrics['weight_change'] = weight_change

        # åˆ†æ•£åŒ–ç¨‹åº¦
        herfindahl_index = np.sum(new_weights ** 2)
        metrics['diversification'] = 1.0 - herfindahl_index

        # é›†ä¸­åº¦
        max_weight = np.max(new_weights)
        metrics['concentration'] = max_weight

        # æœ‰æ•ˆèµ„äº§æ•°
        effective_assets = 1.0 / herfindahl_index
        metrics['effective_assets'] = effective_assets

        return metrics

    def _calculate_quantum_advantage_score(self, quantum_result: Dict[str, Any]) -> float:
        """è®¡ç®—é‡å­ä¼˜åŠ¿åˆ†æ•°"""
        if 'error' in quantum_result:
            return 0.0

        if 'fallback_reason' in quantum_result:
            return 0.3

        # åŸºäºé‡å­ç”µè·¯æ·±åº¦å’Œå‚æ•°æ•°é‡è¯„ä¼°
        quantum_score = min(1.0, self.quantum_depth / 10.0)
        return quantum_score

    def _calculate_sharpe_ratio(self, returns: pd.Series, risk_free_rate: float = 0.02) -> float:
        """è®¡ç®—å¤æ™®æ¯”ç‡"""
        if len(returns) == 0 or returns.std() == 0:
            return 0.0

        excess_returns = returns - risk_free_rate / 252  # æ—¥åŒ–æ— é£é™©åˆ©ç‡
        return np.sqrt(252) * excess_returns.mean() / returns.std()

    def _calculate_max_drawdown(self, portfolio_values: List[float]) -> float:
        """è®¡ç®—æœ€å¤§å›æ’¤"""
        if len(portfolio_values) < 2:
            return 0.0

        cumulative = np.array(portfolio_values)
        peak = np.maximum.accumulate(cumulative)
        drawdown = (cumulative - peak) / peak

        return np.min(drawdown)

    def _assess_quantum_advantage(self, quantum_info: Dict[str, Any]) -> float:
        """è¯„ä¼°é‡å­ä¼˜åŠ¿"""
        if not quantum_info.get('quantum_used', False):
            return 0.0

        # åŸºäºé‡å­æ€çš„è´¨é‡è¯„ä¼°ä¼˜åŠ¿
        quantum_state = quantum_info.get('quantum_state', np.array([]))
        if len(quantum_state) == 0:
            return 0.0

        # é‡å­ä¼˜åŠ¿åº¦é‡ï¼šé‡å­æ€çš„çº ç¼ åº¦å’Œå åŠ æ€è´¨é‡
        state_purity = np.sum(quantum_state ** 2)
        quantum_advantage = 1.0 - state_purity  # è¶Šçº¯æ€ä¼˜åŠ¿è¶Šå°

        return quantum_advantage

    def _analyze_convergence(self, training_stats: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ”¶æ•›æ€§"""
        convergence_info = {}

        episode_rewards = training_stats['episode_rewards']
        if len(episode_rewards) > 10:
            # æ£€æŸ¥æœ€å10ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±
            recent_avg = np.mean(episode_rewards[-10:])
            overall_avg = np.mean(episode_rewards)

            convergence_info['recent_average'] = recent_avg
            convergence_info['overall_average'] = overall_avg
            convergence_info['convergence_ratio'] = recent_avg / overall_avg if overall_avg != 0 else 0
            convergence_info['is_converged'] = abs(recent_avg - overall_avg) / abs(overall_avg) < 0.1 if overall_avg != 0 else False

        return convergence_info

    def _analyze_quantum_performance(self, training_stats: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé‡å­æ€§èƒ½"""
        quantum_performance = {}

        quantum_advantages = training_stats.get('quantum_advantages', [])
        if quantum_advantages:
            quantum_performance['average_quantum_advantage'] = np.mean(quantum_advantages)
            quantum_performance['max_quantum_advantage'] = np.max(quantum_advantages)
            quantum_performance['quantum_usage_rate'] = len([qa for qa in quantum_advantages if qa > 0]) / len(quantum_advantages)

        return quantum_performance

    def get_optimization_summary(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æ€»ç»“"""
        return {
            'is_trained': self.is_trained,
            'training_episodes': self.training_episodes,
            'n_assets': self.n_assets,
            'n_qubits': self.n_qubits,
            'quantum_depth': self.quantum_depth,
            'learning_rate': self.learning_rate,
            'risk_aversion': self.risk_aversion,
            'current_portfolio': self.current_portfolio.tolist(),
            'memory_size': len(self.memory),
            'model_info': self.get_model_info()
        }

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_type': 'Quantum Reinforcement Learning Portfolio Optimizer',
            'torch_available': TORCH_AVAILABLE,
            'qiskit_available': QISKIT_AVAILABLE,
            'gym_available': GYM_AVAILABLE,
            'n_assets': self.n_assets,
            'n_qubits': self.n_qubits,
            'quantum_depth': self.quantum_depth,
            'risk_aversion': self.risk_aversion,
            'transaction_cost': self.transaction_cost
        }


class QuantumQNetwork(nn.Module):
    """é‡å­Qç½‘ç»œ"""
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int, n_qubits: int, quantum_depth: int):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.n_qubits = n_qubits
        self.quantum_depth = quantum_depth

        # ç¥ç»ç½‘ç»œå±‚
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)

        # é‡å­å¢å¼ºå±‚ï¼ˆç®€åŒ–å®ç°ï¼‰
        self.quantum_enhancement = nn.Linear(output_dim, output_dim)

    def forward(self, x):
        # ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­
        neural_output = self.network(x)

        # é‡å­å¢å¼º
        quantum_output = self.quantum_enhancement(neural_output)

        # èåˆè¾“å‡º
        final_output = neural_output + 0.1 * quantum_output

        return final_output


class PortfolioEnvironment:
    """æŠ•èµ„ç»„åˆç¯å¢ƒ"""
    def __init__(self, market_data: pd.DataFrame, n_assets: int, transaction_cost: float = 0.001):
        self.market_data = market_data
        self.n_assets = n_assets
        self.transaction_cost = transaction_cost

        # çŠ¶æ€ç©ºé—´
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(n_assets * 3,),  # ä»·æ ¼ã€æ³¢åŠ¨ç‡ã€ç›¸å…³æ€§
            dtype=np.float32
        )

        # åŠ¨ä½œç©ºé—´
        self.action_space = spaces.Box(
            low=0.0, high=1.0,
            shape=(n_assets,),
            dtype=np.float32
        )

        # ç¯å¢ƒçŠ¶æ€
        self.current_step = 0
        self.max_steps = len(market_data) - 1
        self.current_portfolio = np.ones(n_assets) / n_assets
        self.portfolio_value = 1.0

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.current_portfolio = np.ones(self.n_assets) / self.n_assets
        self.portfolio_value = 1.0
        return self._get_state()

    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # å½’ä¸€åŒ–åŠ¨ä½œ
        action = action / np.sum(action) if np.sum(action) > 0 else np.ones(self.n_assets) / self.n_assets

        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(action)

        # æ›´æ–°çŠ¶æ€
        self.current_step += 1
        self.current_portfolio = action

        # è®¡ç®—æŠ•èµ„ç»„åˆä»·å€¼
        self.portfolio_value *= (1 + reward)

        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self.current_step >= self.max_steps

        # è·å–æ–°çŠ¶æ€
        next_state = self._get_state()

        # ä¿¡æ¯
        info = {
            'portfolio_value': self.portfolio_value,
            'portfolio_weights': action.copy(),
            'step': self.current_step
        }

        return next_state, reward, done, info

    def _get_state(self):
        """è·å–çŠ¶æ€"""
        if self.current_step >= len(self.market_data):
            return np.zeros(self.n_assets * 3)

        # ç®€åŒ–çš„çŠ¶æ€è¡¨ç¤º
        price_data = self.market_data.iloc[self.current_step].select_dtypes(include=[np.number]).values

        # å¡«å……æˆ–æˆªæ–­åˆ°æ­£ç¡®ç»´åº¦
        if len(price_data) < self.n_assets * 3:
            padded_state = np.zeros(self.n_assets * 3)
            padded_state[:len(price_data)] = price_data
            return padded_state
        else:
            return price_data[:self.n_assets * 3]

    def _calculate_reward(self, action):
        """è®¡ç®—å¥–åŠ±"""
        # ç®€åŒ–çš„å¥–åŠ±è®¡ç®—
        if self.current_step >= len(self.market_data) - 1:
            return 0.0

        # è·å–å½“å‰å’Œä¸‹ä¸€æœŸçš„ä»·æ ¼æ•°æ®
        current_prices = self.market_data.iloc[self.current_step].select_dtypes(include=[np.number]).values
        next_prices = self.market_data.iloc[self.current_step + 1].select_dtypes(include=[np.number]).values

        # è®¡ç®—èµ„äº§æ”¶ç›Šç‡
        if len(current_prices) >= self.n_assets and len(next_prices) >= self.n_assets:
            asset_returns = (next_prices[:self.n_assets] - current_prices[:self.n_assets]) / current_prices[:self.n_assets]

            # æŠ•èµ„ç»„åˆæ”¶ç›Šç‡
            portfolio_return = np.dot(action, asset_returns)

            # äº¤æ˜“æˆæœ¬æƒ©ç½š
            trading_cost_penalty = self.transaction_cost * np.sum(np.abs(action - self.current_portfolio))

            # å‡€å¥–åŠ±
            reward = portfolio_return - trading_cost_penalty
        else:
            reward = 0.0

        return reward


class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def push(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size: int):
        """é‡‡æ ·æ‰¹æ¬¡"""
        if len(self.buffer) < batch_size:
            return None

        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        return [self.buffer[i] for i in indices]

    def __len__(self):
        return len(self.buffer)


# ä¾¿æ·å‡½æ•°
def create_quantum_portfolio_optimizer(n_assets: int = 5) -> QuantumPortfolioOptimizer:
    """åˆ›å»ºé‡å­æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨å®ä¾‹"""
    return QuantumPortfolioOptimizer(n_assets=n_assets)

def quick_portfolio_optimization(market_data: pd.DataFrame, n_assets: int = 5) -> Dict[str, Any]:
    """å¿«é€ŸæŠ•èµ„ç»„åˆä¼˜åŒ–"""
    optimizer = create_quantum_portfolio_optimizer(n_assets)
    return optimizer.optimize_portfolio(market_data)

def train_quantum_optimizer(market_data: pd.DataFrame, n_assets: int = 5, n_episodes: int = 50) -> Dict[str, Any]:
    """è®­ç»ƒé‡å­ä¼˜åŒ–å™¨"""
    optimizer = create_quantum_portfolio_optimizer(n_assets)
    return optimizer.train_portfolio_optimization(market_data, n_episodes)