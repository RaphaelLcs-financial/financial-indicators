"""
è®¡ç®—æœºè§†è§‰å¸‚åœºåˆ†ææŒ‡æ ‡
Computer Vision Market Analysis Indicators

åŸºäºæ·±åº¦å­¦ä¹ çš„å¸‚åœºå›¾è¡¨æ¨¡å¼è¯†åˆ«å’Œè§†è§‰åˆ†æç³»ç»Ÿ
æ”¯æŒæŠ€æœ¯å½¢æ€è¯†åˆ«ã€è¶‹åŠ¿çº¿æ£€æµ‹å’Œæ¨¡å¼é¢„æµ‹
"""

import numpy as np
import pandas as pd
from typing import Dict, Any, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as transforms
    from PIL import Image, ImageDraw, ImageFont
    TORCH_AVAILABLE = True
    print("ğŸ§  PyTorch å·²å¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ä¸å¯ç”¨")

# ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ”¯æŒ
try:
    import cv2
    from skimage import measure, feature, filters
    from skimage.transform import resize, hough_line, hough_circle, hough_ellipse
    from scipy import ndimage, signal
    CV_AVAILABLE = True
    print("ğŸ‘ï¸ OpenCV å·²å¯ç”¨")
except ImportError:
    CV_AVAILABLE = False
    print("âš ï¸ OpenCV ä¸å¯ç”¨")

class MarketVisualAnalyzer:
    """
    å¸‚åœºè§†è§‰åˆ†æå™¨

    åŸºäºè®¡ç®—æœºè§†è§‰æŠ€æœ¯åˆ†æé‡‘èå›¾è¡¨
    æ”¯æŒæŠ€æœ¯æ¨¡å¼è¯†åˆ«ã€è¶‹åŠ¿çº¿æ£€æµ‹ã€å½¢æ€åˆ†æ
    """

    def __init__(self, image_size: Tuple[int, int] = (224, 224),
                 pattern_types: List[str] = None,
                 detection_threshold: float = 0.7):
        """
        åˆå§‹åŒ–å¸‚åœºè§†è§‰åˆ†æå™¨

        Args:
            image_size: å›¾åƒå°ºå¯¸
            pattern_types: è¯†åˆ«çš„æ¨¡å¼ç±»å‹
            detection_threshold: æ£€æµ‹é˜ˆå€¼
        """
        self.image_size = image_size
        self.pattern_types = pattern_types or [
            'head_and_shoulders', 'double_top', 'double_bottom',
            'triangle', 'wedge', 'flag', 'pennant', 'cup_and_handle'
        ]
        self.detection_threshold = detection_threshold

        # æ·±åº¦å­¦ä¹ æ¨¡å‹
        self.pattern_recognition_model = None
        self.trend_detection_model = None
        self.feature_extractor = None

        # ä¼ ç»Ÿè§†è§‰ç®—æ³•
        self.edge_detector = None
        self.line_detector = None
        self.circle_detector = None

        # åˆ†æç»“æœ
        self.detected_patterns = []
        self.trend_lines = []
        self.support_resistance_levels = []
        self.visual_features = {}

        # æ€§èƒ½æŒ‡æ ‡
        self.detection_stats = {
            'patterns_detected': 0,
            'trend_lines_found': 0,
            'support_resistance_levels': 0,
            'detection_accuracy': 0.0
        }

    def initialize_models(self):
        """åˆå§‹åŒ–è®¡ç®—æœºè§†è§‰æ¨¡å‹"""
        if TORCH_AVAILABLE:
            try:
                # åˆå§‹åŒ–æ¨¡å¼è¯†åˆ«æ¨¡å‹
                self.pattern_recognition_model = self._create_pattern_recognition_model()

                # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
                self.feature_extractor = self._create_feature_extractor()

                print("âœ… æ·±åº¦å­¦ä¹ è§†è§‰æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ·±åº¦å­¦ä¹ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

        if CV_AVAILABLE:
            try:
                # åˆå§‹åŒ–ä¼ ç»Ÿè§†è§‰ç®—æ³•
                self.edge_detector = cv2.Canny
                self.line_detector = cv2.HoughLinesP
                self.circle_detector = cv2.HoughCircles

                print("âœ… ä¼ ç»Ÿè§†è§‰ç®—æ³•åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ä¼ ç»Ÿè§†è§‰ç®—æ³•åˆå§‹åŒ–å¤±è´¥: {e}")

    def _create_pattern_recognition_model(self) -> nn.Module:
        """åˆ›å»ºæ¨¡å¼è¯†åˆ«æ¨¡å‹"""
        class PatternRecognitionNet(nn.Module):
            def __init__(self, num_patterns: int = 8):
                super().__init__()

                # CNNç‰¹å¾æå–
                self.conv_layers = nn.Sequential(
                    nn.Conv2d(1, 32, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(32),
                    nn.MaxPool2d(2),

                    nn.Conv2d(32, 64, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(64),
                    nn.MaxPool2d(2),

                    nn.Conv2d(64, 128, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(128),
                    nn.MaxPool2d(2),

                    nn.Conv2d(128, 256, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(256),
                    nn.AdaptiveAvgPool2d((1, 1))
                )

                # åˆ†ç±»å¤´
                self.classifier = nn.Sequential(
                    nn.Dropout(0.5),
                    nn.Linear(256, 128),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(128, num_patterns)
                )

            def forward(self, x):
                features = self.conv_layers(x)
                features = features.view(features.size(0), -1)
                output = self.classifier(features)
                return output

        return PatternRecognitionNet(len(self.pattern_types))

    def _create_feature_extractor(self) -> nn.Module:
        """åˆ›å»ºç‰¹å¾æå–å™¨"""
        class FeatureExtractor(nn.Module):
            def __init__(self):
                super().__init__()

                self.conv_layers = nn.Sequential(
                    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                    nn.ReLU(),
                    nn.BatchNorm2d(64),
                    nn.MaxPool2d(3, stride=2, padding=1),

                    nn.Conv2d(64, 128, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(128),
                    nn.Conv2d(128, 128, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(128),
                    nn.MaxPool2d(2),

                    nn.Conv2d(128, 256, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(256),
                    nn.Conv2d(256, 256, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.BatchNorm2d(256),
                    nn.AdaptiveAvgPool2d((1, 1))
                )

            def forward(self, x):
                return self.conv_layers(x)

        return FeatureExtractor()

    def create_chart_image(self, market_data: pd.DataFrame,
                         chart_type: str = 'candlestick',
                         time_period: int = 100) -> np.ndarray:
        """åˆ›å»ºå¸‚åœºå›¾è¡¨å›¾åƒ"""
        print(f"ğŸ“Š åˆ›å»º{chart_type}å›¾è¡¨å›¾åƒ...")

        # è·å–æœ€è¿‘çš„æ•°æ®
        recent_data = market_data.tail(time_period).copy()

        if 'close' not in recent_data.columns:
            raise ValueError("å¸‚åœºæ•°æ®å¿…é¡»åŒ…å« 'close' åˆ—")

        # æ ‡å‡†åŒ–ä»·æ ¼æ•°æ®
        min_price = recent_data['close'].min()
        max_price = recent_data['close'].max()
        price_range = max_price - min_price

        if price_range == 0:
            price_range = 1

        normalized_prices = (recent_data['close'] - min_price) / price_range

        # åˆ›å»ºå›¾åƒ
        if chart_type == 'line':
            image = self._create_line_chart(normalized_prices)
        elif chart_type == 'candlestick':
            image = self._create_candlestick_chart(recent_data)
        elif chart_type == 'volume_profile':
            image = self._create_volume_profile_chart(recent_data)
        else:
            image = self._create_line_chart(normalized_prices)

        return image

    def _create_line_chart(self, prices: pd.Series) -> np.ndarray:
        """åˆ›å»ºçº¿æ€§å›¾è¡¨"""
        height, width = self.image_size
        image = np.ones((height, width), dtype=np.uint8) * 255

        # ç»˜åˆ¶ä»·æ ¼çº¿
        x_coords = np.linspace(10, width-10, len(prices))
        y_coords = (1 - prices.values) * (height - 20) + 10

        # è½¬æ¢ä¸ºæ•´æ•°åæ ‡
        points = np.column_stack([x_coords.astype(int), y_coords.astype(int)])

        # ç»˜åˆ¶çº¿æ¡
        for i in range(len(points) - 1):
            cv2.line(image, points[i], points[i+1], 0, 2)

        return image

    def _create_candlestick_chart(self, data: pd.DataFrame) -> np.ndarray:
        """åˆ›å»ºKçº¿å›¾"""
        height, width = self.image_size
        image = np.ones((height, width), dtype=np.uint8) * 255

        # è®¡ç®—èœ¡çƒ›å‚æ•°
        n_candles = len(data)
        candle_width = max(1, (width - 20) // n_candles)

        # æ ‡å‡†åŒ–ä»·æ ¼
        min_price = data[['high', 'low', 'open', 'close']].min().min()
        max_price = data[['high', 'low', 'open', 'close']].max().max()
        price_range = max_price - min_price

        if price_range == 0:
            price_range = 1

        for i, (_, row) in enumerate(data.iterrows()):
            x = 10 + i * candle_width

            # è®¡ç®—ä»·æ ¼ä½ç½®
            open_y = int((1 - (row['open'] - min_price) / price_range) * (height - 20) + 10)
            close_y = int((1 - (row['close'] - min_price) / price_range) * (height - 20) + 10)
            high_y = int((1 - (row['high'] - min_price) / price_range) * (height - 20) + 10)
            low_y = int((1 - (row['low'] - min_price) / price_range) * (height - 20) + 10)

            # ç¡®å®šé¢œè‰²ï¼ˆä¸Šæ¶¨ä¸ºç™½è‰²ï¼Œä¸‹è·Œä¸ºé»‘è‰²ï¼‰
            color = 255 if row['close'] >= row['open'] else 0

            # ç»˜åˆ¶èœ¡çƒ›
            cv2.line(image, (x + candle_width//2, high_y), (x + candle_width//2, low_y), color, 1)

            if candle_width > 2:
                cv2.rectangle(image, (x, min(open_y, close_y)),
                             (x + candle_width, max(open_y, close_y)), color, -1)

        return image

    def _create_volume_profile_chart(self, data: pd.DataFrame) -> np.ndarray:
        """åˆ›å»ºæˆäº¤é‡åˆ†å¸ƒå›¾"""
        height, width = self.image_size
        image = np.ones((height, width), dtype=np.uint8) * 255

        if 'volume' not in data.columns:
            return image

        # æ ‡å‡†åŒ–æˆäº¤é‡
        volumes = data['volume'].values
        max_volume = volumes.max()
        normalized_volumes = volumes / max_volume if max_volume > 0 else volumes

        # ç»˜åˆ¶æˆäº¤é‡æŸ±çŠ¶å›¾
        bar_width = max(1, (width - 20) // len(volumes))

        for i, volume in enumerate(normalized_volumes):
            x = 10 + i * bar_width
            bar_height = int(volume * (height - 20))
            y = height - 10 - bar_height

            cv2.rectangle(image, (x, y), (x + bar_width, height - 10), 128, -1)

        return image

    def detect_chart_patterns(self, chart_image: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹å›¾è¡¨æ¨¡å¼"""
        print("ğŸ” æ£€æµ‹å›¾è¡¨æ¨¡å¼...")

        pattern_results = {
            'detected_patterns': [],
            'pattern_confidences': {},
            'pattern_locations': {},
            'visual_features': {}
        }

        # æ·±åº¦å­¦ä¹ æ–¹æ³•
        if TORCH_AVAILABLE and self.pattern_recognition_model is not None:
            dl_patterns = self._detect_patterns_deep_learning(chart_image)
            pattern_results['detected_patterns'].extend(dl_patterns['patterns'])
            pattern_results['pattern_confidences'].update(dl_patterns['confidences'])

        # ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ–¹æ³•
        if CV_AVAILABLE:
            cv_patterns = self._detect_patterns_computer_vision(chart_image)
            pattern_results['detected_patterns'].extend(cv_patterns['patterns'])
            pattern_results['pattern_locations'].update(cv_patterns['locations'])

        # æå–è§†è§‰ç‰¹å¾
        visual_features = self._extract_visual_features(chart_image)
        pattern_results['visual_features'] = visual_features

        # å»é‡
        pattern_results['detected_patterns'] = list(set(pattern_results['detected_patterns']))

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.detection_stats['patterns_detected'] = len(pattern_results['detected_patterns'])

        return pattern_results

    def _detect_patterns_deep_learning(self, image: np.ndarray) -> Dict[str, Any]:
        """åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å¼æ£€æµ‹"""
        if not TORCH_AVAILABLE or self.pattern_recognition_model is None:
            return {'patterns': [], 'confidences': {}}

        try:
            # é¢„å¤„ç†å›¾åƒ
            tensor_image = self._preprocess_image_for_dl(image)

            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.pattern_recognition_model(tensor_image)
                probabilities = F.softmax(outputs, dim=1)
                predictions = probabilities.squeeze().numpy()

            # è·å–æ£€æµ‹ç»“æœ
            detected_patterns = []
            confidences = {}

            for i, (pattern_type, confidence) in enumerate(zip(self.pattern_types, predictions)):
                if confidence > self.detection_threshold:
                    detected_patterns.append(pattern_type)
                    confidences[pattern_type] = float(confidence)

            return {'patterns': detected_patterns, 'confidences': confidences}

        except Exception as e:
            print(f"æ·±åº¦å­¦ä¹ æ¨¡å¼æ£€æµ‹å¤±è´¥: {e}")
            return {'patterns': [], 'confidences': {}}

    def _detect_patterns_computer_vision(self, image: np.ndarray) -> Dict[str, Any]:
        """åŸºäºä¼ ç»Ÿè®¡ç®—æœºè§†è§‰çš„æ¨¡å¼æ£€æµ‹"""
        if not CV_AVAILABLE:
            return {'patterns': [], 'locations': {}}

        try:
            # è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(image, 50, 150)

            # ç›´çº¿æ£€æµ‹
            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50,
                                   minLineLength=30, maxLineGap=10)

            detected_patterns = []
            locations = {}

            if lines is not None:
                # åˆ†æçº¿æ¡æ¨¡å¼
                line_patterns = self._analyze_line_patterns(lines, image.shape)
                detected_patterns.extend(line_patterns['patterns'])
                locations.update(line_patterns['locations'])

            # å‡ ä½•å½¢çŠ¶æ£€æµ‹
            geometric_patterns = self._detect_geometric_shapes(image)
            detected_patterns.extend(geometric_patterns['patterns'])
            locations.update(geometric_patterns['locations'])

            return {'patterns': detected_patterns, 'locations': locations}

        except Exception as e:
            print(f"ä¼ ç»Ÿè§†è§‰æ¨¡å¼æ£€æµ‹å¤±è´¥: {e}")
            return {'patterns': [], 'locations': {}}

    def _analyze_line_patterns(self, lines, image_shape) -> Dict[str, Any]:
        """åˆ†æçº¿æ¡æ¨¡å¼"""
        patterns = []
        locations = {}

        if lines is None:
            return {'patterns': [], 'locations': {}}

        # åˆ†æçº¿æ¡è§’åº¦å’Œé•¿åº¦
        angles = []
        lengths = []

        for line in lines:
            x1, y1, x2, y2 = line[0]
            angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi
            length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

            angles.append(angle)
            lengths.append(length)

        # æ£€æµ‹è¶‹åŠ¿çº¿æ¨¡å¼
        if len(angles) > 3:
            # æ£€æµ‹æ”¯æ’‘çº¿å’Œé˜»åŠ›çº¿
            horizontal_lines = [angle for angle in angles if abs(angle) < 10 or abs(angle) > 170]
            if len(horizontal_lines) > len(angles) * 0.3:
                patterns.append('support_resistance')
                locations['support_resistance'] = {'count': len(horizontal_lines)}

            # æ£€æµ‹è¶‹åŠ¿é€šé“
            angle_std = np.std(angles)
            if angle_std < 20:  # è§’åº¦ä¸€è‡´æ€§é«˜
                avg_angle = np.mean(angles)
                if -30 < avg_angle < 30:  # è¿‘ä¼¼æ°´å¹³
                    patterns.append('horizontal_channel')
                elif avg_angle > 30:
                    patterns.append('ascending_channel')
                elif avg_angle < -30:
                    patterns.append('descending_channel')

                locations['channel'] = {'angle': avg_angle, 'consistency': 1.0 - angle_std/45}

        return {'patterns': patterns, 'locations': locations}

    def _detect_geometric_shapes(self, image: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹å‡ ä½•å½¢çŠ¶"""
        patterns = []
        locations = {}

        try:
            # åœ†å½¢æ£€æµ‹ï¼ˆç”¨äºæ£€æµ‹åº•éƒ¨å½¢æ€ï¼‰
            circles = cv2.HoughCircles(image, cv2.HOUGH_GRADIENT, 1, 20,
                                     param1=50, param2=30, minRadius=10, maxRadius=100)

            if circles is not None:
                patterns.append('rounding_bottom')
                locations['rounding_bottom'] = {'count': len(circles[0])}

            # ä¸‰è§’å½¢æ£€æµ‹
            contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            for contour in contours:
                if len(contour) >= 3:
                    # è¿‘ä¼¼å¤šè¾¹å½¢
                    epsilon = 0.04 * cv2.arcLength(contour, True)
                    approx = cv2.approxPolyDP(contour, epsilon, True)

                    if len(approx) == 3:
                        patterns.append('triangle')
                        locations['triangle'] = {'vertices': 3, 'area': cv2.contourArea(contour)}
                    elif len(approx) == 4:
                        patterns.append('rectangle')
                        locations['rectangle'] = {'vertices': 4, 'area': cv2.contourArea(contour)}

        except Exception as e:
            print(f"å‡ ä½•å½¢çŠ¶æ£€æµ‹å¤±è´¥: {e}")

        return {'patterns': patterns, 'locations': locations}

    def _extract_visual_features(self, image: np.ndarray) -> Dict[str, Any]:
        """æå–è§†è§‰ç‰¹å¾"""
        features = {}

        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features['mean_intensity'] = np.mean(image)
        features['std_intensity'] = np.std(image)
        features['entropy'] = self._calculate_entropy(image)

        # çº¹ç†ç‰¹å¾
        features['texture_energy'] = self._calculate_texture_energy(image)
        features['texture_contrast'] = self._calculate_texture_contrast(image)

        # å½¢çŠ¶ç‰¹å¾
        features['aspect_ratio'] = image.shape[1] / image.shape[0]
        features['center_of_mass'] = ndimage.center_of_mass(image)

        # é¢‘åŸŸç‰¹å¾
        features['dominant_frequency'] = self._calculate_dominant_frequency(image)

        return features

    def _calculate_entropy(self, image: np.ndarray) -> float:
        """è®¡ç®—å›¾åƒç†µ"""
        histogram = cv2.calcHist([image], [0], None, [256], [0, 256])
        histogram = histogram / histogram.sum()
        entropy = -np.sum(histogram * np.log2(histogram + 1e-10))
        return entropy

    def _calculate_texture_energy(self, image: np.ndarray) -> float:
        """è®¡ç®—çº¹ç†èƒ½é‡"""
        # ä½¿ç”¨ç°åº¦å…±ç”ŸçŸ©é˜µçš„ç®€åŒ–ç‰ˆæœ¬
        return np.sum(image**2) / image.size

    def _calculate_texture_contrast(self, image: np.ndarray) -> float:
        """è®¡ç®—çº¹ç†å¯¹æ¯”åº¦"""
        return np.std(image)

    def _calculate_dominant_frequency(self, image: np.ndarray) -> float:
        """è®¡ç®—ä¸»é¢‘ç‡"""
        f_transform = np.fft.fft2(image)
        f_shift = np.fft.fftshift(f_transform)
        magnitude_spectrum = np.abs(f_shift)

        # æ‰¾åˆ°ä¸»é¢‘ç‡
        max_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)
        return np.sqrt(max_idx[0]**2 + max_idx[1]**2)

    def _preprocess_image_for_dl(self, image: np.ndarray) -> torch.Tensor:
        """ä¸ºæ·±åº¦å­¦ä¹ é¢„å¤„ç†å›¾åƒ"""
        # è°ƒæ•´å¤§å°
        if image.shape != self.image_size:
            image = cv2.resize(image, self.image_size)

        # å½’ä¸€åŒ–
        image = image.astype(np.float32) / 255.0

        # æ·»åŠ é€šé“ç»´åº¦
        if len(image.shape) == 2:
            image = np.expand_dims(image, axis=0)

        # è½¬æ¢ä¸ºtensor
        tensor = torch.FloatTensor(image).unsqueeze(0)

        return tensor

    def detect_trend_lines(self, chart_image: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹è¶‹åŠ¿çº¿"""
        print("ğŸ“ˆ æ£€æµ‹è¶‹åŠ¿çº¿...")

        if not CV_AVAILABLE:
            return {'trend_lines': [], 'support_resistance': []}

        try:
            # è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(chart_image, 50, 150)

            # ç›´çº¿æ£€æµ‹
            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50,
                                   minLineLength=30, maxLineGap=10)

            trend_lines = []
            support_resistance = []

            if lines is not None:
                for line in lines:
                    x1, y1, x2, y2 = line[0]

                    # è®¡ç®—çº¿æ¡å±æ€§
                    angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi
                    length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

                    line_info = {
                        'start': (x1, y1),
                        'end': (x2, y2),
                        'angle': angle,
                        'length': length,
                        'type': self._classify_trend_line(angle)
                    }

                    # åˆ†ç±»è¶‹åŠ¿çº¿
                    if line_info['type'] == 'support':
                        support_resistance.append(line_info)
                    elif line_info['type'] == 'resistance':
                        support_resistance.append(line_info)
                    else:
                        trend_lines.append(line_info)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.detection_stats['trend_lines_found'] = len(trend_lines)
            self.detection_stats['support_resistance_levels'] = len(support_resistance)

            return {
                'trend_lines': trend_lines,
                'support_resistance': support_resistance
            }

        except Exception as e:
            print(f"è¶‹åŠ¿çº¿æ£€æµ‹å¤±è´¥: {e}")
            return {'trend_lines': [], 'support_resistance': []}

    def _classify_trend_line(self, angle: float) -> str:
        """åˆ†ç±»è¶‹åŠ¿çº¿"""
        # æ ‡å‡†åŒ–è§’åº¦åˆ°[-90, 90]
        while angle > 90:
            angle -= 180
        while angle < -90:
            angle += 180

        if abs(angle) < 15:
            return 'support_resistance'
        elif angle > 15:
            return 'resistance'
        elif angle < -15:
            return 'support'
        else:
            return 'neutral'

    def generate_visual_signals(self, market_data: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆè§†è§‰äº¤æ˜“ä¿¡å·"""
        print("ğŸ¯ ç”Ÿæˆè§†è§‰äº¤æ˜“ä¿¡å·...")

        # åˆ›å»ºå›¾è¡¨å›¾åƒ
        chart_image = self.create_chart_image(market_data)

        # æ£€æµ‹æ¨¡å¼
        patterns = self.detect_chart_patterns(chart_image)

        # æ£€æµ‹è¶‹åŠ¿çº¿
        trend_lines = self.detect_trend_lines(chart_image)

        # ç”Ÿæˆä¿¡å·
        signals = []
        confidences = []
        pattern_signals = []

        for i in range(len(market_data)):
            # åŸºäºæ¨¡å¼ç”Ÿæˆä¿¡å·
            pattern_signal = self._generate_pattern_signal(patterns['detected_patterns'])
            pattern_signals.append(pattern_signal)

            # åŸºäºè¶‹åŠ¿çº¿ç”Ÿæˆä¿¡å·
            trend_signal = self._generate_trend_signal(trend_lines)

            # ç»¼åˆä¿¡å·
            combined_signal = 0.6 * pattern_signal + 0.4 * trend_signal
            signal = np.tanh(combined_signal)

            # ç½®ä¿¡åº¦
            confidence = self._calculate_visual_confidence(patterns, trend_lines)

            signals.append(signal)
            confidences.append(confidence)

        signals_df = pd.DataFrame({
            'visual_signal': signals,
            'confidence': confidences,
            'pattern_count': [len(patterns['detected_patterns'])] * len(signals),
            'trend_line_count': [len(trend_lines['trend_lines'])] * len(signals),
            'support_resistance_count': [len(trend_lines['support_resistance'])] * len(signals)
        }, index=market_data.index)

        return signals_df

    def _generate_pattern_signal(self, detected_patterns: List[str]) -> float:
        """åŸºäºæ¨¡å¼ç”Ÿæˆä¿¡å·"""
        pattern_signals = {
            'head_and_shoulders': -0.8,
            'inverse_head_and_shoulders': 0.8,
            'double_top': -0.7,
            'double_bottom': 0.7,
            'triangle': 0.3,
            'ascending_triangle': 0.6,
            'descending_triangle': -0.6,
            'flag': 0.4,
            'pennant': 0.4,
            'cup_and_handle': 0.7,
            'rounding_bottom': 0.6
        }

        signal = 0.0
        for pattern in detected_patterns:
            if pattern in pattern_signals:
                signal += pattern_signals[pattern]

        return np.clip(signal, -1.0, 1.0)

    def _generate_trend_signal(self, trend_lines: Dict) -> float:
        """åŸºäºè¶‹åŠ¿çº¿ç”Ÿæˆä¿¡å·"""
        signal = 0.0

        # åˆ†æè¶‹åŠ¿çº¿
        up_trends = [line for line in trend_lines['trend_lines'] if line['angle'] > 10]
        down_trends = [line for line in trend_lines['trend_lines'] if line['angle'] < -10]

        if len(up_trends) > len(down_trends):
            signal = 0.5
        elif len(down_trends) > len(up_trends):
            signal = -0.5

        return signal

    def _calculate_visual_confidence(self, patterns: Dict, trend_lines: Dict) -> float:
        """è®¡ç®—è§†è§‰ç½®ä¿¡åº¦"""
        confidence = 0.0

        # åŸºäºæ£€æµ‹åˆ°çš„æ¨¡å¼æ•°é‡
        pattern_count = len(patterns['detected_patterns'])
        confidence += min(pattern_count * 0.2, 0.6)

        # åŸºäºè¶‹åŠ¿çº¿ç½®ä¿¡åº¦
        trend_count = len(trend_lines['trend_lines'])
        confidence += min(trend_count * 0.1, 0.3)

        # åŸºäºæ”¯æ’‘é˜»åŠ›ä½
        sr_count = len(trend_lines['support_resistance'])
        confidence += min(sr_count * 0.1, 0.1)

        return min(confidence, 1.0)

    def get_visual_insights(self) -> Dict[str, Any]:
        """è·å–è§†è§‰åˆ†ææ´å¯Ÿ"""
        return {
            'image_size': self.image_size,
            'pattern_types': self.pattern_types,
            'detection_threshold': self.detection_threshold,
            'detection_stats': self.detection_stats,
            'recent_patterns': self.detected_patterns[-5:],  # æœ€è¿‘5ä¸ªæ¨¡å¼
            'visual_features': self.visual_features,
            'torch_available': TORCH_AVAILABLE,
            'cv_available': CV_AVAILABLE
        }

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'image_size': self.image_size,
            'pattern_types': self.pattern_types,
            'detection_threshold': self.detection_threshold,
            'detection_stats': self.detection_stats,
            'models_initialized': self.pattern_recognition_model is not None,
            'torch_available': TORCH_AVAILABLE,
            'cv_available': CV_AVAILABLE,
            'model_type': 'Computer Vision Market Analyzer'
        }

# ä¾¿æ·å‡½æ•°
def create_market_visual_analyzer(image_size: Tuple[int, int] = (224, 224),
                                pattern_types: List[str] = None,
                                detection_threshold: float = 0.7) -> MarketVisualAnalyzer:
    """åˆ›å»ºå¸‚åœºè§†è§‰åˆ†æå™¨å®ä¾‹"""
    analyzer = MarketVisualAnalyzer(image_size, pattern_types, detection_threshold)
    analyzer.initialize_models()
    return analyzer

def quick_visual_analysis(market_data: pd.DataFrame) -> Dict[str, Any]:
    """å¿«é€Ÿè§†è§‰åˆ†æ"""
    analyzer = MarketVisualAnalyzer()
    analyzer.initialize_models()

    # åˆ›å»ºå›¾è¡¨
    chart_image = analyzer.create_chart_image(market_data)

    # æ£€æµ‹æ¨¡å¼
    patterns = analyzer.detect_chart_patterns(chart_image)

    # ç”Ÿæˆä¿¡å·
    signals = analyzer.generate_visual_signals(market_data)

    return {
        'detected_patterns': patterns['detected_patterns'],
        'latest_signal': signals['visual_signal'].iloc[-1] if len(signals) > 0 else 0,
        'average_confidence': signals['confidence'].mean() if len(signals) > 0 else 0,
        'model_info': analyzer.get_model_info()
    }