"""
ç¥ç»ç¬¦å·é‡‘èæ¨ç†ç³»ç»Ÿ
Neuro-Symbolic Financial Reasoning System

ç»“åˆç¥ç»ç½‘ç»œä¸ç¬¦å·æ¨ç†çš„æ··åˆæ™ºèƒ½ç³»ç»Ÿï¼Œå®ç°å¯è§£é‡Šçš„é‡‘èå†³ç­–
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
    print("ğŸ§  PyTorch å·²å¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ä¸å¯ç”¨")

try:
    import sympy as sp
    from sympy import symbols, Eq, solve, sympify
    SYMPY_AVAILABLE = True
    print("ğŸ”¢ SymPy å·²å¯ç”¨")
except ImportError:
    SYMPY_AVAILABLE = False
    print("âš ï¸ SymPy ä¸å¯ç”¨")

class NeuroSymbolicFinancialReasoner:
    """
    ç¥ç»ç¬¦å·é‡‘èæ¨ç†å™¨

    ç»“åˆæ·±åº¦å­¦ä¹ çš„æ¨¡å¼è¯†åˆ«èƒ½åŠ›ä¸ç¬¦å·æ¨ç†çš„é€»è¾‘è§£é‡Šèƒ½åŠ›
    """

    def __init__(self,
                 neural_hidden_dims: List[int] = [128, 64, 32],
                 symbolic_rules: List[str] = None,
                 reasoning_depth: int = 3,
                 confidence_threshold: float = 0.7):
        """
        åˆå§‹åŒ–ç¥ç»ç¬¦å·æ¨ç†å™¨

        Args:
            neural_hidden_dims: ç¥ç»ç½‘ç»œéšè—å±‚ç»´åº¦
            symbolic_rules: ç¬¦å·è§„åˆ™åˆ—è¡¨
            reasoning_depth: æ¨ç†æ·±åº¦
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.neural_hidden_dims = neural_hidden_dims
        self.symbolic_rules = symbolic_rules or self._default_financial_rules()
        self.reasoning_depth = reasoning_depth
        self.confidence_threshold = confidence_threshold

        # ç¥ç»ç½‘ç»œç»„ä»¶
        self.neural_network = None
        self.symbolic_reasoner = SymbolicReasoner(self.symbolic_rules)

        # æ¨ç†ç»“æœ
        self.reasoning_trace = []
        self.explanation_chain = []
        self.confidence_scores = {}

        # çŸ¥è¯†å›¾è°±
        self.knowledge_graph = FinancialKnowledgeGraph()

        if TORCH_AVAILABLE:
            self._build_neural_network()

    def _default_financial_rules(self) -> List[str]:
        """é»˜è®¤é‡‘èè§„åˆ™"""
        return [
            "IF price > sma_20 AND rsi < 30 THEN buy_signal",
            "IF price < sma_20 AND rsi > 70 THEN sell_signal",
            "IF volume > avg_volume * 1.5 AND price_change > 0 THEN strong_buy",
            "IF volatility > avg_volatility * 2 THEN risk_warning",
            "IF correlation > 0.8 THEN market_sync",
            "IF dividend_yield > 0.05 THEN value_investment"
        ]

    def _build_neural_network(self):
        """æ„å»ºç¥ç»ç½‘ç»œ"""
        if not TORCH_AVAILABLE:
            return

        class NeuralFeatureExtractor(nn.Module):
            def __init__(self, input_dim: int, hidden_dims: List[int]):
                super().__init__()
                layers = []
                prev_dim = input_dim

                for hidden_dim in hidden_dims:
                    layers.extend([
                        nn.Linear(prev_dim, hidden_dim),
                        nn.ReLU(),
                        nn.BatchNorm1d(hidden_dim),
                        nn.Dropout(0.2)
                    ])
                    prev_dim = hidden_dim

                layers.append(nn.Linear(prev_dim, hidden_dims[-1]))
                self.network = nn.Sequential(*layers)

            def forward(self, x):
                return self.network(x)

        self.neural_network = NeuralFeatureExtractor(10, self.neural_hidden_dims)

    def extract_neural_features(self, market_data: pd.DataFrame) -> Dict[str, np.ndarray]:
        """æå–ç¥ç»ç½‘ç»œç‰¹å¾"""
        print("ğŸ§  æå–ç¥ç»ç½‘ç»œç‰¹å¾...")

        features = {}

        # åŸºç¡€å¸‚åœºç‰¹å¾
        if 'close' in market_data.columns:
            prices = market_data['close'].dropna()
            if len(prices) > 0:
                features['price_momentum'] = self._calculate_momentum(prices)
                features['volatility_regime'] = self._classify_volatility_regime(prices)
                features['trend_strength'] = self._calculate_trend_strength(prices)

        # å¤šå˜é‡ç‰¹å¾
        numeric_columns = market_data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) >= 2:
            correlation_matrix = market_data[numeric_columns].corr()
            features['market_structure'] = self._analyze_market_structure(correlation_matrix)
            features['dependency_patterns'] = self._extract_dependency_patterns(correlation_matrix)

        # æ—¶åºæ¨¡å¼ç‰¹å¾
        if len(market_data) > 20:
            features['temporal_patterns'] = self._extract_temporal_patterns(market_data)
            features['cycle_components'] = self._decompose_cycles(market_data)

        return features

    def symbolic_reasoning(self,
                          neural_features: Dict[str, np.ndarray],
                          market_data: pd.DataFrame) -> Dict[str, Any]:
        """ç¬¦å·æ¨ç†"""
        print("ğŸ” æ‰§è¡Œç¬¦å·æ¨ç†...")

        reasoning_results = {}

        # å°†ç¥ç»ç½‘ç»œç‰¹å¾è½¬æ¢ä¸ºç¬¦å·è¡¨ç¤º
        symbolic_facts = self._neural_to_symbolic(neural_features)

        # åº”ç”¨è§„åˆ™æ¨ç†
        for rule in self.symbolic_rules:
            rule_result = self.symbolic_reasoner.apply_rule(rule, symbolic_facts, market_data)
            reasoning_results[rule] = rule_result

        # é€»è¾‘æ¨ç†é“¾
        logical_chain = self._build_reasoning_chain(symbolic_facts, reasoning_results)

        # å†²çªæ£€æµ‹ä¸è§£å†³
        conflicts = self._detect_conflicts(reasoning_results)
        resolved_conflicts = self._resolve_conflicts(conflicts, logical_chain)

        reasoning_results['logical_chain'] = logical_chain
        reasoning_results['conflicts'] = conflicts
        reasoning_results['resolved_conflicts'] = resolved_conflicts

        return reasoning_results

    def hybrid_reasoning(self,
                        market_data: pd.DataFrame,
                        query: str = None) -> Dict[str, Any]:
        """æ··åˆæ¨ç†ä¸»å‡½æ•°"""
        print("ğŸ¤– æ‰§è¡Œç¥ç»ç¬¦å·æ··åˆæ¨ç†...")

        # ç¬¬ä¸€æ­¥ï¼šç¥ç»ç½‘ç»œç‰¹å¾æå–
        neural_features = self.extract_neural_features(market_data)

        # ç¬¬äºŒæ­¥ï¼šç¬¦å·æ¨ç†
        symbolic_results = self.symbolic_reasoning(neural_features, market_data)

        # ç¬¬ä¸‰æ­¥ï¼šçŸ¥è¯†å›¾è°±å¢å¼º
        knowledge_enhancement = self.knowledge_graph.enhance_reasoning(
            neural_features, symbolic_results
        )

        # ç¬¬å››æ­¥ï¼šå†³ç­–èåˆ
        final_decision = self._fuse_decisions(neural_features, symbolic_results, knowledge_enhancement)

        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆè§£é‡Š
        explanation = self._generate_explanation(
            neural_features, symbolic_results, final_decision, query
        )

        # ç¬¬å…­æ­¥ï¼šç½®ä¿¡åº¦è¯„ä¼°
        confidence_assessment = self._assess_confidence(
            neural_features, symbolic_results, final_decision
        )

        results = {
            'neural_features': neural_features,
            'symbolic_reasoning': symbolic_results,
            'knowledge_enhancement': knowledge_enhancement,
            'final_decision': final_decision,
            'explanation': explanation,
            'confidence': confidence_assessment,
            'reasoning_trace': self.reasoning_trace,
            'timestamp': pd.Timestamp.now()
        }

        return results

    def _calculate_momentum(self, prices: pd.Series) -> float:
        """è®¡ç®—åŠ¨é‡ç‰¹å¾"""
        if len(prices) < 5:
            return 0.0

        returns = prices.pct_change().dropna()
        if len(returns) == 0:
            return 0.0

        # å¤šæ—¶é—´å°ºåº¦åŠ¨é‡
        momentum_5 = returns.iloc[-5:].mean() if len(returns) >= 5 else 0
        momentum_10 = returns.iloc[-10:].mean() if len(returns) >= 10 else 0
        momentum_20 = returns.iloc[-20:].mean() if len(returns) >= 20 else 0

        # åŠ æƒåˆæˆ
        total_weight = 0
        weighted_momentum = 0

        if len(returns) >= 5:
            weighted_momentum += momentum_5 * 0.5
            total_weight += 0.5
        if len(returns) >= 10:
            weighted_momentum += momentum_10 * 0.3
            total_weight += 0.3
        if len(returns) >= 20:
            weighted_momentum += momentum_20 * 0.2
            total_weight += 0.2

        return weighted_momentum / total_weight if total_weight > 0 else 0

    def _classify_volatility_regime(self, prices: pd.Series) -> str:
        """åˆ†ç±»æ³¢åŠ¨ç‡åˆ¶åº¦"""
        if len(prices) < 10:
            return "unknown"

        returns = prices.pct_change().dropna()
        volatility = returns.std()

        # åŸºäºå†å²åˆ†ä½æ•°åˆ†ç±»
        if volatility < np.percentile(returns.std(), 25):
            return "low_volatility"
        elif volatility < np.percentile(returns.std(), 75):
            return "normal_volatility"
        else:
            return "high_volatility"

    def _calculate_trend_strength(self, prices: pd.Series) -> float:
        """è®¡ç®—è¶‹åŠ¿å¼ºåº¦"""
        if len(prices) < 10:
            return 0.0

        # çº¿æ€§å›å½’è¶‹åŠ¿
        x = np.arange(len(prices))
        y = prices.values

        slope, intercept = np.polyfit(x, y, 1)

        # è¶‹åŠ¿å¼ºåº¦æ ‡å‡†åŒ–
        trend_strength = slope / np.mean(y) if np.mean(y) != 0 else 0

        return np.clip(trend_strength * 100, -1, 1)

    def _analyze_market_structure(self, correlation_matrix: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºç»“æ„"""
        structure = {}

        # ç›¸å…³æ€§èšç±»
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                if abs(correlation_matrix.iloc[i, j]) > 0.7:
                    high_corr_pairs.append((
                        correlation_matrix.columns[i],
                        correlation_matrix.columns[j],
                        correlation_matrix.iloc[i, j]
                    ))

        structure['high_correlation_pairs'] = high_corr_pairs
        structure['market_clustering'] = self._cluster_market_structure(correlation_matrix)
        structure['centrality_measures'] = self._calculate_centrality(correlation_matrix)

        return structure

    def _extract_dependency_patterns(self, correlation_matrix: pd.DataFrame) -> Dict[str, Any]:
        """æå–ä¾èµ–æ¨¡å¼"""
        patterns = {}

        # ä¸»æˆåˆ†åˆ†æ
        try:
            eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix.fillna(0))
            patterns['principal_components'] = {
                'eigenvalues': eigenvalues.real,
                'explained_variance_ratio': eigenvalues.real / np.sum(eigenvalues.real)
            }
        except:
            patterns['principal_components'] = {'eigenvalues': [], 'explained_variance_ratio': []}

        # ä¾èµ–ç½‘ç»œå±æ€§
        patterns['network_density'] = self._calculate_network_density(correlation_matrix)
        patterns['dependency_strength'] = np.mean(np.abs(correlation_matrix.values))

        return patterns

    def _extract_temporal_patterns(self, market_data: pd.DataFrame) -> Dict[str, Any]:
        """æå–æ—¶åºæ¨¡å¼"""
        patterns = {}

        if 'close' in market_data.columns:
            prices = market_data['close'].dropna()
            if len(prices) > 20:
                # è‡ªç›¸å…³åˆ†æ
                autocorr = [prices.autocorr(lag=i) for i in range(1, min(11, len(prices)//2))]
                patterns['autocorrelation'] = autocorr

                # å­£èŠ‚æ€§æ£€æµ‹
                patterns['seasonality'] = self._detect_seasonality(prices)

        return patterns

    def _decompose_cycles(self, market_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†è§£å‘¨æœŸæˆåˆ†"""
        cycles = {}

        if 'close' in market_data.columns and len(market_data) > 50:
            prices = market_data['close'].dropna()

            # ç®€åŒ–çš„å‘¨æœŸåˆ†è§£
            try:
                # FFTé¢‘è°±åˆ†æ
                fft_values = np.fft.fft(prices.values)
                frequencies = np.fft.fftfreq(len(prices))

                # ä¸»é¢‘ç‡è¯†åˆ«
                power_spectrum = np.abs(fft_values) ** 2
                dominant_freq_idx = np.argmax(power_spectrum[1:len(power_spectrum)//2]) + 1

                cycles['dominant_frequency'] = frequencies[dominant_freq_idx]
                cycles['dominant_period'] = 1 / frequencies[dominant_freq_idx] if frequencies[dominant_freq_idx] != 0 else 0
                cycles['power_spectrum'] = power_spectrum[:len(power_spectrum)//2]

            except Exception as e:
                cycles['error'] = str(e)

        return cycles

    def _neural_to_symbolic(self, neural_features: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """ç¥ç»ç½‘ç»œç‰¹å¾è½¬ç¬¦å·è¡¨ç¤º"""
        symbolic_facts = {}

        for feature_name, feature_value in neural_features.items():
            if isinstance(feature_value, (int, float)):
                # æ•°å€¼ç‰¹å¾ç¬¦å·åŒ–
                symbolic_facts[feature_name] = self._quantize_feature(feature_value, feature_name)
            elif isinstance(feature_value, dict):
                # å­—å…¸ç‰¹å¾é€’å½’å¤„ç†
                for sub_key, sub_value in feature_value.items():
                    symbolic_facts[f"{feature_name}_{sub_key}"] = self._quantize_feature(sub_value, f"{feature_name}_{sub_key}")

        return symbolic_facts

    def _quantize_feature(self, value: float, feature_name: str) -> str:
        """ç‰¹å¾é‡åŒ–ä¸ºç¬¦å·"""
        if abs(value) < 1e-6:
            return "zero"
        elif value > 0:
            if value < 0.3:
                return "low_positive"
            elif value < 0.7:
                return "medium_positive"
            else:
                return "high_positive"
        else:
            if value > -0.3:
                return "low_negative"
            elif value > -0.7:
                return "medium_negative"
            else:
                return "high_negative"

    def _cluster_market_structure(self, correlation_matrix: pd.DataFrame) -> List[List[str]]:
        """å¸‚åœºç»“æ„èšç±»"""
        # ç®€åŒ–çš„å±‚æ¬¡èšç±»
        clusters = []
        assets = correlation_matrix.columns.tolist()

        # åŸºäºé«˜ç›¸å…³æ€§åˆ†ç»„
        used_assets = set()
        for asset in assets:
            if asset in used_assets:
                continue

            cluster = [asset]
            used_assets.add(asset)

            # æ‰¾é«˜ç›¸å…³èµ„äº§
            for other_asset in assets:
                if other_asset not in used_assets:
                    correlation = correlation_matrix.loc[asset, other_asset]
                    if not np.isnan(correlation) and abs(correlation) > 0.7:
                        cluster.append(other_asset)
                        used_assets.add(other_asset)

            if len(cluster) > 1:
                clusters.append(cluster)

        return clusters

    def _calculate_centrality(self, correlation_matrix: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—ä¸­å¿ƒæ€§"""
        centralities = {}

        # åº¦ä¸­å¿ƒæ€§ï¼ˆé«˜ç›¸å…³æ€§è¿æ¥æ•°ï¼‰
        for asset in correlation_matrix.columns:
            high_corr_count = np.sum(np.abs(correlation_matrix[asset]) > 0.7) - 1  # å‡å»è‡ªèº«
            centralities[asset] = high_corr_count

        return centralities

    def _calculate_network_density(self, correlation_matrix: pd.DataFrame) -> float:
        """è®¡ç®—ç½‘ç»œå¯†åº¦"""
        n = len(correlation_matrix)
        if n <= 1:
            return 0.0

        # å¼ºè¿æ¥æ•°ï¼ˆç›¸å…³ç³»æ•°ç»å¯¹å€¼ > 0.5ï¼‰
        strong_connections = 0
        for i in range(n):
            for j in range(i+1, n):
                if abs(correlation_matrix.iloc[i, j]) > 0.5:
                    strong_connections += 1

        max_possible_connections = n * (n - 1) / 2
        return strong_connections / max_possible_connections if max_possible_connections > 0 else 0

    def _detect_seasonality(self, prices: pd.Series) -> Dict[str, Any]:
        """æ£€æµ‹å­£èŠ‚æ€§"""
        seasonality = {}

        if len(prices) < 20:
            return seasonality

        # ç®€å•çš„å­£èŠ‚æ€§æ£€æµ‹
        try:
            # è®¡ç®—ä¸åŒæ»åæœŸçš„è‡ªç›¸å…³
            seasonal_lags = [5, 10, 20]  # å‡è®¾çš„å‘¨æœŸ
            seasonal_strengths = {}

            for lag in seasonal_lags:
                if len(prices) > lag:
                    autocorr = prices.autocorr(lag=lag)
                    seasonal_strengths[f"lag_{lag}"] = autocorr if not np.isnan(autocorr) else 0

            seasonality['seasonal_strengths'] = seasonal_strengths
            seasonality['dominant_seasonality'] = max(seasonal_strengths.items(), key=lambda x: abs(x[1]))[0] if seasonal_strengths else None

        except Exception as e:
            seasonality['error'] = str(e)

        return seasonality

    def _build_reasoning_chain(self, facts: Dict[str, Any], rule_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ„å»ºæ¨ç†é“¾"""
        chain = []

        # äº‹å®åˆ°æ¨ç†çš„é“¾
        for fact_name, fact_value in facts.items():
            chain.append({
                'type': 'fact',
                'content': f"{fact_name} is {fact_value}",
                'confidence': 0.9
            })

        # è§„åˆ™åº”ç”¨
        for rule, result in rule_results.items():
            if isinstance(result, dict) and 'triggered' in result and result['triggered']:
                chain.append({
                    'type': 'rule_application',
                    'content': f"Rule '{rule}' triggered with confidence {result.get('confidence', 0)}",
                    'confidence': result.get('confidence', 0)
                })

        return chain

    def _detect_conflicts(self, reasoning_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ¨ç†å†²çª"""
        conflicts = []

        # æ£€æµ‹ç›¸åçš„ä¿¡å·
        buy_signals = []
        sell_signals = []

        for rule, result in reasoning_results.items():
            if isinstance(result, dict) and result.get('triggered', False):
                if 'buy' in rule.lower():
                    buy_signals.append((rule, result.get('confidence', 0)))
                elif 'sell' in rule.lower():
                    sell_signals.append((rule, result.get('confidence', 0)))

        # å¦‚æœåŒæ—¶å­˜åœ¨ä¹°å–ä¿¡å·ï¼Œæ ‡è®°ä¸ºå†²çª
        if buy_signals and sell_signals:
            conflicts.append({
                'type': 'buy_sell_conflict',
                'buy_signals': buy_signals,
                'sell_signals': sell_signals,
                'severity': 'high'
            })

        return conflicts

    def _resolve_conflicts(self, conflicts: List[Dict[str, Any]], reasoning_chain: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è§£å†³å†²çª"""
        resolved = {}

        for conflict in conflicts:
            if conflict['type'] == 'buy_sell_conflict':
                # åŸºäºç½®ä¿¡åº¦è§£å†³å†²çª
                buy_confidence = max([conf for _, conf in conflict['buy_signals']], default=0)
                sell_confidence = max([conf for _, conf in conflict['sell_signals']], default=0)

                if buy_confidence > sell_confidence:
                    resolved['resolution'] = 'prefer_buy'
                    resolved['confidence'] = buy_confidence
                elif sell_confidence > buy_confidence:
                    resolved['resolution'] = 'prefer_sell'
                    resolved['confidence'] = sell_confidence
                else:
                    resolved['resolution'] = 'no_action'
                    resolved['confidence'] = 0

        return resolved

    def _fuse_decisions(self,
                       neural_features: Dict[str, np.ndarray],
                       symbolic_results: Dict[str, Any],
                       knowledge_enhancement: Dict[str, Any]) -> Dict[str, Any]:
        """èåˆå†³ç­–"""

        # ç¥ç»ç½‘ç»œå†³ç­–
        neural_decision = self._neural_decision(neural_features)

        # ç¬¦å·æ¨ç†å†³ç­–
        symbolic_decision = self._symbolic_decision(symbolic_results)

        # çŸ¥è¯†å›¾è°±å¢å¼ºå†³ç­–
        knowledge_decision = self._knowledge_decision(knowledge_enhancement)

        # åŠ æƒèåˆ
        final_decision = {
            'signal': self._weighted_signal_fusion(neural_decision, symbolic_decision, knowledge_decision),
            'confidence': self._calculate_fusion_confidence(neural_decision, symbolic_decision, knowledge_decision),
            'neural_contribution': neural_decision,
            'symbolic_contribution': symbolic_decision,
            'knowledge_contribution': knowledge_decision,
            'fusion_weights': {
                'neural': 0.4,
                'symbolic': 0.4,
                'knowledge': 0.2
            }
        }

        return final_decision

    def _neural_decision(self, neural_features: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """ç¥ç»ç½‘ç»œå†³ç­–"""
        decision = {'signal': 0, 'confidence': 0}

        # åŸºäºç¥ç»ç½‘ç»œç‰¹å¾çš„ç®€å•å†³ç­–é€»è¾‘
        if 'price_momentum' in neural_features:
            momentum = neural_features['price_momentum']
            if isinstance(momentum, (int, float)):
                decision['signal'] += np.sign(momentum) * 0.5
                decision['confidence'] += abs(momentum) * 0.3

        if 'trend_strength' in neural_features:
            trend = neural_features['trend_strength']
            if isinstance(trend, (int, float)):
                decision['signal'] += trend * 0.3
                decision['confidence'] += abs(trend) * 0.2

        decision['signal'] = np.clip(decision['signal'], -1, 1)
        decision['confidence'] = np.clip(decision['confidence'], 0, 1)

        return decision

    def _symbolic_decision(self, symbolic_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç¬¦å·æ¨ç†å†³ç­–"""
        decision = {'signal': 0, 'confidence': 0}

        # ç»Ÿè®¡è§¦å‘çš„è§„åˆ™
        triggered_rules = []
        for rule, result in symbolic_results.items():
            if isinstance(result, dict) and result.get('triggered', False):
                triggered_rules.append((rule, result.get('confidence', 0)))

        if triggered_rules:
            # åŸºäºç½®ä¿¡åº¦åŠ æƒ
            total_confidence = sum(conf for _, conf in triggered_rules)
            if total_confidence > 0:
                for rule, conf in triggered_rules:
                    if 'buy' in rule.lower():
                        decision['signal'] += (conf / total_confidence) * 1.0
                    elif 'sell' in rule.lower():
                        decision['signal'] -= (conf / total_confidence) * 1.0

                decision['confidence'] = total_confidence / len(triggered_rules)

        decision['signal'] = np.clip(decision['signal'], -1, 1)
        decision['confidence'] = np.clip(decision['confidence'], 0, 1)

        return decision

    def _knowledge_decision(self, knowledge_enhancement: Dict[str, Any]) -> Dict[str, Any]:
        """çŸ¥è¯†å›¾è°±å†³ç­–"""
        decision = {'signal': 0, 'confidence': 0}

        # ç®€åŒ–çš„çŸ¥è¯†å›¾è°±å†³ç­–
        if 'similar_patterns' in knowledge_enhancement:
            similar_patterns = knowledge_enhancement['similar_patterns']
            if similar_patterns:
                avg_outcome = np.mean([p.get('outcome', 0) for p in similar_patterns])
                decision['signal'] = avg_outcome * 0.3
                decision['confidence'] = 0.5

        return decision

    def _weighted_signal_fusion(self, neural: Dict, symbolic: Dict, knowledge: Dict) -> float:
        """åŠ æƒä¿¡å·èåˆ"""
        weights = {'neural': 0.4, 'symbolic': 0.4, 'knowledge': 0.2}

        fused_signal = (
            weights['neural'] * neural['signal'] +
            weights['symbolic'] * symbolic['signal'] +
            weights['knowledge'] * knowledge['signal']
        )

        return np.clip(fused_signal, -1, 1)

    def _calculate_fusion_confidence(self, neural: Dict, symbolic: Dict, knowledge: Dict) -> float:
        """è®¡ç®—èåˆç½®ä¿¡åº¦"""
        weights = {'neural': 0.4, 'symbolic': 0.4, 'knowledge': 0.2}

        fused_confidence = (
            weights['neural'] * neural['confidence'] +
            weights['symbolic'] * symbolic['confidence'] +
            weights['knowledge'] * knowledge['confidence']
        )

        return np.clip(fused_confidence, 0, 1)

    def _generate_explanation(self,
                            neural_features: Dict[str, np.ndarray],
                            symbolic_results: Dict[str, Any],
                            final_decision: Dict[str, Any],
                            query: str = None) -> str:
        """ç”Ÿæˆè§£é‡Š"""
        explanation_parts = []

        # ç¥ç»ç½‘ç»œéƒ¨åˆ†è§£é‡Š
        explanation_parts.append("ğŸ§  ç¥ç»ç½‘ç»œåˆ†æ:")
        if 'price_momentum' in neural_features:
            momentum = neural_features['price_momentum']
            if isinstance(momentum, (int, float)):
                direction = "ä¸Šæ¶¨" if momentum > 0 else "ä¸‹è·Œ"
                explanation_parts.append(f"  - ä»·æ ¼åŠ¨é‡æ˜¾ç¤º{direction}è¶‹åŠ¿ï¼Œå¼ºåº¦: {abs(momentum):.3f}")

        # ç¬¦å·æ¨ç†éƒ¨åˆ†è§£é‡Š
        explanation_parts.append("ğŸ” ç¬¦å·æ¨ç†ç»“æœ:")
        triggered_rules = []
        for rule, result in symbolic_results.items():
            if isinstance(result, dict) and result.get('triggered', False):
                triggered_rules.append(rule)

        if triggered_rules:
            for rule in triggered_rules[:3]:  # æ˜¾ç¤ºå‰3ä¸ªè§„åˆ™
                explanation_parts.append(f"  - è§¦å‘è§„åˆ™: {rule}")
        else:
            explanation_parts.append("  - æœªè§¦å‘æ˜æ˜¾è§„åˆ™")

        # æœ€ç»ˆå†³ç­–è§£é‡Š
        explanation_parts.append("ğŸ¯ æœ€ç»ˆå†³ç­–:")
        signal_direction = "ä¹°å…¥" if final_decision['signal'] > 0.1 else "å–å‡º" if final_decision['signal'] < -0.1 else "æŒæœ‰"
        explanation_parts.append(f"  - å»ºè®®{signal_direction}ï¼Œä¿¡å·å¼ºåº¦: {abs(final_decision['signal']):.3f}")
        explanation_parts.append(f"  - ç½®ä¿¡åº¦: {final_decision['confidence']:.3f}")

        return "\n".join(explanation_parts)

    def _assess_confidence(self,
                           neural_features: Dict[str, np.ndarray],
                           symbolic_results: Dict[str, Any],
                           final_decision: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°ç½®ä¿¡åº¦"""
        assessment = {
            'overall_confidence': final_decision['confidence'],
            'confidence_components': {
                'neural_confidence': final_decision['neural_contribution']['confidence'],
                'symbolic_confidence': final_decision['symbolic_contribution']['confidence'],
                'knowledge_confidence': final_decision['knowledge_contribution']['confidence']
            },
            'confidence_factors': []
        }

        # ç½®ä¿¡åº¦å› å­åˆ†æ
        if final_decision['confidence'] > 0.8:
            assessment['confidence_factors'].append("é«˜ç½®ä¿¡åº¦ï¼šå¤šç³»ç»Ÿä¸€è‡´ç¡®è®¤")
        elif final_decision['confidence'] > 0.5:
            assessment['confidence_factors'].append("ä¸­ç­‰ç½®ä¿¡åº¦ï¼šéƒ¨åˆ†ç³»ç»Ÿç¡®è®¤")
        else:
            assessment['confidence_factors'].append("ä½ç½®ä¿¡åº¦ï¼šç³»ç»Ÿæ„è§ä¸ä¸€è‡´")

        # ç¥ç»ç½‘ç»œç‰¹å¾è´¨é‡
        neural_quality = len(neural_features) / 10.0  # ç®€åŒ–çš„è´¨é‡è¯„ä¼°
        assessment['confidence_factors'].append(f"ç¥ç»ç½‘ç»œç‰¹å¾è´¨é‡: {neural_quality:.2f}")

        # ç¬¦å·è§„åˆ™ä¸€è‡´æ€§
        triggered_count = sum(1 for result in symbolic_results.values()
                           if isinstance(result, dict) and result.get('triggered', False))
        rule_consistency = triggered_count / len(self.symbolic_rules) if self.symbolic_rules else 0
        assessment['confidence_factors'].append(f"ç¬¦å·è§„åˆ™ä¸€è‡´æ€§: {rule_consistency:.2f}")

        return assessment

    def get_reasoning_summary(self) -> Dict[str, Any]:
        """è·å–æ¨ç†æ€»ç»“"""
        return {
            'total_reasoning_steps': len(self.reasoning_trace),
            'explanation_depth': len(self.explanation_chain),
            'symbolic_rules_count': len(self.symbolic_rules),
            'confidence_threshold': self.confidence_threshold,
            'model_info': self.get_model_info()
        }

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_type': 'Neuro-Symbolic Financial Reasoner',
            'neural_network_available': TORCH_AVAILABLE,
            'symbolic_reasoning_available': SYMPY_AVAILABLE,
            'hidden_dimensions': self.neural_hidden_dims,
            'reasoning_depth': self.reasoning_depth,
            'rules_count': len(self.symbolic_rules),
            'knowledge_graph_nodes': len(getattr(self.knowledge_graph, 'nodes', {}))
        }


class SymbolicReasoner:
    """ç¬¦å·æ¨ç†å™¨"""

    def __init__(self, rules: List[str]):
        self.rules = rules
        self.facts = {}
        self.inference_engine = InferenceEngine()

    def apply_rule(self, rule: str, facts: Dict[str, Any], market_data: pd.DataFrame) -> Dict[str, Any]:
        """åº”ç”¨è§„åˆ™"""
        try:
            # è§£æè§„åˆ™
            conditions, conclusion = self._parse_rule(rule)

            # æ£€æŸ¥æ¡ä»¶
            conditions_met = self._check_conditions(conditions, facts, market_data)

            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_rule_confidence(conditions_met, conditions)

            return {
                'triggered': conditions_met,
                'confidence': confidence,
                'conditions_checked': conditions,
                'conclusion': conclusion
            }

        except Exception as e:
            return {
                'triggered': False,
                'confidence': 0,
                'error': str(e)
            }

    def _parse_rule(self, rule: str) -> Tuple[List[str], str]:
        """è§£æè§„åˆ™"""
        if 'IF' in rule and 'THEN' in rule:
            parts = rule.split('THEN')
            condition_part = parts[0].replace('IF', '').strip()
            conclusion = parts[1].strip()

            # ç®€å•çš„æ¡ä»¶è§£æ
            conditions = [cond.strip() for cond in condition_part.split('AND')]
            return conditions, conclusion
        else:
            return [], rule

    def _check_conditions(self, conditions: List[str], facts: Dict[str, Any], market_data: pd.DataFrame) -> bool:
        """æ£€æŸ¥æ¡ä»¶"""
        for condition in conditions:
            if not self._evaluate_condition(condition, facts, market_data):
                return False
        return True

    def _evaluate_condition(self, condition: str, facts: Dict[str, Any], market_data: pd.DataFrame) -> bool:
        """è¯„ä¼°å•ä¸ªæ¡ä»¶"""
        try:
            # ç®€åŒ–çš„æ¡ä»¶è¯„ä¼°
            if '>' in condition:
                var, value = condition.split('>')
                var = var.strip()
                value = float(value.strip())

                # ä»factsæˆ–market_dataè·å–å€¼
                if var in facts:
                    return float(str(facts[var]).replace('positive_', '1').replace('negative_', '-1')) > value
                elif var in market_data.columns:
                    return float(market_data[var].iloc[-1]) > value

            elif '<' in condition:
                var, value = condition.split('<')
                var = var.strip()
                value = float(value.strip())

                if var in facts:
                    return float(str(facts[var]).replace('positive_', '1').replace('negative_', '-1')) < value
                elif var in market_data.columns:
                    return float(market_data[var].iloc[-1]) < value

            return False

        except:
            return False

    def _calculate_rule_confidence(self, conditions_met: bool, conditions: List[str]) -> float:
        """è®¡ç®—è§„åˆ™ç½®ä¿¡åº¦"""
        if not conditions_met:
            return 0.0

        # åŸºäºæ¡ä»¶æ•°é‡å’ŒåŒ¹é…åº¦çš„ç®€å•ç½®ä¿¡åº¦è®¡ç®—
        base_confidence = 0.7
        condition_bonus = min(len(conditions) * 0.1, 0.3)

        return min(base_confidence + condition_bonus, 1.0)


class InferenceEngine:
    """æ¨ç†å¼•æ“"""
    def forward_chaining(self, facts: Dict[str, Any], rules: List[str]) -> Dict[str, Any]:
        """å‰å‘é“¾æ¨ç†"""
        new_facts = facts.copy()
        derived_facts = {}

        for rule in rules:
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å‰å‘é“¾æ¨ç†
            pass

        return derived_facts

    def backward_chaining(self, goal: str, facts: Dict[str, Any], rules: List[str]) -> bool:
        """åå‘é“¾æ¨ç†"""
        # è¿™é‡Œå¯ä»¥å®ç°åå‘é“¾æ¨ç†
        return False


class FinancialKnowledgeGraph:
    """é‡‘èçŸ¥è¯†å›¾è°±"""

    def __init__(self):
        self.nodes = {}
        self.edges = {}
        self.concepts = self._initialize_financial_concepts()

    def _initialize_financial_concepts(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–é‡‘èæ¦‚å¿µ"""
        return {
            'market_regime': ['bull_market', 'bear_market', 'sideways_market'],
            'technical_patterns': ['head_and_shoulders', 'double_top', 'triangle'],
            'risk_factors': ['volatility', 'liquidity', 'correlation'],
            'market_anomalies': ['momentum_anomaly', 'value_anomaly', 'size_anomaly']
        }

    def enhance_reasoning(self,
                          neural_features: Dict[str, np.ndarray],
                          symbolic_results: Dict[str, Any]) -> Dict[str, Any]:
        """çŸ¥è¯†å›¾è°±å¢å¼ºæ¨ç†"""
        enhancement = {}

        # æ¨¡å¼åŒ¹é…
        enhancement['similar_patterns'] = self._find_similar_patterns(neural_features)

        # æ¦‚å¿µå…³è”
        enhancement['concept_relations'] = self._find_concept_relations(symbolic_results)

        # å†å²æ¡ˆä¾‹
        enhancement['historical_cases'] = self._find_historical_cases(neural_features)

        return enhancement

    def _find_similar_patterns(self, features: Dict[str, np.ndarray]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ¨¡å¼"""
        # ç®€åŒ–çš„æ¨¡å¼åŒ¹é…
        similar_patterns = []

        if 'trend_strength' in features:
            trend = features['trend_strength']
            if isinstance(trend, (int, float)) and abs(trend) > 0.5:
                similar_patterns.append({
                    'pattern_type': 'strong_trend',
                    'similarity': abs(trend),
                    'outcome': 1.0 if trend > 0 else -1.0
                })

        return similar_patterns

    def _find_concept_relations(self, symbolic_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾æ¦‚å¿µå…³è”"""
        relations = []

        # ç®€åŒ–çš„æ¦‚å¿µå…³è”
        for rule, result in symbolic_results.items():
            if isinstance(result, dict) and result.get('triggered', False):
                if 'trend' in rule.lower():
                    relations.append({
                        'concept': 'trend_analysis',
                        'related_concepts': ['momentum', 'volatility', 'market_regime']
                    })

        return relations

    def _find_historical_cases(self, features: Dict[str, np.ndarray]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾å†å²æ¡ˆä¾‹"""
        # è¿™é‡Œåº”è¯¥è¿æ¥åˆ°å†å²æ•°æ®åº“
        return []


# ä¾¿æ·å‡½æ•°
def create_neuro_symbolic_reasoner(rules: List[str] = None) -> NeuroSymbolicFinancialReasoner:
    """åˆ›å»ºç¥ç»ç¬¦å·æ¨ç†å™¨å®ä¾‹"""
    return NeuroSymbolicFinancialReasoner(symbolic_rules=rules)

def quick_financial_reasoning(market_data: pd.DataFrame, query: str = None) -> Dict[str, Any]:
    """å¿«é€Ÿé‡‘èæ¨ç†"""
    reasoner = create_neuro_symbolic_reasoner()
    return reasoner.hybrid_reasoning(market_data, query)

def analyze_with_explanation(market_data: pd.DataFrame, query: str = None) -> str:
    """å¸¦è§£é‡Šçš„åˆ†æ"""
    results = quick_financial_reasoning(market_data, query)
    return results.get('explanation', 'åˆ†æå¤±è´¥')