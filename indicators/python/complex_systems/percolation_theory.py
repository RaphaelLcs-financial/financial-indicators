"""
é‡‘èæ¸—æµç†è®ºæŒ‡æ ‡
Financial Percolation Theory Indicator

åŸºäºæ¸—æµç†è®ºçš„å¸‚åœºç›¸å˜å’Œç³»ç»Ÿæ€§é£é™©åˆ†æ
"""

import numpy as np
import pandas as pd
import networkx as nx
from typing import Dict, Any, List, Tuple, Optional
from scipy import stats
from scipy.spatial.distance import pdist, squareform
import warnings
warnings.filterwarnings('ignore')

class FinancialPercolationAnalyzer:
    """
    é‡‘èæ¸—æµåˆ†æå™¨

    åŸºäºæ¸—æµç†è®ºåˆ†æå¸‚åœºç›¸å˜ã€ä¸´ç•Œç°è±¡å’Œç³»ç»Ÿæ€§é£é™©
    æ£€æµ‹å¸‚åœºçŠ¶æ€çš„çªç„¶è½¬å˜å’Œæ³¡æ²«å½¢æˆ
    """

    def __init__(self, window_size: int = 50, correlation_threshold: float = 0.7):
        """
        åˆå§‹åŒ–æ¸—æµåˆ†æå™¨

        Args:
            window_size: åˆ†æçª—å£å¤§å°
            correlation_threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼ˆæ¸—æµé˜ˆå€¼ï¼‰
        """
        self.window_size = window_size
        self.correlation_threshold = correlation_threshold

        # æ¸—æµå‚æ•°
        self.critical_exponents = {
            'beta': 0.4,     # åºå‚é‡ä¸´ç•ŒæŒ‡æ•°
            'gamma': 1.2,   # ç£åŒ–ç‡ä¸´ç•ŒæŒ‡æ•°
            'nu': 0.8,      # å…³è”é•¿åº¦ä¸´ç•ŒæŒ‡æ•°
            'alpha': 0.1    # æ¯”çƒ­ä¸´ç•ŒæŒ‡æ•°
        }

        # åˆ†æç»“æœå­˜å‚¨
        self.percolation_history = []
        self.phase_transitions = []
        self.critical_points = []

    def _calculate_correlation_matrix(self, data: pd.DataFrame) -> np.ndarray:
        """è®¡ç®—èµ„äº§ç›¸å…³æ€§çŸ©é˜µ"""
        returns = data.pct_change().dropna()

        # å¦‚æœæ˜¯å•èµ„äº§æ—¶é—´åºåˆ—ï¼Œåˆ›å»ºæ»šåŠ¨çª—å£ç›¸å…³æ€§
        if returns.shape[1] == 1:
            # ä½¿ç”¨ä¸åŒæ»åæœŸçš„ç›¸å…³æ€§
            correlation_matrix = np.zeros((self.window_size, self.window_size))

            for i in range(self.window_size):
                for j in range(self.window_size):
                    if i != j:
                        # è®¡ç®—ä¸åŒæ—¶é—´ç‚¹çš„ç›¸å…³æ€§
                        series1 = returns.iloc[-(self.window_size + i):-(i) if i > 0 else len(returns)]
                        series2 = returns.iloc[-(self.window_size + j):-(j) if j > 0 else len(returns)]
                        min_len = min(len(series1), len(series2))
                        if min_len > 10:
                            correlation_matrix[i, j] = np.corrcoef(series1[-min_len:], series2[-min_len:])[0, 1]
        else:
            # å¤šèµ„äº§ç›¸å…³æ€§çŸ©é˜µ
            correlation_matrix = returns.corr().fillna(0).values

        return np.abs(correlation_matrix)  # ä½¿ç”¨ç»å¯¹å€¼ç›¸å…³æ€§

    def _create_correlation_network(self, correlation_matrix: np.ndarray) -> nx.Graph:
        """åˆ›å»ºç›¸å…³æ€§ç½‘ç»œ"""
        G = nx.Graph()

        n_assets = correlation_matrix.shape[0]

        # æ·»åŠ èŠ‚ç‚¹
        for i in range(n_assets):
            G.add_node(i)

        # æ·»åŠ è¾¹ï¼ˆåŸºäºç›¸å…³æ€§é˜ˆå€¼ï¼‰
        for i in range(n_assets):
            for j in range(i + 1, n_assets):
                if correlation_matrix[i, j] > self.correlation_threshold:
                    G.add_edge(i, j, weight=correlation_matrix[i, j])

        return G

    def _calculate_percolation_parameters(self, G: nx.Graph) -> Dict[str, float]:
        """è®¡ç®—æ¸—æµå‚æ•°"""
        if len(G.nodes()) == 0:
            return {
                'largest_cluster_size': 0,
                'number_of_clusters': 0,
                'average_cluster_size': 0,
                'percolation_probability': 0,
                'correlation_length': 0
            }

        # è®¡ç®—è¿é€šåˆ†é‡
        connected_components = list(nx.connected_components(G))

        if not connected_components:
            return {
                'largest_cluster_size': 0,
                'number_of_clusters': 0,
                'average_cluster_size': 0,
                'percolation_probability': 0,
                'correlation_length': 0
            }

        # æœ€å¤§ç°‡å¤§å°
        largest_cluster = max(connected_components, key=len)
        largest_cluster_size = len(largest_cluster) / len(G.nodes())

        # ç°‡æ•°é‡
        number_of_clusters = len(connected_components)

        # å¹³å‡ç°‡å¤§å°
        average_cluster_size = np.mean([len(cluster) for cluster in connected_components])

        # æ¸—æµæ¦‚ç‡ï¼ˆæœ€å¤§ç°‡çš„ç›¸å¯¹å¤§å°ï¼‰
        percolation_probability = largest_cluster_size

        # å…³è”é•¿åº¦ï¼ˆä½¿ç”¨å¹³å‡æœ€çŸ­è·¯å¾„ï¼‰
        if len(G.nodes()) > 1 and len(G.edges()) > 0:
            try:
                correlation_length = nx.average_shortest_path_length(G)
            except:
                correlation_length = 0
        else:
            correlation_length = 0

        return {
            'largest_cluster_size': largest_cluster_size,
            'number_of_clusters': number_of_clusters,
            'average_cluster_size': average_cluster_size,
            'percolation_probability': percolation_probability,
            'correlation_length': correlation_length
        }

    def _detect_critical_point(self, percolation_params: Dict[str, float],
                            history: List[Dict[str, float]]) -> bool:
        """æ£€æµ‹ä¸´ç•Œç‚¹"""
        if len(history) < 5:
            return False

        # è®¡ç®—æœ€è¿‘æ¸—æµæ¦‚ç‡çš„å˜åŒ–ç‡
        recent_percolation = [p['percolation_probability'] for p in history[-5:]]
        percolation_change = np.diff(recent_percolation)

        # æ£€æµ‹çªç„¶å˜åŒ–ï¼ˆç›¸å˜ç‰¹å¾ï¼‰
        sudden_change_threshold = 0.1
        if np.any(np.abs(percolation_change) > sudden_change_threshold):
            return True

        # æ£€æµ‹ä¸´ç•Œç‚¹é™„è¿‘çš„å¹‚å¾‹è¡Œä¸º
        current_percolation = percolation_params['percolation_probability']
        if 0.3 < current_percolation < 0.7:  # ä¸´ç•ŒåŒºåŸŸ
            # æ£€æŸ¥ç°‡å¤§å°åˆ†å¸ƒæ˜¯å¦ç¬¦åˆå¹‚å¾‹
            if self._check_power_law_behavior(percolation_params):
                return True

        return False

    def _check_power_law_behavior(self, percolation_params: Dict[str, float]) -> bool:
        """æ£€æŸ¥å¹‚å¾‹è¡Œä¸º"""
        # ç®€åŒ–çš„å¹‚å¾‹æ£€æµ‹
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è¿›è¡Œæ›´ä¸¥æ ¼çš„ç»Ÿè®¡æ£€éªŒ

        # æ£€æŸ¥æ ‡åº¦å…³ç³»
        expected_scaling = 0.5  # æœŸæœ›çš„æ ‡åº¦æŒ‡æ•°

        # åŸºäºæ¸—æµç†è®ºçš„æ ‡åº¦å…³ç³»
        largest_cluster = percolation_params['largest_cluster_size']
        correlation_length = percolation_params['correlation_length']

        if correlation_length > 0:
            scaling_ratio = largest_cluster / (correlation_length ** expected_scaling)
            # å¦‚æœåœ¨åˆç†èŒƒå›´å†…ï¼Œè®¤ä¸ºç¬¦åˆå¹‚å¾‹
            return 0.1 < scaling_ratio < 10

        return False

    def _calculate_order_parameter(self, percolation_params: Dict[str, float]) -> float:
        """è®¡ç®—åºå‚é‡"""
        # åœ¨æ¸—æµç†è®ºä¸­ï¼Œåºå‚é‡é€šå¸¸æ˜¯æœ€å¤§ç°‡çš„å¤§å°
        return percolation_params['largest_cluster_size']

    def _calculate_susceptibility(self, percolation_params: Dict[str, float],
                                 history: List[Dict[str, float]]) -> float:
        """è®¡ç®—ç£åŒ–ç‡ï¼ˆ susceptibilityï¼‰"""
        if len(history) < 2:
            return 0

        # ç£åŒ–ç‡å®šä¹‰ä¸ºåºå‚é‡çš„æ¶¨è½
        recent_order_parameters = [self._calculate_order_parameter(p) for p in history[-10:]]

        if len(recent_order_parameters) > 1:
            susceptibility = np.var(recent_order_parameters)
        else:
            susceptibility = 0

        return susceptibility

    def analyze_market_phase(self, data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºç›¸æ€"""
        print("ğŸŒ€ å¼€å§‹æ¸—æµç†è®ºå¸‚åœºåˆ†æ...")

        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = self._calculate_correlation_matrix(data)

        # åˆ›å»ºç›¸å…³æ€§ç½‘ç»œ
        correlation_network = self._create_correlation_network(correlation_matrix)

        # è®¡ç®—æ¸—æµå‚æ•°
        percolation_params = self._calculate_percolation_parameters(correlation_network)

        # æ£€æµ‹ä¸´ç•Œç‚¹
        is_critical = self._detect_critical_point(percolation_params, self.percolation_history)

        # è®¡ç®—åºå‚é‡å’Œç£åŒ–ç‡
        order_parameter = self._calculate_order_parameter(percolation_params)
        susceptibility = self._calculate_susceptibility(percolation_params, self.percolation_history)

        # ä¿å­˜å†å²
        self.percolation_history.append(percolation_params.copy())

        # æ£€æµ‹ç›¸å˜
        if is_critical:
            self.critical_points.append(len(self.percolation_history) - 1)

        # ç¡®å®šå¸‚åœºç›¸æ€
        market_phase = self._determine_market_phase(order_parameter, susceptibility, is_critical)

        # è®¡ç®—ç³»ç»Ÿæ€§é£é™©æŒ‡æ ‡
        systemic_risk = self._calculate_systemic_risk(percolation_params, correlation_network)

        # ç”Ÿæˆäº¤æ˜“ä¿¡å·
        trading_signal = self._generate_trading_signal(market_phase, systemic_risk, order_parameter)

        results = {
            'timestamp': data.index[-1] if hasattr(data.index, '__getitem__') else len(data),
            'market_phase': market_phase,
            'order_parameter': order_parameter,
            'susceptibility': susceptibility,
            'is_critical': is_critical,
            'systemic_risk': systemic_risk,
            'trading_signal': trading_signal,
            'percolation_parameters': percolation_params,
            'network_metrics': self._calculate_network_metrics(correlation_network),
            'phase_transition_probability': self._calculate_phase_transition_probability()
        }

        return results

    def _determine_market_phase(self, order_parameter: float, susceptibility: float,
                               is_critical: bool) -> str:
        """ç¡®å®šå¸‚åœºç›¸æ€"""
        if is_critical:
            return "CRITICAL"

        # åŸºäºåºå‚é‡å’Œç£åŒ–ç‡ç¡®å®šç›¸æ€
        if order_parameter < 0.3:
            if susceptibility > 0.1:
                return "DISORDERED_HIGH_VOLATILITY"
            else:
                return "DISORDERED_LOW_VOLATILITY"
        elif order_parameter > 0.7:
            if susceptibility > 0.1:
                return "ORDERED_HIGH_CORRELATION"
            else:
                return "ORDERED_STABLE"
        else:
            return "TRANSITIONAL"

    def _calculate_systemic_risk(self, percolation_params: Dict[str, float],
                               network: nx.Graph) -> float:
        """è®¡ç®—ç³»ç»Ÿæ€§é£é™©"""
        # åŸºäºæ¸—æµå‚æ•°çš„ç³»ç»Ÿæ€§é£é™©æŒ‡æ ‡
        risk_components = []

        # 1. æ¸—æµé£é™©ï¼ˆå¤§ç°‡å½¢æˆï¼‰
        percolation_risk = percolation_params['percolation_probability']
        risk_components.append(percolation_risk)

        # 2. ç½‘ç»œå¯†åº¦é£é™©
        if len(network.nodes()) > 1:
            density = nx.density(network)
            risk_components.append(density)

        # 3. è¿é€šæ€§é£é™©
        if len(network.nodes()) > 0:
            connectivity = len(network.edges()) / (len(network.nodes()) * (len(network.nodes()) - 1) / 2)
            risk_components.append(connectivity)

        # 4. é›†ä¸­é£é™©
        if len(network.nodes()) > 0:
            centrality = nx.degree_centrality(network)
            centrality_concentration = np.std(list(centrality.values()))
            risk_components.append(centrality_concentration)

        # ç»¼åˆé£é™©æŒ‡æ ‡
        if risk_components:
            systemic_risk = np.mean(risk_components)
        else:
            systemic_risk = 0

        return systemic_risk

    def _calculate_network_metrics(self, network: nx.Graph) -> Dict[str, float]:
        """è®¡ç®—ç½‘ç»œæŒ‡æ ‡"""
        if len(network.nodes()) == 0:
            return {
                'density': 0,
                'clustering_coefficient': 0,
                'average_path_length': 0,
                'assortativity': 0,
                'modularity': 0
            }

        metrics = {}

        # ç½‘ç»œå¯†åº¦
        metrics['density'] = nx.density(network)

        # èšç±»ç³»æ•°
        try:
            metrics['clustering_coefficient'] = nx.average_clustering(network)
        except:
            metrics['clustering_coefficient'] = 0

        # å¹³å‡è·¯å¾„é•¿åº¦
        if len(network.edges()) > 0:
            try:
                metrics['average_path_length'] = nx.average_shortest_path_length(network)
            except:
                metrics['average_path_length'] = 0
        else:
            metrics['average_path_length'] = 0

        # åŒé…æ€§
        try:
            metrics['assortativity'] = nx.degree_assortativity_coefficient(network)
        except:
            metrics['assortativity'] = 0

        # æ¨¡å—åº¦
        try:
            communities = nx.community.greedy_modularity_communities(network)
            metrics['modularity'] = nx.community.modularity(network, communities)
        except:
            metrics['modularity'] = 0

        return metrics

    def _calculate_phase_transition_probability(self) -> float:
        """è®¡ç®—ç›¸å˜æ¦‚ç‡"""
        if len(self.percolation_history) < 10:
            return 0

        # åŸºäºå†å²æ•°æ®è®¡ç®—ç›¸å˜æ¦‚ç‡
        recent_order_params = [p['percolation_probability'] for p in self.percolation_history[-10:]]

        # è®¡ç®—å˜åŒ–ç‡
        changes = np.diff(recent_order_params)
        volatility = np.std(changes)

        # ç›¸å˜æ¦‚ç‡ä¸æ³¢åŠ¨æ€§æ­£ç›¸å…³
        transition_prob = min(volatility * 10, 1.0)

        return transition_prob

    def _generate_trading_signal(self, market_phase: str, systemic_risk: float,
                               order_parameter: float) -> int:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        # åŸºäºå¸‚åœºç›¸æ€å’Œç³»ç»Ÿæ€§é£é™©çš„äº¤æ˜“é€»è¾‘

        if market_phase == "CRITICAL":
            # ä¸´ç•Œç‚¹ï¼šè§‚æœ›æˆ–å‡ä»“
            return -1 if systemic_risk > 0.7 else 0

        elif market_phase == "DISORDERED_HIGH_VOLATILITY":
            # é«˜æ³¢åŠ¨æ— åºçŠ¶æ€ï¼šè°¨æ…æˆ–åå‘æ“ä½œ
            return 1 if systemic_risk < 0.5 else -1

        elif market_phase == "ORDERED_HIGH_CORRELATION":
            # é«˜ç›¸å…³æ€§æœ‰åºçŠ¶æ€ï¼šé£é™©è¾ƒé«˜ï¼Œè€ƒè™‘å‡ä»“
            return -1 if systemic_risk > 0.6 else 0

        elif market_phase == "ORDERED_STABLE":
            # ç¨³å®šæœ‰åºçŠ¶æ€ï¼šé€‚åˆè¶‹åŠ¿è·Ÿè¸ª
            return 1

        elif market_phase == "TRANSITIONAL":
            # è¿‡æ¸¡çŠ¶æ€ï¼šè§‚æœ›
            return 0

        else:
            return 0

    def analyze_time_series(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ—¶é—´åºåˆ—åˆ†æ"""
        print(f"ğŸ“ˆ å¼€å§‹æ—¶é—´åºåˆ—æ¸—æµåˆ†æï¼ˆ{len(data)}ä¸ªæ•°æ®ç‚¹ï¼‰...")

        results = []

        # æ»šåŠ¨çª—å£åˆ†æ
        for i in range(self.window_size, len(data)):
            window_data = data.iloc[i - self.window_size:i]

            try:
                analysis_result = self.analyze_market_phase(window_data)
                results.append(analysis_result)
            except Exception as e:
                print(f"åˆ†æçª—å£ {i} æ—¶å‡ºé”™: {e}")
                continue

        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(results)

        if not results_df.empty:
            # è®¡ç®—é¢å¤–æŒ‡æ ‡
            results_df['phase_change'] = results_df['market_phase'].ne(
                results_df['market_phase'].shift()).fillna(False).astype(int)

            # è®¡ç®—é£é™©ç­‰çº§
            results_df['risk_level'] = self._calculate_risk_level(results_df)

        return results_df

    def _calculate_risk_level(self, results_df: pd.DataFrame) -> pd.Series:
        """è®¡ç®—é£é™©ç­‰çº§"""
        risk_scores = []

        for idx, row in results_df.iterrows():
            risk_score = 0

            # åŸºäºå¸‚åœºç›¸æ€
            phase_risk = {
                'CRITICAL': 5,
                'DISORDERED_HIGH_VOLATILITY': 4,
                'ORDERED_HIGH_CORRELATION': 4,
                'TRANSITIONAL': 3,
                'DISORDERED_LOW_VOLATILITY': 2,
                'ORDERED_STABLE': 1
            }
            risk_score += phase_risk.get(row['market_phase'], 0)

            # åŸºäºç³»ç»Ÿæ€§é£é™©
            risk_score += row['systemic_risk'] * 5

            # åŸºäºä¸´ç•Œç‚¹çŠ¶æ€
            if row['is_critical']:
                risk_score += 2

            risk_scores.append(min(risk_score, 10))  # æœ€å¤§é£é™©ç­‰çº§ä¸º10

        return pd.Series(risk_scores, index=results_df.index)

    def get_critical_points_analysis(self) -> Dict[str, Any]:
        """è·å–ä¸´ç•Œç‚¹åˆ†æ"""
        if not self.critical_points:
            return {'critical_points': [], 'analysis': 'No critical points detected'}

        analysis = {
            'critical_points': self.critical_points,
            'number_of_critical_points': len(self.critical_points),
            'critical_point_density': len(self.critical_points) / max(len(self.percolation_history), 1),
            'average_distance_between_critical_points': np.mean(np.diff(self.critical_points)) if len(self.critical_points) > 1 else 0
        }

        return analysis

    def reset_analysis(self):
        """é‡ç½®åˆ†æçŠ¶æ€"""
        self.percolation_history = []
        self.phase_transitions = []
        self.critical_points = []

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'window_size': self.window_size,
            'correlation_threshold': self.correlation_threshold,
            'critical_exponents': self.critical_exponents,
            'analysis_history_length': len(self.percolation_history),
            'number_of_critical_points': len(self.critical_points),
            'model_type': 'Financial Percolation Theory Analyzer'
        }

# ä¾¿æ·å‡½æ•°
def create_percolation_analyzer(window_size: int = 50,
                              correlation_threshold: float = 0.7) -> FinancialPercolationAnalyzer:
    """åˆ›å»ºæ¸—æµåˆ†æå™¨å®ä¾‹"""
    return FinancialPercolationAnalyzer(window_size, correlation_threshold)

def quick_percolation_analysis(data: pd.DataFrame) -> Dict[str, Any]:
    """å¿«é€Ÿæ¸—æµåˆ†æ"""
    analyzer = FinancialPercolationAnalyzer()

    # æ‰§è¡Œåˆ†æ
    result = analyzer.analyze_market_phase(data)

    # è·å–æ¨¡å‹ä¿¡æ¯
    model_info = analyzer.get_model_info()

    return {
        'analysis_result': result,
        'model_info': model_info,
        'critical_points_analysis': analyzer.get_critical_points_analysis()
    }