"""
æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦äº¤æ˜“æŒ‡æ ‡
Deep Deterministic Policy Gradient Trader

åŸºäºDDPGç®—æ³•çš„è¿ç»­åŠ¨ä½œç©ºé—´å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Dict, Any, List, Tuple
from collections import deque
import random
import warnings
warnings.filterwarnings('ignore')

class ActorNetwork(nn.Module):
    """Actorç½‘ç»œ - ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

        # æ‰¹å½’ä¸€åŒ–
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)

        # åˆå§‹åŒ–æƒé‡
        self.initialize_weights()

    def initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        x = F.relu(self.bn1(self.fc1(state)))
        x = F.relu(self.bn2(self.fc2(x)))
        # ä½¿ç”¨tanhæ¿€æ´»å‡½æ•°å°†è¾“å‡ºé™åˆ¶åœ¨[-1, 1]
        action = torch.tanh(self.fc3(x))
        return action

class CriticNetwork(nn.Module):
    """Criticç½‘ç»œ - ä»·å€¼ç½‘ç»œ"""
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(CriticNetwork, self).__init__()
        # Q1ç½‘ç»œ
        self.q1_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q1_fc3 = nn.Linear(hidden_dim, 1)

        # Q2ç½‘ç»œï¼ˆç”¨äºåŒQå­¦ä¹ ï¼‰
        self.q2_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.q2_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q2_fc3 = nn.Linear(hidden_dim, 1)

        # æ‰¹å½’ä¸€åŒ–
        self.q1_bn1 = nn.BatchNorm1d(hidden_dim)
        self.q1_bn2 = nn.BatchNorm1d(hidden_dim)
        self.q2_bn1 = nn.BatchNorm1d(hidden_dim)
        self.q2_bn2 = nn.BatchNorm1d(hidden_dim)

        self.initialize_weights()

    def initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ï¼Œè¿”å›ä¸¤ä¸ªQå€¼"""
        x = torch.cat([state, action], dim=1)

        # Q1
        q1 = F.relu(self.q1_bn1(self.q1_fc1(x)))
        q1 = F.relu(self.q1_bn2(self.q1_fc2(q1)))
        q1 = self.q1_fc3(q1)

        # Q2
        q2 = F.relu(self.q2_bn1(self.q2_fc1(x)))
        q2 = F.relu(self.q2_bn2(self.q2_fc2(q2)))
        q2 = self.q2_fc3(q2)

        return q1, q2

    def q1_forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """ä»…ä½¿ç”¨Q1ç½‘ç»œ"""
        x = torch.cat([state, action], dim=1)
        q1 = F.relu(self.q1_bn1(self.q1_fc1(x)))
        q1 = F.relu(self.q1_bn2(self.q1_fc2(q1)))
        q1 = self.q1_fc3(q1)
        return q1

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, max_size: int = 100000):
        self.max_size = max_size
        self.buffer = deque(maxlen=max_size)

    def add(self, state: np.ndarray, action: np.ndarray, reward: float,
            next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """é‡‡æ ·æ‰¹é‡æ•°æ®"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones, dtype=np.float32)
        )

    def size(self) -> int:
        """ç¼“å†²åŒºå¤§å°"""
        return len(self.buffer)

class DDPGTrader:
    """
    DDPGäº¤æ˜“ç³»ç»Ÿ

    å®ç°æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œæ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´
    åŠ¨ä½œï¼š[-1, 1] è¡¨ç¤ºæŒä»“æ¯”ä¾‹
    """

    def __init__(self, state_dim: int = 20, action_dim: int = 1,
                 hidden_dim: int = 256, learning_rate: float = 1e-4):
        """
        åˆå§‹åŒ–DDPGäº¤æ˜“ç³»ç»Ÿ

        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            learning_rate: å­¦ä¹ ç‡
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.lr = learning_rate

        # è®¾å¤‡é…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆ›å»ºç½‘ç»œ
        self.actor = ActorNetwork(state_dim, action_dim, hidden_dim).to(self.device)
        self.critic = CriticNetwork(state_dim, action_dim, hidden_dim).to(self.device)

        # åˆ›å»ºç›®æ ‡ç½‘ç»œ
        self.target_actor = ActorNetwork(state_dim, action_dim, hidden_dim).to(self.device)
        self.target_critic = CriticNetwork(state_dim, action_dim, hidden_dim).to(self.device)

        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic.load_state_dict(self.critic.state_dict())

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.lr)

        # è¶…å‚æ•°
        self.gamma = 0.99  # æŠ˜æ‰£å› å­
        self.tau = 0.005   # è½¯æ›´æ–°ç³»æ•°
        self.batch_size = 64
        self.memory_size = 100000

        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(self.memory_size)

        # å™ªå£°å‚æ•°ï¼ˆOUå™ªå£°ï¼‰
        self.ou_noise = OUNoise(action_dim)

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'actor_losses': [],
            'critic_losses': [],
            'rewards': [],
            'episode_lengths': []
        }

        # è®­ç»ƒçŠ¶æ€
        self.is_trained = False
        self.total_steps = 0

    def _create_state(self, data: pd.DataFrame, idx: int) -> np.ndarray:
        """åˆ›å»ºçŠ¶æ€å‘é‡"""
        if idx < 20:  # éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
            return np.zeros(self.state_dim)

        # æå–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
        prices = data['close'].iloc[max(0, idx-20):idx+1].values
        volumes = data['volume'].iloc[max(0, idx-20):idx+1].values if 'volume' in data.columns else None

        # è®¡ç®—ç‰¹å¾
        features = []

        # 1. ä»·æ ¼ç‰¹å¾
        features.extend([
            prices[-1] / prices[-2] - 1,  # å½“å‰æ”¶ç›Šç‡
            prices[-1] / prices[-5] - 1,  # 5æ—¥æ”¶ç›Šç‡
            prices[-1] / prices[-10] - 1, # 10æ—¥æ”¶ç›Šç‡
            prices[-1] / prices[-20] - 1, # 20æ—¥æ”¶ç›Šç‡
        ])

        # 2. ç§»åŠ¨å¹³å‡çº¿ç‰¹å¾
        features.extend([
            prices[-1] / np.mean(prices[-5:]) - 1,   # ç›¸å¯¹5æ—¥å‡çº¿
            prices[-1] / np.mean(prices[-10:]) - 1,  # ç›¸å¯¹10æ—¥å‡çº¿
            prices[-1] / np.mean(prices[-20:]) - 1,  # ç›¸å¯¹20æ—¥å‡çº¿
        ])

        # 3. æ³¢åŠ¨ç‡ç‰¹å¾
        returns = np.diff(prices) / prices[:-1]
        features.extend([
            np.std(returns[-5:]),    # çŸ­æœŸæ³¢åŠ¨ç‡
            np.std(returns[-10:]),   # ä¸­æœŸæ³¢åŠ¨ç‡
            np.std(returns[-20:]),   # é•¿æœŸæ³¢åŠ¨ç‡
        ])

        # 4. æŠ€æœ¯æŒ‡æ ‡
        if len(prices) >= 14:
            # RSI
            gains = returns[returns > 0]
            losses = -returns[returns < 0]
            avg_gain = np.mean(gains[-14:]) if len(gains) > 0 else 0
            avg_loss = np.mean(losses[-14:]) if len(losses) > 0 else 0
            if avg_loss > 0:
                rsi = 100 - (100 / (1 + avg_gain / avg_loss))
            else:
                rsi = 100
            features.append(rsi / 100)  # å½’ä¸€åŒ–
        else:
            features.append(0.5)

        # 5. æˆäº¤é‡ç‰¹å¾
        if volumes is not None and len(volumes) > 0:
            features.extend([
                volumes[-1] / np.mean(volumes[-5:]) - 1,  # æˆäº¤é‡å˜åŒ–
                np.std(volumes[-10:]) / np.mean(volumes[-10:]) if np.mean(volumes[-10:]) > 0 else 0,  # æˆäº¤é‡æ³¢åŠ¨ç‡
            ])
        else:
            features.extend([0, 0])

        # 6. ä½ç½®ç‰¹å¾
        if len(prices) >= 20:
            high_20 = np.max(prices[-20:])
            low_20 = np.min(prices[-20:])
            price_position = (prices[-1] - low_20) / (high_20 - low_20)
            features.append(price_position)
        else:
            features.append(0.5)

        # 7. è¶‹åŠ¿ç‰¹å¾
        if len(returns) >= 10:
            trend = np.polyfit(range(10), returns[-10:], 1)[0]
            features.append(trend)
        else:
            features.append(0)

        # 8. å¡«å……åˆ°æŒ‡å®šç»´åº¦
        while len(features) < self.state_dim:
            features.append(0)

        return np.array(features[:self.state_dim], dtype=np.float32)

    def _calculate_reward(self, action: float, next_price: float, current_price: float,
                          transaction_cost: float = 0.001) -> float:
        """è®¡ç®—å¥–åŠ±"""
        # ä»·æ ¼å˜åŒ–æ”¶ç›Š
        price_return = (next_price - current_price) / current_price

        # ç­–ç•¥æ”¶ç›Š
        strategy_return = action * price_return

        # äº¤æ˜“æˆæœ¬
        cost = abs(action) * transaction_cost

        # é£é™©è°ƒæ•´æ”¶ç›Š
        risk_penalty = 0.1 * abs(action) * abs(price_return)

        # æ€»å¥–åŠ±
        reward = strategy_return - cost - risk_penalty

        return reward

    def train_episode(self, data: pd.DataFrame, start_idx: int = 20) -> Dict[str, Any]:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        self.actor.train()
        self.critic.train()

        total_reward = 0
        episode_length = 0
        actor_losses = []
        critic_losses = []

        state = self._create_state(data, start_idx)
        done = False
        current_idx = start_idx

        while not done and current_idx < len(data) - 1:
            # é€‰æ‹©åŠ¨ä½œ
            action = self.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state = self._create_state(data, current_idx + 1)
            reward = self._calculate_reward(
                action,
                data['close'].iloc[current_idx + 1],
                data['close'].iloc[current_idx]
            )

            done = current_idx >= len(data) - 2

            # å­˜å‚¨ç»éªŒ
            self.replay_buffer.add(state, action, reward, next_state, done)

            # è®­ç»ƒ
            if self.replay_buffer.size() >= self.batch_size:
                actor_loss, critic_loss = self._train_step()
                actor_losses.append(actor_loss)
                critic_losses.append(critic_loss)

            # æ›´æ–°çŠ¶æ€
            state = next_state
            total_reward += reward
            episode_length += 1
            current_idx += 1

        # æ›´æ–°ç»Ÿè®¡
        self.training_stats['rewards'].append(total_reward)
        self.training_stats['episode_lengths'].append(episode_length)
        if actor_losses:
            self.training_stats['actor_losses'].append(np.mean(actor_losses))
        if critic_losses:
            self.training_stats['critic_losses'].append(np.mean(critic_losses))

        self.total_steps += episode_length

        return {
            'total_reward': total_reward,
            'episode_length': episode_length,
            'actor_loss': np.mean(actor_losses) if actor_losses else 0,
            'critic_loss': np.mean(critic_losses) if critic_losses else 0
        }

    def select_action(self, state: np.ndarray, add_noise: bool = True) -> float:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        self.actor.eval()
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().numpy()[0]

        # æ·»åŠ æ¢ç´¢å™ªå£°
        if add_noise:
            action += self.ou_noise.noise()

        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = np.clip(action, -1, 1)

        return action[0] if isinstance(action, np.ndarray) else action

    def _train_step(self) -> Tuple[float, float]:
        """è®­ç»ƒä¸€æ­¥"""
        # é‡‡æ ·ç»éªŒ
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)

        # è½¬æ¢ä¸ºtensor
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.FloatTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        # è®­ç»ƒCritic
        self.critic_optimizer.zero_grad()

        with torch.no_grad():
            next_actions = self.target_actor(next_states)
            target_q1, target_q2 = self.target_critic(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_q = rewards + (1 - dones) * self.gamma * target_q

        current_q1, current_q2 = self.critic(states, actions)

        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)
        critic_loss.backward()
        self.critic_optimizer.step()

        # è®­ç»ƒActor
        self.actor_optimizer.zero_grad()

        actor_actions = self.actor(states)
        actor_loss = -self.critic.q1_forward(states, actor_actions).mean()
        actor_loss.backward()
        self.actor_optimizer.step()

        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._soft_update()

        return actor_loss.item(), critic_loss.item()

    def _soft_update(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):
            target_param.data.copy_(target_param.data * (1 - self.tau) + param.data * self.tau)

        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):
            target_param.data.copy_(target_param.data * (1 - self.tau) + param.data * self.tau)

    def train(self, data: pd.DataFrame, episodes: int = 100) -> Dict[str, Any]:
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸ¤– å¼€å§‹DDPGäº¤æ˜“ç³»ç»Ÿè®­ç»ƒ...")

        for episode in range(episodes):
            # éšæœºèµ·å§‹ç‚¹
            start_idx = np.random.randint(20, len(data) - 50)
            episode_result = self.train_episode(data, start_idx)

            if episode % 10 == 0:
                print(f"Episode {episode}/{episodes}, "
                      f"Reward: {episode_result['total_reward']:.4f}, "
                      f"Actor Loss: {episode_result['actor_loss']:.4f}, "
                      f"Critic Loss: {episode_result['critic_loss']:.4f}")

        self.is_trained = True

        # è®¡ç®—è®­ç»ƒç»Ÿè®¡
        stats = {
            'total_episodes': episodes,
            'total_steps': self.total_steps,
            'avg_reward': np.mean(self.training_stats['rewards']),
            'avg_episode_length': np.mean(self.training_stats['episode_lengths']),
            'final_actor_loss': np.mean(self.training_stats['actor_losses'][-10:]) if self.training_stats['actor_losses'] else 0,
            'final_critic_loss': np.mean(self.training_stats['critic_losses'][-10:]) if self.training_stats['critic_losses'] else 0,
            'training_stats': self.training_stats
        }

        print(f"âœ… è®­ç»ƒå®Œæˆï¼å¹³å‡å¥–åŠ±: {stats['avg_reward']:.4f}")
        return stats

    def get_trading_signals(self, data: pd.DataFrame) -> pd.DataFrame:
        """è·å–äº¤æ˜“ä¿¡å·"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨trainæ–¹æ³•")

        self.actor.eval()
        signals = []

        for i in range(len(data)):
            if i < 20:
                signals.append(0)
            else:
                state = self._create_state(data, i)
                action = self.select_action(state, add_noise=False)
                signals.append(action)

        signals_df = pd.DataFrame({
            'signal': signals,
            'position': signals,  # ç›´æ¥ä½¿ç”¨åŠ¨ä½œä½œä¸ºæŒä»“
            'confidence': np.abs(signals)  # ç»å¯¹å€¼ä½œä¸ºç½®ä¿¡åº¦
        }, index=data.index)

        return signals_df

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
            'hidden_dim': self.hidden_dim,
            'learning_rate': self.lr,
            'device': str(self.device),
            'is_trained': self.is_trained,
            'total_steps': self.total_steps,
            'buffer_size': self.replay_buffer.size(),
            'training_stats': self.training_stats
        }

class OUNoise:
    """OUå™ªå£°è¿‡ç¨‹"""
    def __init__(self, action_dim: int, mu: float = 0, theta: float = 0.15, sigma: float = 0.2):
        self.action_dim = action_dim
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.state = np.ones(action_dim) * mu
        self.reset()

    def reset(self):
        """é‡ç½®å™ªå£°çŠ¶æ€"""
        self.state = np.ones(self.action_dim) * self.mu

    def noise(self) -> np.ndarray:
        """ç”Ÿæˆå™ªå£°"""
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)
        self.state = x + dx
        return self.state

# ä¾¿æ·å‡½æ•°
def create_ddpg_trader(state_dim: int = 20, learning_rate: float = 1e-4) -> DDPGTrader:
    """åˆ›å»ºDDPGäº¤æ˜“å™¨å®ä¾‹"""
    return DDPGTrader(state_dim=state_dim, learning_rate=learning_rate)

def quick_ddpg_training(data: pd.DataFrame, episodes: int = 50) -> Dict[str, Any]:
    """å¿«é€ŸDDPGè®­ç»ƒ"""
    trader = DDPGTrader()

    # è®­ç»ƒæ¨¡å‹
    training_stats = trader.train(data, episodes=episodes)

    # ç”Ÿæˆä¿¡å·
    signals = trader.get_trading_signals(data)

    return {
        'training_stats': training_stats,
        'latest_signal': signals['signal'].iloc[-1] if len(signals) > 0 else 0,
        'latest_position': signals['position'].iloc[-1] if len(signals) > 0 else 0,
        'model_info': trader.get_model_info()
    }